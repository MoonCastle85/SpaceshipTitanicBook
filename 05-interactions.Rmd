# Interactions
So far we've only created features from single variables but what about the effects of two variables together? Would it help our models if we added an interaction effect between for example HomePlanet and Destination to create the new feature HomeDestination? What about other such interactions?

In the previous competition for the Titanic of 1912, the sex of a passenger mattered (women more likely to survive) and the ticket class mattered (first class more likely to survive) but the interaction "woman in first class" had an almost 100% of survival and this interaction improved the model (if I remember correctly). We want to discover if such interactions exist between the variables that we have for our space odyssey.

Of course, we won't know the extent of the improvement until we test the interaction effects in various models. Some models inherently discover interactions (like tree-models) and the addition of interaction effects might not matter while it might matter for others. 

## Visual exploration of interactions
Since we only have a few categorical variables, let's visualize all possible interactions. Here I use the default `glm` logistic regression model where the formula:
`Transported ~ (.)^2` that amounts to *Outcome \~ Variable_1 + Variable_2 + Variable_1 x Variable_2*

```{r interactions-all, warning = FALSE, cache=TRUE, message=FALSE, fig.cap = "The effect of interactions between all pairs of variables against the response with confidence intervals included."}
plot_simple_int <- function(df, v1, v2) {
  tmp_vars <- c("Transported", v1, v2)
  tmp_model <- glm(Transported ~ (.)^2, data = df[, tmp_vars], family = binomial())
  p <- interactions::cat_plot(tmp_model, pred = {{ v1 }}, modx = {{ v2 }}, geom = "line", colors = c25,
                       main.title = paste(v1, "and", v2))
  return(p)
}

preds_cat <- c("CryoSleep", "HomePlanet", "Destination", "VIP", "Deck", "Side")
pairs <- combn(preds_cat, 2, simplify = FALSE)
c25 <- c("dodgerblue2", "#E31A1C", "green4", "#6A3D9A", "#FF7F00", "black", "gold1", "skyblue2", "#FB9A99", "palegreen2", "#CAB2D6",
  "#FDBF6F", "gray70", "khaki2", "maroon", "orchid1", "deeppink1", "blue1", "steelblue4", "darkturquoise", "green1", "yellow4",
  "yellow3", "darkorange4", "brown") # Had to add extra colours because I couldn't get `cat_plot` to work with defaults

save_plot <- function(p, i) {
  ggsave(filename = paste0("PairInt", i, ".png"), plot = p)
}

pair_int_plots <- pairs %>%
  map(.x = ., .f = \(vp) plot_simple_int(train5, vp[1], vp[2]))

plotsToSVG <- walk2(.x = pair_int_plots, .y = seq_along(pair_int_plots), .f = save_plot)
plotsToSVG <- map(1:length(pair_int_plots), .f = \(i) paste0("PairInt", i, ".png"))
slickR::slickR(plotsToSVG, height = "480px", width = "672px") +
  slickR::settings(slidesToShow = 1)
```

Parallel lines indicate no significant interaction effects while lines that cross indicate a potential for a significant interactions. I'll highlight a few interactions below.

```{r int-cryo-home, warning = FALSE, cache=TRUE, fig.cap = "Interaction between CryoSleep and HomePlanet"}
plot_simple_int(train6, "CryoSleep", "HomePlanet")
```

The interaction between CryoSleep and HomePlanet suggests that passengers from Earth are less likely to be transported when in cryosleep which suggests that the interaction CryoSleep & HomePlanet could be useful.

```{r int-deck-side, warning = FALSE, results = "hide", fig.cap = "Interaction between Deck and Side"}
plot_simple_int(train6, "Deck", "Side")
```

The interaction between Deck and Side, however, seems to show only a minor effect, if any.

## Interaction significance with only variable pairs and their interactions
One problem with the above visual approach is that we don't know if these interaction effects are real - in the sense they represent some true relationship with the outcome - or if they're so called false positives - that is, random effects that happened to be present in the training data. For example, does the fact that passengers from Earth seem less likely to be transported even when in cryo sleep reflect some true aspect of the spacetime anomaly that caused the transportation to another dimension or is this pattern just something that happened this time on pure chance? Another way to think of this is: if the Spaceship Titanic were to pass through this spacetime anomaly a thousand times, would the pattern persist?

To explore this further, we must validate some of these effects by cross validation. We will continue to use our simple `glm` model to model the response against each pair of variables and then compare it to a similar model that includes the interaction term and we'll use the accuracy metric for comparison. 

```{r int-pairwise, warning = FALSE, cache = TRUE}
compare_models_1way <- function(a, b, metric = a$metric[1], ...) { # A customized compare_models function from caret that allows for
  mods <- list(a, b)                                               # a custom t.test adjustment in the diff-function
  rs <- resamples(mods)
  diffs <- diff(rs, metric = metric[1], adjustment = "none", ...)
  res <- diffs$statistics[[1]][[1]]
  return(res)
}

pair_model <- function(df, v1, v2) { # Model without interactions with only two variables
  tmp_vars <- c("Transported", v1, v2)
  set.seed(8584)
  m <- train(Transported ~ ., data = df[, tmp_vars], preProc = NULL, method = "glm", metric = "accuracy", trControl = ctrl)
  return(m)
}

pair_int_model <- function(df, v1, v2) { # Model with interactions with only two variables
  tmp_vars <- c("Transported", v1, v2)
  set.seed(8584)
  m <- train(Transported ~ (.)^2, data = df[, tmp_vars], preProc = NULL, method = "glm", metric = "accuracy", trControl = ctrl)
  return(m)
}

preds_cat <- c("CryoSleep", "HomePlanet", "Destination", "Age", "VIP", "Deck", "Side", "RoomService", "FoodCourt", "ShoppingMall",
               "Spa", "VRDeck", "CabinNumber", "LastNameAsNumber", "PassengerGroup")
pairs <- combn(preds_cat, 2, simplify = FALSE)
pairs_cols <- combn(preds_cat, 2, simplify = TRUE) %>%
  t() %>%
  as.data.frame()

ctrl <- trainControl(method = "repeatedcv", repeats = 5, classProbs = TRUE, summaryFunction = prSummary, allowParallel = TRUE)

# my_cluster <- makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# 
# no_int_mods <- pairs %>%
#   map(.x = ., .f = \(vp) pair_model(train6, vp[1], vp[2]))
# 
# int_mods <- pairs %>%
#   map(.x = ., .f = \(vp) pair_int_model(train6, vp[1], vp[2]))
# 
# stopCluster(my_cluster)
# 
# no_int_acc <- no_int_mods %>%
#   list_flatten(.) %>%
#   map(.x = ., .f = \(m) getTrainPerf(m)[1, "TrainPrecision"]) %>%
#   list_c(.)
# 
# diff_res <- map2(.x = no_int_mods, .y = int_mods, .f = \(m1, m2) compare_models_1way(m1, m2, alternative = "greater"))
# 
# save(no_int_acc, file = "No int acc.RData")
# save(diff_res, file = "Diff results pairs.RData")

load("No int acc.RData")
load("Diff results pairs.RData")

diff_res2 <-
  data.frame(Improvement = map_dbl(.x = diff_res, .f = \(est) est$estimate),
             Pvalue = map_dbl(.x = diff_res, .f = \(p) p$p.value)) %>%
  bind_cols(., Accuracy = no_int_acc) %>%
  bind_cols(pairs_cols, .)
```

```{r int-pairwise-results}
diff_res2 %>% 
  filter(Pvalue <= 0.05) %>%
  arrange(desc(Improvement)) %>%
  slice(1:5)

diff_res2 %>% 
  filter(Pvalue <= 0.05) %>%
  arrange(desc(Accuracy)) %>%
  slice(1:5)
```

The p-value test here tells us whether the effects of the interaction terms were large enough to be considered non-random. We've used a cutoff of 5% but without any adjustment. This can be problematic since the more tests we run with a 5% cutoff, the higher the chance of finding interactions that have an effect purely by chance (in fact, for our 105 pairs with a 5% cutoff, the chances of getting a false positive is practically guaranteed). We will look at some adjustment methods for p-values to take this into account later.

For now, we can conclude that there are 36 variable pairs that seem to improve model performance compared to a model without interaction effects. The two pairs that seem to add a significant improvement are Spa & VRDeck as well as CryoSleep & Destination. Notice also that the accuracy is impressively high for a model that only used RoomService and VRDeck (without interactions) as well as Spa and VRDeck. This suggests that the amenity variables will be very important.

## Interaction significance with entire model and pairwise interactions
One weakness with the above approach is that we've looked at variable pairs and their interactions in absence of the other variables so we don't know how these pairwise interactions contribute to a model with all the other variables. Do the improvements still persist or do some of them become correlated existing variables so that their effects lessen or even introduce noise to the data? 

Let's explore that.

```{r int-full-unadjusted, warning = FALSE, cache = TRUE}
my_split <- initial_split(train6, prop = 0.8)
int_train <- training(my_split) %>%
  mutate(across(.cols = c(PassengerGroup), .fns = as.integer)) %>%
  select(CryoSleep, HomePlanet, Destination, VIP, Deck, Side, Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, CabinNumber,
         LastNameAsNumber, PassengerGroup, Transported)

norm_ctrl <- trainControl(method = "repeatedcv", repeats = 5, classProbs = TRUE, summaryFunction = prSummary)

norm_rec <- recipe(Transported ~ ., data = int_train) %>%
  step_dummy(CryoSleep, HomePlanet, Destination, VIP, Deck, Side)

# my_cluster <- makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# 
# set.seed(8584)
# norm_m <- train(norm_rec, data = int_train, method = "glm", metric = "accuracy", trControl = norm_ctrl)
# 
# norm_m_acc <- getTrainPerf(norm_m)[1, "TrainPrecision"]
# 
# int_ctrl <- trainControl(method = "repeatedcv", repeats = 5, classProbs = TRUE, summaryFunction = prSummary)
# 
# int_function <- function(rec, f) {
#   ir <- step_interact(recipe = rec, terms = !!f)
#   return(ir)
# }
# # Map over pairs of vars to create int formulas
# int_form <- map(.x = pairs, .f = \(vp) formula(paste0("~starts_with('", vp[1], "'):starts_with('", vp[2], "')")))
# int_rec <- map(.x = int_form, .f = \(form) int_function(norm_rec, f = form))
# 
# train(int_rec[[65]], data = int_train, method = "glm", metric = "accuracy", trControl = int_ctrl)
# 
# set.seed(8584)
# int_m <- map(.x = int_rec, .f = \(r) train(r, data = int_train, method = "glm", metric = "accuracy", trControl = int_ctrl))
# 
# int_m_acc <- map_dbl(.x = int_m, .f = \(m) getTrainPerf(m)[1, "TrainPrecision"])
# 
# anova_res <- map2(.x = list(norm_m), .y = int_m, .f = \(m1, m2) anova(m1$finalModel, m2$finalModel, test = "Chisq")[2, 'Pr(>Chi)'])
# 
# diff_all_res <- map2(.x = int_m, .y = list(norm_m), .f = \(m1, m2) compare_models_1way(m1, m2, alternative = "greater"))
# 
# stopCluster(my_cluster)
# unregister()
# 
# diff_all_res2 <-
#   data.frame(Improvement = map_dbl(.x = diff_all_res, .f = \(est) est$estimate),
#              Resampled_Pvalue = map_dbl(.x = diff_all_res, .f = \(p) p$p.value),
#              Traditional_Pvalue = map_dbl(.x = anova_res, .f = \(p) p)) %>%
#   bind_cols(., No_Int_Accuracy = norm_m_acc) %>%
#   bind_cols(., With_Int_Accuracy = int_m_acc) %>%
#   bind_cols(pairs_cols, .)
# 
# save(diff_all_res2, file = "Results pairwise interactions with all other variables.RData")
load("Results pairwise interactions with all other variables.RData")

diff_all_res2_adj <- diff_all_res2 %>%
  mutate(Resampled_pvalue_FDR = p.adjust(Resampled_Pvalue, method = "fdr"),
         Traditional_pvalue_FDR = p.adjust(Traditional_Pvalue, method = "fdr"),
         Resampled_pvalue_Bon = p.adjust(Resampled_Pvalue, method = "bonferroni"),
         Traditional_pvalue_Bon = p.adjust(Traditional_Pvalue, method = "bonferroni"))
```

```{r int-full-unadjusted-results}
diff_all_res2_adj %>% 
  filter(Resampled_Pvalue <= 0.05) %>%
  select(V1, V2, Improvement, Resampled_Pvalue, With_Int_Accuracy) %>%
  arrange(desc(Improvement))
```

Now that we've added all variables to our models, we see that only two interaction effects are statistically significant without any p-value adjustment (column Resampled_Pvalue). Kuhn and Johnson write: 
> When the interactions that were discovered were included in a broader model that contains other (perhaps correlated) predictors, their importance to the model may be diminished. (...) This might reduce the number of predictors considered important (since the residual degrees of freedom are smaller) but the discovered interactions are likely to be more reliably important to a larger model.

```{r int-full-adjusted}
diff_all_res2_adj %>% 
  filter(Resampled_pvalue_FDR <= 0.2) %>%
  select(V1, V2, Improvement, Resampled_pvalue_FDR, With_Int_Accuracy) %>%
  arrange(desc(Improvement))

diff_all_res2_adj %>% 
  filter(Resampled_pvalue_Bon <= 0.2) %>%
  select(V1, V2, Improvement, Resampled_pvalue_Bon, With_Int_Accuracy) %>%
  arrange(desc(Improvement))
```

If we were to adjust our values, only a single interaction effect remains that is CryoSleep and HomePlanet. The stricter adjustment is the Boneferroni which multiplies all the p-values with the number of tests (105 in our case) while a less strict adjustment is the FDR that sorts the p-values in ascending order and multiplies each one with the total number of tests divided by the p-value's position (so that the lowest value gets multiplied by 105/1 in our case, the second lowest by 105/2 and so on). In the case of our interaction, it holds even with the stricter Bonferroni adjustment which implies that the effect is relatively strong.

Does this mean that we shouldn't include the other interaction effects in our model? No. We can still try to add the ones we've discovered so far to a later stage where we will evaluate which features to select.

```{r int-full-store}
int_vars_very_imp <- diff_all_res2_adj %>% 
  filter(Resampled_pvalue_Bon <= 0.2) %>% 
  select(V1, V2) %>%
  mutate(ForFormula = str_c("starts_with('", V1, "'):starts_with('", V2, "')")) %>%
  mutate(RevFormula = str_c("starts_with('", V2, "'):starts_with('", V1, "')"))

int_vars_maybe_imp <- diff_all_res2_adj %>% 
  filter(Resampled_Pvalue <= 0.05) %>% 
  select(V1, V2) %>%
  mutate(ForFormula = str_c("starts_with('", V1, "'):starts_with('", V2, "')")) %>%
  mutate(RevFormula = str_c("starts_with('", V2, "'):starts_with('", V1, "')"))
```

## All interaction effects
Let's turn to a more comprehensive interaction analysis where we'll use penalized regression to see whether any interactions prove beneficial to the model. Here we use the `glmnet`-function to first fit a model without interactions and then a model with interactions that we tune with different parameters. 

The $\lambda$ parameter controls the penalty that is applied to different variables to maximize accuracy while the $\alpha$ controls the weight applied to each penalty function so that $\alpha = 1$ signifies a Lasso penalty where the penalty is applied to the absolute of the regression coefficient while $\alpha = 0$ signifies a Ridge penalty where the penalty is applied to the squared coefficient. In our case, the glmnet model allows us to tune for the mix of these factors that gives the best accuracy.

```{r pen-reg-without-int, warning = FALSE, message = FALSE, cache = TRUE}
df_pen <- train6 %>%
  select(-c(Cabin, Name, LastName, PassengerCount, HomePlanetsPerGroup)) %>%
    mutate(across(.cols = c(PassengerGroupSize, tidyselect::ends_with("PerGroup")), .fns = as.integer))

set.seed(8584)
my_pen_split <- initial_split(df_pen, prop = 0.8)
pen_train <- training(my_pen_split)
my_pen_folds <- vfold_cv(pen_train, v = 10, repeats = 5)

my_vars <- data.frame(Variables = names(pen_train)) %>%
  mutate(Roles = if_else(Variables == "PassengerId", "id", "predictor"),
         Roles = if_else(Variables == "Transported", "outcome", Roles))

pen_rec <- recipe(x = pen_train, vars = my_vars$Variables, roles = my_vars$Roles) %>%
  step_normalize(Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, TotalSpent, CabinNumber, LastNameAsNumber, PassengerGroup,
                 TotalSpentPerGroup) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors())

my_acc <- metric_set(accuracy)
pen_ctrl <- control_grid(verbose = TRUE, save_pred = TRUE, save_workflow = TRUE)
pen_grid <- expand.grid(mixture = c(0.2, 0.6, 1), penalty = c(1e-04, 2e-04, 3e-04, 4e-04, 5e-04))

glmnet_mod <- logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet")

# my_cluster <- makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# 
# system.time({
#   set.seed(8584)
#   pen_tune <- glmnet_mod %>%
#     tune_grid(pen_rec, resamples = my_pen_folds, metrics = my_acc, control = pen_ctrl, grid = pen_grid)
# })
# 
# save(pen_tune, file = "Penalized regression without interactions.RData")
# 
# stopCluster(my_cluster)
# unregister()

load("Penalized regression without interactions.RData")
```

```{r pen-reg-without-int2, warning = FALSE, message = FALSE, fig.cap = "Tuning results for a Lasso/Ridge regression without interaction effects."}
pen_best <- fit_best(pen_tune)
pen_coef <- pen_best %>%
  tidy() %>%
  filter(estimate != 0) %>%
  filter(term != "(Intercept)") %>%
  pull(term)

show_best(pen_tune, metric = "accuracy", n = 20) %>%
  mutate(mixture = as.factor(round(mixture, 2))) %>%
  ggplot(aes(x = penalty, y = mean, label = mixture, colour = mixture)) +
  geom_line() +
  geom_point() +
  scale_x_log10() +
  labs(title = "Tune results without interactions", x = "Lambda penalty", y = "Resample accuracy", colour = "Alpha")
```

Without any interaction effects, the regression model favoured the pure Lasso regression ($\alpha = 1$) with the penalty $\lambda = 0.0002$. The most important variables (with the highest coefficients) were in this case VRDeck, Spa, Cryosleep, HomePlanet and Deck. The final model used 32 out of 36 variables and the resampled accuracy without any interactions was close to 0.79.

Now if we repeat the process but we include interaction effects between the variables that were chosen in the previous section:

```{r pen-reg-with-int, warning = FALSE, message = FALSE, cache = TRUE}
int_vars <- pen_coef %>%
  str_split_i(., "_", 1) %>%
  unique() %>%
  combn(., 2, simplify = FALSE)

# Map over pairs of vars to create int formula
int_formula <- map_chr(.x = int_vars, .f = \(vp) paste0("starts_with('", vp[1], "'):starts_with('", vp[2], "')")) %>%
  str_flatten(., collapse = "+") %>%
  paste("~", .) %>%
  as.formula(.)

pen_int_rec <- recipe(x = pen_train, vars = my_vars$Variables, roles = my_vars$Roles) %>%
  step_normalize(Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, TotalSpent, CabinNumber, LastNameAsNumber, PassengerGroup,
                 TotalSpentPerGroup) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(int_formula) %>%
  step_zv(all_predictors())
  
# my_cluster <- makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# clusterExport(cl = my_cluster, "int_formula")
# 
# set.seed(8584)
# pen_int_tune <- glmnet_mod %>%
#   tune_grid(pen_int_rec, resamples = my_pen_folds, metrics = my_acc, control = pen_ctrl, grid = pen_grid)
# 
# save(pen_int_tune, file = "Penalized regression with interactions.RData")
# 
# stopCluster(my_cluster)
# unregister()

load("Penalized regression with interactions.RData")
```

We see the results from the penalized regression model with interactions below.

```{r pen-reg-with-int2, warning = FALSE, message = FALSE}
pen_int_best <- fit_best(pen_int_tune)
pen_int_coef <- pen_int_best %>%
  tidy()
```

```{r pen-reg-with-int3, warning = FALSE, message = FALSE, fig.cap = "Tuning results for a Lasso/Ridge regression with interaction effects."}
show_best(pen_int_tune, metric = "accuracy", n = 20) %>%
  mutate(mixture = as.factor(round(mixture, 2))) %>%
  ggplot(aes(x = penalty, y = mean, label = mixture, colour = mixture)) +
  geom_line() +
  geom_point() +
  scale_x_log10() +
  labs(title = "Tune results with interactions", x = "Lambda penalty", y = "Resample accuracy", colour = "Alpha")
```

The regression model still favours the pure Lasso ($\alpha = 1$) but with a higher penalty $\lambda = 0.0005$. Out of 490 total variables with interaction effects, it selected 296 for the best accuracy which was close to 0.80. Compared to the model without interactions effects (accuracy = 0.79), this doesn't seem like the greatest improvement. 

We can look at the best variables with interactions below.

```{r pen-reg-int-top}
pen_int_top <- pen_int_coef %>%
  select(-penalty) %>%
  filter(estimate != 0) %>%
  arrange(desc(abs(estimate))) %>%
  slice(1:10)

pen_int_top
```

It's surprising to see that the strongest effects were between Deck & Side, which we visually inspected in Figure \@ref(fig:int-deck-side) where it looked as if the effect of the interaction wasn't very large. It also didn't show up in our pairwise exploration 

The most important variables were almost all interaction effects such as CryoSleep & Deck, HomePlanet & CryoSleep and so on. If we go back to our visualizations, we see that Deck = F does seem to have an effect when CryoSleep changes from False to True and this is true for the change between homeplanets, as well.

The original variables that made the top ten list were VRDeck, Spa and Cryosleep. From our smoothed graphs, we saw that any credits spent on VRDeck significantly reduced the chances of being transported so this also makes sense. 

Let's add these discoveries to our important variable list.

```{r int-pen-reg-store}
int_vars_very_imp2 <- pen_int_top %>%
  filter(str_detect(term, "_x_")) %>%
  mutate(V1 = str_split_i(term, "_", 1),
         V2 = str_split_i(term, "_", -2),
         ForFormula = str_c("starts_with('", V1, "'):starts_with('", V2, "')"),
         RevFormula = str_c("starts_with('", V2, "'):starts_with('", V1, "')")) %>%
  select(V1, V2, ForFormula, RevFormula) %>%
  bind_rows(int_vars_very_imp, .)

int_vars_very_imp2
```

## Tree model interactions
Max and Kuhn also offer another method for discovering interaction effects by the use of a tree-based model like random forest. The algorithm doesn't evaluate the effects of interactions but it does rank the original variables based on their importance which can be used to model interaction effects in a second stage.
```{r tree-based-var-imp, cache = TRUE, fig.cap = "Variable importance based on a the ranger tree-model"}
df_tree <- train6 %>%
  mutate(across(.cols = c(PassengerGroupSize, Solo, LargeGroup, TravelTogether, tidyselect::ends_with("PerGroup")),
                .fns = as.integer))

set.seed(8584)
my_tree_split <- initial_split(df_tree, prop = 0.8)
tree_train <- training(my_tree_split)

my_vars <- data.frame(Variables = names(tree_train)) %>%
  mutate(Roles = if_else(Variables %in% c("PassengerId", "Cabin", "Name", "LastName", "PassengerCount", "HomePlanetsPerGroup"),
                         "id", "predictor"),
         Roles = if_else(Variables == "Transported", "outcome", Roles))

tree_rec <- recipe(x = tree_train, vars = my_vars$Variables, roles = my_vars$Roles) %>%
  step_zv(all_predictors()) %>%
  step_normalize(Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, TotalSpent, CabinNumber, LastNameAsNumber, PassengerGroup,
                 TotalSpentPerGroup) %>%
  step_dummy(all_nominal_predictors())

tree_baked <- tree_rec %>%
  prep() %>%
  bake(new_data = NULL) %>%
  select(-c(Cabin, Name, LastName, PassengerCount, HomePlanetsPerGroup))

# rf_mod <- ranger::ranger(Transported ~ . -PassengerId, data = tree_baked, num.trees = 1000, importance = "impurity", 
#                  num.threads = detectCores() - 1, seed = 8584)
# 
# save(rf_mod, file = "Ranger tree model variable importance.RData")
load("Ranger tree model variable importance.RData")

rf_imp <- tibble(Predictor = names(rf_mod$variable.importance),
                 Importance = unname(rf_mod$variable.importance))
```


```{r tree-based-var-imp2, fig.cap = "Variable importance based on a the ranger tree-model"}
ggplot(rf_imp, aes(x = reorder(Predictor, Importance), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  xlab("")
```

The tree model heavily favours the amenity variables as well as some other variables that our penalized model didn't consider important enough. We saw, however, from our numerical analysis, that many of the amenities had large effects on the response and so perhaps their interactions do, as well? Based on the ranking from the tree model, as well as our exploration from earlier, let's add additional interactions that might be interesting to explore.

```{r tree-int-important}
tree_int_vars <- rf_imp %>%
  filter(Importance > 150) %>%
  pull(Predictor) %>%
  combn(., 2, simplify = TRUE) %>%
  t() %>%
  as.data.frame(.) %>%
  mutate(V1 = str_split_i(V1, "_", 1),
         V2 = str_split_i(V2, "_", 1),
         ForFormula = str_c("starts_with('", V1, "'):starts_with('", V2, "')"),
         RevFormula = str_c("starts_with('", V2, "'):starts_with('", V1, "')"))
```


```{r remove-05, include=FALSE}
rm(pairs, pair_int_plots, plotsToSVG, pairs_cols, int_mods, no_int_mods, no_int_acc, diff_res, diff_res2, my_split, int_train,
   norm_ctrl, norm_rec, diff_all_res2, diff_all_res2_adj, df_pen, my_pen_split, pen_train, my_pen_folds, my_vars, pen_rec, 
   pen_ctrl, pen_grid, glmnet_mod, pen_best, pen_coef, int_vars, int_formula, pen_int_rec, pen_int_top, rf_imp, df_tree,
   my_tree_split, tree_train, tree_rec, tree_baked)
```
