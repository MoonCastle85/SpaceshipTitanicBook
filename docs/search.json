[{"path":"index.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"name Vanja Manborg ever since got dabble data science engineering job, ’ve spent free time RStudio instead things like family, friends going movies. believe learning--methodology attempt get relatively high scores Spaceship Titanic competition Kaggle learn new concepts tricks data-science ’m . competition futuristic version Titanic challenge goal predict high accuracy possible space-faring passengers got transported another dimension spaceships maiden voyage solar system another.formalities:won’t go details packages functionsI won’t go details packages functionsI’ve relied excellent Max Kuhn’s Kjell Johnson’s book ‘Feature Engineering Selection: Practical Approach Predictive Models’ used code advanced feature exploration.’ve relied excellent Max Kuhn’s Kjell Johnson’s book ‘Feature Engineering Selection: Practical Approach Predictive Models’ used code advanced feature exploration.Without ado, let’s dive !","code":""},{"path":"early-data-exploration.html","id":"early-data-exploration","chapter":"2 Early data exploration","heading":"2 Early data exploration","text":"","code":""},{"path":"early-data-exploration.html","id":"overview-of-the-data","chapter":"2 Early data exploration","heading":"2.1 Overview of the data","text":"data already divided train test sets let’s load get sense train data looks like. ’ll leave test data (won’t even look ) end.Let’s first get sense train data.8693 observations 14 variables “Transported” response. specifics different variables explained detail competition website won’t repeat . ’s enough note mix categorical numerical variables response binary classification (True False).","code":"\ntrain <- read.csv(\"train.csv\", na.strings = \"\")\nstr(train)\n#> 'data.frame':    8693 obs. of  14 variables:\n#>  $ PassengerId : chr  \"0001_01\" \"0002_01\" \"0003_01\" \"0003_02\" ...\n#>  $ HomePlanet  : chr  \"Europa\" \"Earth\" \"Europa\" \"Europa\" ...\n#>  $ CryoSleep   : chr  \"False\" \"False\" \"False\" \"False\" ...\n#>  $ Cabin       : chr  \"B/0/P\" \"F/0/S\" \"A/0/S\" \"A/0/S\" ...\n#>  $ Destination : chr  \"TRAPPIST-1e\" \"TRAPPIST-1e\" \"TRAPPIST-1e\" \"TRAPPIST-1e\" ...\n#>  $ Age         : num  39 24 58 33 16 44 26 28 35 14 ...\n#>  $ VIP         : chr  \"False\" \"False\" \"True\" \"False\" ...\n#>  $ RoomService : num  0 109 43 0 303 0 42 0 0 0 ...\n#>  $ FoodCourt   : num  0 9 3576 1283 70 ...\n#>  $ ShoppingMall: num  0 25 0 371 151 0 3 0 17 0 ...\n#>  $ Spa         : num  0 549 6715 3329 565 ...\n#>  $ VRDeck      : num  0 44 49 193 2 0 0 NA 0 0 ...\n#>  $ Name        : chr  \"Maham Ofracculy\" \"Juanna Vines\" \"Altark Susent\" \"Solam Susent\" ...\n#>  $ Transported : chr  \"False\" \"True\" \"False\" \"False\" ..."},{"path":"early-data-exploration.html","id":"do-some-variables-have-zero-or-near-zero-variance","chapter":"2 Early data exploration","heading":"2.2 Do some variables have zero or near-zero variance?","text":"One first things might useful check whether variables zero near-zero variance. zero variance variable one rows value (imagine variable rows just number 1). variables useful prediction since don’t change way response.close cousin near-zero variances high percentage rows value don’t. can particularly problematic want repeated cross validation divide data subdata test models. subdata might zero variance others might get high variance variable.VIP variable near-zero variation (2.3% rows). ’ll see becomes issue later process.","code":"\nnearZeroVar(train, saveMetrics = TRUE) %>% rownames_to_column() %>% filter(nzv)\n#>   rowname freqRatio percentUnique zeroVar  nzv\n#> 1     VIP  41.66332    0.02300702   FALSE TRUE"},{"path":"early-data-exploration.html","id":"frequency-distribution-of-all-variables","chapter":"2 Early data exploration","heading":"2.3 Frequency distribution of all variables","text":"Let’s see values different variables distributed.\nFigure 2.1: Distribution response\nresponse training set balanced good means don’t need - oversampling balance . balanced response necessary tuning many models since makes metrics tune make sense. great unbalance true responses vs false ones, metrics like accuracy might distorted since even random guesses might give us high accuracy just fact distirbution unbalanced.\nFigure 2.2: Distribution predictor variables\nnumeric variables right-skewed, means outliers right horizontal axis amenities (RoomService, FoodCourt etc) high proportion zero values. difference scales also large. need apply kind preproces scale help models like K-nearest-neighbors (KNN). might also need add binary features (yes/1/0) amenities zero perhaps help models differentiate rest.Cabin Name variables near uniform distribution implies either won’t much predictive power even reduce accuracy introducing unnecessary noise model. either derive features variables include models.","code":"\nsjPlot::plot_frq(train, Transported, type = \"histogram\")\nmy_frq <- sjPlot::plot_frq(train, type = \"histogram\")\nsave_plot <- function(p, i) {\n  ggsave(filename = paste0(\"Variable\", i, \".png\"), plot = p)\n}\nplotsToSVG <- walk2(.x = my_frq, .y = seq_along(my_frq), .f = save_plot)\nplotsToSVG <- map(1:length(my_frq), .f = \\(i) paste0(\"Variable\", i, \".png\"))\nslickR::slickR(plotsToSVG, height = \"480px\", width = \"672px\") +\n  slickR::settings(slidesToShow = 1)"},{"path":"early-data-exploration.html","id":"correlation-between-the-variables","chapter":"2 Early data exploration","heading":"2.4 Correlation between the variables","text":"\nFigure 2.3: Correlation numerical predictor variables\nsee ’s relatively high correlation home planets destinations well moderate correlation CryoSleep Transported already gives us hint CryoSleep might important. Otherwise, variables seem relatively uncorrelated good thing since models handle variables correlated.","code":"\ntrain %>%\n  na.omit(.) %>%\nDataExplorer::plot_correlation(., type = \"all\", theme_config = list(text = element_text(size = 20), axis.text.x = element_text(angle = 90)))"},{"path":"early-data-exploration.html","id":"overview-of-missing-values","chapter":"2 Early data exploration","heading":"2.5 Overview of missing values","text":"\nFigure 2.4: Missing values overview\npredictor variables around 2% missing data relatively low. cluster missing values, can also see don’t overlap suggests ’re missing random due specific pattern. tells us can probably find suitable algorithm help us impute missing values based non-missing values. pattern clusters missing data, ’d explore uncover reasons non-random missingness.However, see exploration data might pattern missingness, least values missing can inferred others.","code":"\nplot_na_hclust(train)"},{"path":"handle-missing-data.html","id":"handle-missing-data","chapter":"3 Handle missing data","heading":"3 Handle missing data","text":"best way handle missing data first visualize get sense ’s going .","code":""},{"path":"handle-missing-data.html","id":"simple-but-useful-features","chapter":"3 Handle missing data","heading":"3.1 Simple but useful features","text":"’re given several useful bits information data competition website can apply create useful features. features, mean new variables derived existing ones. example, Cabin comprised information cabin number, deck side PassengerId contains passenger group id.’ve created function derives new variables existing ones. also creates features count number unique categories different variables passenger group. Since passenger group variable one missing values, makes sense start exploration others based . Questions ask : ‘passenger group travelling different home planets?’","code":"\nuseful_features <- function(x) {\n  x2 <- x %>%\n    mutate(PassengerGroup = str_split_i(PassengerId, \"_\", 1),\n           LastName = word(Name, -1),\n           Deck = str_split_i(Cabin, \"/\", 1),\n           CabinNumber = str_split_i(Cabin, \"/\", 2),\n           Side = str_split_i(Cabin, \"/\", 3),\n           TotalSpent = RoomService + FoodCourt + ShoppingMall + Spa + VRDeck,\n           PassengerCount = 1) %>% # I added this to use as a count variable in visualizations\n    group_by(PassengerGroup) %>%\n    add_count(PassengerGroup, name = \"PassengerGroupSize\") %>%\n    mutate(HomePlanetsPerGroup = n_distinct(HomePlanet, na.rm = TRUE),\n           DestinationsPerGroup = n_distinct(Destination, na.rm = TRUE),\n           CabinsPerGroup = n_distinct(Cabin, na.rm = TRUE),\n           TotalSpentPerGroup = sum(TotalSpent, na.rm = TRUE),\n           CryoSleepsPerGroup = n_distinct(CryoSleep, na.rm = TRUE),\n           VIPsPerGroup = n_distinct(VIP, na.rm = TRUE),\n           LastNamesPerGroup = n_distinct(LastName, na.rm = TRUE)) %>%\n    ungroup() %>%\n    mutate(across(.cols = c(HomePlanet, CryoSleep, Destination, VIP, Transported, Deck, Side, HomePlanetsPerGroup,\n                            PassengerGroupSize, DestinationsPerGroup, CabinsPerGroup, CryoSleepsPerGroup, VIPsPerGroup,\n                            LastNamesPerGroup, PassengerGroup, PassengerId),\n                  .fns = as.factor)) %>%\n    mutate(across(.cols = c(CabinNumber, Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck),\n                  .fns = as.integer))\n  return(x2)\n}\n\ntrain2 <- useful_features(train)"},{"path":"handle-missing-data.html","id":"closer-look-at-missing-values","chapter":"3 Handle missing data","heading":"3.2 Closer look at missing values","text":"closer look variable missing values gives us insights. example, every passenger group always starts home planet. 3.1 shows sample passenger groups home planets. one patterns missingness alluded earlier can use information replace missing HomePlanet values passengers belonging group known home planet. leave us passengers travelling alone.\nFigure 3.1: Sample missing values HomePlanet\ncan also see groups two travelers never housed cabins (non-solo) groups. words, group two passengers can spread across several cabins never share cabins groups two passengers. 3.2 shows sample. can use information replace missing cabin values.\nFigure 3.2: Sample missing values Cabin\nlook new feature Deck, can see decks seem dedicated passengers specific home planet, see 3.3. helps us replace even missing values HomePlanet.\nFigure 3.3: Missing values Deck\nalso see aren’t VIPs travelling Earth can use information replace missing VIP-values home planet known.\nFigure 3.4: Missing values VIP\nalso told passengers cryosleep confined cabins must assume means spend credits amenities. words, can replace missing amenities zeroes passengers cryo sleep. reverse also true: passenger spent credits, passenger cryo sleep.One last insight can get missing data passengers 12 years age don’t spend credits. can therefore use replace even missing values amenities.\nFigure 3.5: Missing values ameneties\n","code":"\ntrain2 %>%\n  group_by(PassengerGroup) %>%\n  filter(any(is.na(HomePlanet))) %>%\n  ungroup() %>%\n  slice_sample(n = 50) %>%\n  ggplot(., mapping = aes(x = PassengerGroup, y = PassengerCount, fill = HomePlanet)) +\n    geom_col()  +\n    labs(title = \"Example missing HomePlanet\", x = \"Passenger group\", y = \"Number of passengers\") +\n    scale_y_continuous(breaks = seq(0, 9000, 50)) +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))\ntrain2 %>%\n  filter(PassengerGroupSize != 1 & Side == \"S\" & Deck == \"G\") %>%\n  slice_sample(n = 50) %>%\n  ggplot(., mapping = aes(x = as.factor(CabinNumber), y = PassengerCount, fill = PassengerGroup)) +\n  geom_col() +\n  labs(title = \"Example missing CabinNumber\", x = \"Cabin number\", y = \"Number of passengers\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1), legend.position = \"none\")\ntrain2 %>%\n  group_by(Deck) %>%\n  filter(any(is.na(HomePlanet))) %>%\n  ungroup() %>%\n  ggplot(., mapping = aes(x = Deck, y = PassengerCount, fill = HomePlanet)) +\n  geom_col() +\n  labs(title = \"Missing HomePlanet by Deck\", x = \"Deck\", y = \"Number of passengers\") +\n  scale_y_continuous(breaks = seq(0, 9000, 200)) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\ntrain2 %>%\n  ggplot(., mapping = aes(x = HomePlanet, y = PassengerCount, fill = VIP)) +\n  geom_col() +\n  labs(title = \"Missing VIP by HomePlanet\", x = \"HomePlanet\", y = \"Number of passengers\") +\n  scale_y_continuous(breaks = seq(0, 9000, 200)) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\ntrain2 %>%\n  ggplot(., mapping = aes(x = Age, y = TotalSpent)) +\n  geom_col() +\n  labs(title = \"Missing Age\", x = \"Age of passengers\", y = \"TotalSpent\") +\n  scale_y_continuous(breaks = seq(0, 500000, 50000)) +\n  scale_x_continuous(breaks = seq(0, 100, 2)) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))"},{"path":"handle-missing-data.html","id":"replace-data-with-manual-rules","chapter":"3 Handle missing data","heading":"3.3 Replace data with manual rules","text":"’ve summed replacement rules function .see ’ve managed halv amount missing values many variables manual replacements. rest can handle imputation.","code":"\nmy_na_replace <- function(d) {\n  d2 <- d %>%\n    # Replace HomePlanet for passengers in groups where the homeplanet is known from the other passengers\n    group_by(PassengerGroup) %>% \n    fill(HomePlanet, .direction = \"downup\") %>% \n    \n    # Replace Cabin by group cabin for groups with group count > 1. Update the Deck, CabinNumber and Side variables.\n    mutate(Cabin2 = Cabin) %>%\n    fill(data = ., Cabin2, .direction = \"downup\") %>%\n    ungroup() %>%\n    mutate(Cabin = if_else(is.na(Cabin) & PassengerGroupSize != 1, Cabin2, Cabin),\n           Deck = str_split_i(Cabin, \"/\", 1),\n           CabinNumber = str_split_i(Cabin, \"/\", 2),\n           Side = str_split_i(Cabin, \"/\", 3)) %>%\n    select(-Cabin2) %>%\n    \n    # Replace HomePlanet if the passenger is housed on a dedicated Deck\n    mutate(HomePlanet = if_else(is.na(HomePlanet) & Deck == \"G\", \"Earth\", HomePlanet),\n           HomePlanet = if_else(is.na(HomePlanet) & Deck %in% c(\"A\", \"B\", \"C\"), \"Europa\", HomePlanet)) %>%\n    \n    # Replace all of VIPs from Earth to FALSE\n    mutate(VIP = if_else(is.na(VIP) & HomePlanet == \"Earth\", \"False\", VIP)) %>%\n    \n    # Replace amenities with zero if CryoSleep is TRUE or if Age <= 12\n    # Replace CryoSleep with FALSE if the passenger has spent credits\n    mutate(across(.cols = c(RoomService, FoodCourt, ShoppingMall, Spa, VRDeck), \n                  .fns = ~ if_else(condition = CryoSleep == \"True\" | Age <= 12, true = 0, false = .x, missing = .x)),\n           CryoSleep = if_else(TotalSpent > 0 & is.na(CryoSleep), \"False\", CryoSleep))\n  return(d2)\n}\n\ntrain3 <- my_na_replace(train2)\ntrain3 <- useful_features(train3)\n\nplot_na_hclust(train3)"},{"path":"handle-missing-data.html","id":"replace-data-with-imputation","chapter":"3 Handle missing data","heading":"3.4 Replace data with imputation","text":"algorithms require missing data since find patterns missing data, can use imputation algorithms rest. ’ve used two: MissRanger KNN. Since KNN can sensitive scale data outliers, ’ve normalized numeric data avoid possible issues imputation.Note ’ve saved results imputations since can take time run.\n### MissRanger - Chained Random ForestsThe missRanger algorithm can impute variables time option select id variable included data used imputation. Note excluded Name variables improve computation time.","code":"\nrev_normalization <- function(v, rec) { # Custom function that will \"unnormalise\" numeric values inside mutate(across())\n  tidy_rec <- tidy(rec, number = 1)\n  v2 <- v * filter(tidy_rec, terms == cur_column() & statistic == \"sd\")$value + \n    filter(tidy_rec, terms == cur_column() & statistic == \"mean\")$value\n  v3 <- round(v2, 0)\n  return(v3)\n}\n\n# ranger_norm <- recipe(Transported ~ ., data = train3) %>%\n#   step_normalize(all_numeric_predictors()) %>%\n#   prep()\n# \n# train3_ranger <- ranger_norm %>%\n#   bake(new_data = NULL) %>%\n#   select(-c(Cabin, Name, LastName, PassengerCount))\n# \n# ranger <- missRanger(train3_ranger, formula = . ~ . -PassengerId, seed = 8584, verbose = 0)\n# \n# ranger_unnorm <- ranger %>%\n#   mutate(across(.cols = c(Age, CabinNumber, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck),\n#                 .fns = ~ rev_normalization(.x, ranger_norm)))\n# \n# ranger2 <- train3 %>%\n#   select(PassengerId, Cabin, Name, LastName, PassengerCount) %>%\n#   left_join(ranger_unnorm, ., by = \"PassengerId\")\n# \n# save(ranger2, file = \"MissRanger.RData\")\nload(\"MissRanger.RData\")"},{"path":"handle-missing-data.html","id":"knn---k-nearest-neighbors","chapter":"3 Handle missing data","heading":"3.4.1 KNN - K-Nearest Neighbors","text":"use recipe-package set process imputation step_impute_knn-function allows specify variables used imputation imputed.KNN-algorithm fast can impute multiple variables time. can also relatively easily tuned changing number neighbors imputation based although ’ve used default = 5.","code":"\ntrain3_for_knn <- train3 %>%\n  mutate(across(.cols = where(is.factor), .fns = as.character))\n\nvars_to_impute <- c(\"HomePlanet\", \"CryoSleep\", \"Destination\", \"Age\", \"VIP\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\",\n                    \"Spa\", \"VRDeck\", \"Deck\", \"Side\", \"CabinNumber\", \"LastName\")\nvars_for_imputing <- c(\"HomePlanet\", \"CryoSleep\", \"Destination\", \"Age\", \"VIP\", \"RoomService\", \"FoodCourt\",\n                              \"ShoppingMall\", \"Spa\", \"VRDeck\", \"PassengerGroup\", \"Deck\", \"Side\", \"CabinNumber\",\n                              \"PassengerGroupSize\", \"DestinationsPerGroup\", \"CabinsPerGroup\",\n                              \"CryoSleepsPerGroup\", \"VIPsPerGroup\", \"LastNamesPerGroup\")\n\ntrain3_noNA <- train3_for_knn[complete.cases(train3_for_knn),]\n  \nknn_impute_rec <- recipe(Transported ~ ., data = train3_noNA) %>%\n  step_normalize(Age, CabinNumber, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck) %>%\n  step_impute_knn(recipe = ., all_of(vars_to_impute), impute_with = imp_vars(all_of(vars_for_imputing)), neighbors = 5) \n\nset.seed(8584)\nknn_impute_prep <- knn_impute_rec %>% prep(strings_as_factors = FALSE)\n\nset.seed(8584)\nknn_impute_bake <- bake(knn_impute_prep, new_data = train3_for_knn)\n\nknn_impute_res <- knn_impute_bake %>%\n  mutate(across(.cols = c(Age, CabinNumber, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck),\n                .fns = ~ rev_normalization(.x, knn_impute_prep)))"},{"path":"handle-missing-data.html","id":"comparison-of-imputation-algorithms---numerical-variables","chapter":"3 Handle missing data","heading":"3.4.2 Comparison of imputation algorithms - numerical variables","text":"use skimr-package set metrics evaluate data wrangling imputated variables can compared non-imputed ones.\nFigure 3.6: Comparisson standard deviation numerical values imputation\nstandard deviation numeric variables isn’t affected imputation, although expected since 1% missing data. true mean, can seen 3.7.\nFigure 3.7: Comparisson mean numerical values imputation\n","code":"\nmy_skim <- skimr::skim_with(numeric = skimr::sfl(min = ~min(., na.rm = TRUE), median = ~median(., na.rm = TRUE), \n                                   mean = ~mean(., na.rm = TRUE), max = ~max(., na.rm = TRUE), \n                                   sd = ~sd(., na.rm = TRUE)), append = FALSE)\n\nno_imp_skim <- train3 %>%\n  select(Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, CabinNumber) %>%\n  my_skim(.)\n\nranger_skim <- ranger2 %>%\n  select(Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, CabinNumber) %>%\n  my_skim(.)\n\nknn_skim <- knn_impute_res %>%\n  select(Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, CabinNumber) %>%\n  my_skim(.)\n\nsd_skim <- bind_cols(\"Variable\" = no_imp_skim$skim_variable, \"No imp\" = no_imp_skim$numeric.sd, \n                     \"MissRanger\" = ranger_skim$numeric.sd,  \"KNN\" = knn_skim$numeric.sd) %>%\n  pivot_longer(., cols = !Variable, names_to = \"Metric\", values_to = \"Standard_deviation\")\n\nmean_skim <- bind_cols(\"Variable\" = no_imp_skim$skim_variable, \"No imp\" = no_imp_skim$numeric.mean, \n                     \"missRanger\" = ranger_skim$numeric.mean, \"KNN\" = knn_skim$numeric.mean) %>%\n  pivot_longer(., cols = !Variable, names_to = \"Metric\", values_to = \"Mean\")\n\nsd_skim %>%\n  filter(Metric != \"No imp\") %>%\n  ggplot(., mapping = aes(x = Metric, y = Standard_deviation)) +\n  geom_point() +\n  geom_hline(data = sd_skim %>% filter(Metric == \"No imp\"), aes(yintercept = Standard_deviation, colour = \"orange\")) +\n  lims(y = c(0, NA)) +\n  facet_wrap(~Variable, scales = \"free\") +\n  scale_colour_discrete(name = \"Non-imputed\", labels = \"sd\", type = \"orange\")\nmean_skim %>%\n  filter(Metric != \"No imp\") %>%\n  ggplot(., mapping = aes(x = Metric, y = Mean)) +\n  facet_wrap(~Variable, scales = \"free\") +\n  geom_point() +\n  geom_hline(data = mean_skim %>% filter(Metric == \"No imp\"), aes(yintercept = Mean, colour = \"green\")) +\n  lims(y = c(0, NA)) +\n  scale_colour_discrete(name = \"Non-imputed\", labels = \"mean\", type = \"green\")"},{"path":"handle-missing-data.html","id":"comparison-of-imputation-algorithms---categorical-variables","chapter":"3 Handle missing data","heading":"3.4.3 Comparison of imputation algorithms - categorical variables","text":"comparison proportions categorical variables also shows imputation seems reasonable hasn’t introduced strange values like outliers. two sample comparisons HomePlanet CryoSleep encourage (follow along code) explore categorical variable seperately.\nFigure 3.8: Imputed distribution HomePlanet compared imputation\n\nFigure 3.9: Imputed distribution HomePlanet compared imputation\n","code":"\nmy_prop_plot <- function(df, v, t) {\n  g <- ggplot(data = df, mapping = aes(x = !!sym(t), fill = !!sym(v))) +\n    geom_bar() +\n    geom_text(aes(by = as.factor(!!sym(t))), stat = \"prop\", position = position_stack(vjust = 0.5)) +\n    labs(title = deparse(substitute(df)), x = t, y = \"Number of passengers\") +\n    scale_y_continuous(breaks = seq(0, 9000, 500)) +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))\n  return(g)\n}\n\nh1 <- my_prop_plot(train3, \"HomePlanet\", \"Transported\")\nh2 <- my_prop_plot(ranger2, \"HomePlanet\", \"Transported\")\nh3 <- my_prop_plot(knn_impute_res, \"HomePlanet\", \"Transported\")\n\n(h1 | h2 | h3) + plot_layout(guides = \"collect\", axis_titles = \"collect\")\nc1 <- my_prop_plot(train3, \"CryoSleep\", \"Transported\")\nc2 <- my_prop_plot(ranger2, \"CryoSleep\", \"Transported\")\nc3 <- my_prop_plot(knn_impute_res, \"CryoSleep\", \"Transported\")\n\n(c1 | c2 | c3) + plot_layout(guides = \"collect\", axis_titles = \"collect\")"},{"path":"handle-missing-data.html","id":"do-the-imputated-values-break-any-rules","chapter":"3 Handle missing data","heading":"3.4.4 Do the imputated values break any “rules”?","text":"One last thing must accept imputed values check whether imputation managed adhere ‘rules’ discovered earlier used manual replacement NA-values. use modified version useful_features-function update simple features.imputation didn’t seem break rule every passenger group travels planet, great.also didn’t break rule passengers cryo sleep <=12 years old don’t spend credits. Great!rule breaking regarding Deck variable passengers imputed housed decks despite travelling ‘wrong’ homeplanet deck. can deal general function checks ‘rules’ adjusts.\nFigure 3.10: Missing values overview imputation\nmissing values now variables won’t use modelling (’ll use features derived instead).","code":"\nuseful_features2 <- function(x) {\n  x2 <- x %>%\n    mutate(TotalSpent = RoomService + FoodCourt + ShoppingMall + Spa + VRDeck) %>%\n    group_by(PassengerGroup) %>%\n    add_count(PassengerGroup, name = \"PassengerGroupSize\") %>%\n    mutate(HomePlanetsPerGroup = n_distinct(HomePlanet, na.rm = TRUE),\n           DestinationsPerGroup = n_distinct(Destination, na.rm = TRUE),\n           CabinsPerGroup = n_distinct(Cabin, na.rm = TRUE),\n           TotalSpentPerGroup = sum(TotalSpent, na.rm = TRUE),\n           CryoSleepsPerGroup = n_distinct(CryoSleep, na.rm = TRUE),\n           VIPsPerGroup = n_distinct(VIP, na.rm = TRUE),\n           LastNamesPerGroup = n_distinct(LastName, na.rm = TRUE)) %>%\n    ungroup() %>%\n    mutate(across(.cols = c(HomePlanet, CryoSleep, Destination, VIP, Transported, Deck, Side, HomePlanetsPerGroup,\n                            PassengerGroupSize, DestinationsPerGroup, CabinsPerGroup, CryoSleepsPerGroup, VIPsPerGroup,\n                            LastNamesPerGroup, PassengerGroup, PassengerId),\n                  .fns = as.factor)) %>%\n    mutate(across(.cols = c(CabinNumber, Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck),\n                  .fns = as.integer))\n  return(x2)\n}\n\ncheck_knn <- useful_features2(knn_impute_res)\n\ncheck_knn %>%\n  filter(HomePlanetsPerGroup != 1) %>%\n  select(PassengerId, HomePlanet, HomePlanetsPerGroup)\n#> # A tibble: 0 × 3\n#> # ℹ 3 variables: PassengerId <fct>, HomePlanet <fct>,\n#> #   HomePlanetsPerGroup <fct>\ncheck_knn %>%\n  filter(Age <= 12 & TotalSpent > 0) %>%\n  select(PassengerId, CryoSleep, Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, TotalSpent)\n#> # A tibble: 0 × 9\n#> # ℹ 9 variables: PassengerId <fct>, CryoSleep <fct>,\n#> #   Age <int>, RoomService <int>, FoodCourt <int>,\n#> #   ShoppingMall <int>, Spa <int>, VRDeck <int>,\n#> #   TotalSpent <dbl>\n\ncheck_knn %>%\n  filter(CryoSleep == TRUE & TotalSpent > 0) %>%\n  select(PassengerId, CryoSleep, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, TotalSpent)\n#> # A tibble: 0 × 8\n#> # ℹ 8 variables: PassengerId <fct>, CryoSleep <fct>,\n#> #   RoomService <int>, FoodCourt <int>, ShoppingMall <int>,\n#> #   Spa <int>, VRDeck <int>, TotalSpent <dbl>\ncheck_knn %>%\n  filter(Deck %in% c(\"A\", \"B\", \"C\") & HomePlanet != \"Europa\" | Deck == \"G\" & HomePlanet != \"Earth\") %>%\n  select(PassengerId, HomePlanet, Deck)\n#> # A tibble: 15 × 3\n#>    PassengerId HomePlanet Deck \n#>    <fct>       <fct>      <fct>\n#>  1 0101_01     Mars       G    \n#>  2 0380_01     Europa     G    \n#>  3 0833_01     Europa     G    \n#>  4 0932_01     Mars       B    \n#>  5 1134_01     Europa     G    \n#>  6 2900_01     Mars       G    \n#>  7 3553_01     Mars       G    \n#>  8 4130_01     Mars       G    \n#>  9 4397_01     Earth      C    \n#> 10 4667_01     Earth      C    \n#> 11 6451_01     Mars       G    \n#> 12 7746_01     Europa     G    \n#> 13 7843_01     Mars       C    \n#> 14 7983_01     Europa     G    \n#> 15 7995_01     Europa     G\n\ncheck_knn %>%\n  filter(VIP == TRUE & HomePlanet == \"Earth\") %>%\n  select(PassengerId, HomePlanet, VIP)\n#> # A tibble: 0 × 3\n#> # ℹ 3 variables: PassengerId <fct>, HomePlanet <fct>,\n#> #   VIP <fct>\nfix_knn <- function(df) {\n  amenities_summary <- df %>%\n    filter(Age > 12 & TotalSpent > 0) %>%\n    summarise(mean_age = round(mean(Age), 0), .by = c(CryoSleep, Deck))\n  \n  wrong_age <- df %>%\n    filter(Age <= 12 & TotalSpent > 0) %>%\n    left_join(., amenities_summary, by = c(\"CryoSleep\", \"Deck\")) %>%\n    select(PassengerId, mean_age)\n  \n  wrong_planet <- df %>%\n    filter(HomePlanetsPerGroup != 1) %>%\n    select(PassengerId, PassengerGroup, HomePlanet) %>%\n    group_by(PassengerGroup) %>%\n    mutate(HomePlanet_correct = first(HomePlanet)) %>%\n    ungroup() %>%\n    select(PassengerId, HomePlanet_correct)\n  \n  wrong_deck <- df %>%\n    filter(Deck %in% c(\"A\", \"B\", \"C\", \"G\")) %>%\n    mutate(Deck_correct = case_when(Deck %in% c(\"A\", \"B\", \"C\") & HomePlanet == \"Earth\" ~ \"G\",\n                                    Deck %in% c(\"A\", \"B\", \"C\") & HomePlanet == \"Mars\" ~ \"F\",\n                                    Deck == \"G\" & HomePlanet == \"Mars\" ~ \"F\",\n                                    Deck == \"G\" & HomePlanet == \"Europa\" ~ \"C\", .default = Deck)) %>%\n    select(PassengerId, Deck_correct)\n  \n  res <- df %>%\n  left_join(., wrong_age, by = \"PassengerId\") %>%\n  left_join(., wrong_planet, by = \"PassengerId\") %>%\n  left_join(., wrong_deck, by = \"PassengerId\") %>%\n  mutate(Age = if_else(Age <= 12 & TotalSpent > 0, mean_age, Age),\n         HomePlanet = if_else(HomePlanetsPerGroup != 1, HomePlanet_correct, HomePlanet),\n         Deck_correct = coalesce(Deck_correct, Deck),\n         Deck = Deck_correct) %>%\n  select(-mean_age, -HomePlanet_correct, -Deck_correct)\n  \n  return(res)\n}\n\nfixed_knn <- fix_knn(check_knn)\ntrain4 <- useful_features2(fixed_knn)\n\nplot_na_hclust(train4)"},{"path":"exploration-of-categorical-variables.html","id":"exploration-of-categorical-variables","chapter":"4 Exploration of categorical variables","heading":"4 Exploration of categorical variables","text":"","code":""},{"path":"exploration-of-categorical-variables.html","id":"chi-squared-tests","chapter":"4 Exploration of categorical variables","heading":"4.1 Chi-squared tests","text":"traditional statistical test two categorical values chi^2 test, compares actual frequencies combination categorical variables perfectly independent frequency (frequency occur combination ratio overall ratio one variable). null hypothesis chi^2 test relationship variables.use set helper functions iterate different categorical variables summarize results tables.use typical 95% confidence interval cut-point comparison, can see three pairs variables p-values higher 5%, means variables independent . pairs involve Side feature perhaps suggests people VIP tickets cryo sleep weren’t clustered one side ship, well people travelling certain destination.simple comparison might suggest possible interactions effects variables consider. Let’s visualise variables related .","code":"\nmy_chisq_test <- function(var1, var2) { # Test and extract Chi^2 statistics\n  res <- rstatix::chisq_test(x = var1, y = var2)\n  ex_res <- c(chi = res$statistic, pval = res$p)\n  return(ex_res)\n}\n\nchi_sqr_res <- train4 %>%\n  select(HomePlanet, Destination, Deck, Side, CryoSleep, VIP, LastName) %>%\n  summarise(chi_stats = map(.x = across(everything()), .f = \\(x) map(.x = across(everything()), .f = \\(y) my_chisq_test(x, y))))\n\nchi_sqr_res2 <- chi_sqr_res %>%\n  unnest() %>%\n  unnest_wider(col = everything()) %>%\n  rename(chi_sqr = `chi.X-squared`, p_value = pval) %>%\n  bind_cols(expand.grid(Var1 = c(\"HomePlanet\", \"Destination\", \"Deck\", \"Side\", \"CryoSleep\", \"VIP\", \"LastName\"), \n                        Var2 = c(\"HomePlanet\", \"Destination\", \"Deck\", \"Side\", \"CryoSleep\", \"VIP\", \"LastName\")), .)\n\nchi_sqr_res2 %>%\n  select(Var1, Var2, chi_sqr, p_value) %>%\n  filter(p_value > 0.05 & Var1 != Var2) %>%\n  distinct(chi_sqr, .keep_all = TRUE) %>%\n  arrange(as.character(Var1), desc(p_value))\n#>        Var1        Var2   chi_sqr p_value\n#> 1 CryoSleep        Side 3.7295140  0.0535\n#> 2      Side Destination 1.2464848  0.5360\n#> 3       VIP        Side 0.5087764  0.4760"},{"path":"exploration-of-categorical-variables.html","id":"mosaic-plots","chapter":"4 Exploration of categorical variables","heading":"4.2 Mosaic plots","text":"use custom function Kuhn’s Johnsson’s code improve default mosaic plot plot categorical variables help contigency tables pair. ’ve left LastName since many levels.\nFigure 4.1: Mosaic plots visualize relationships categorical variables\nmosaic plot can read horisontally can see height bars changes different (vertical) values variable. example, see share passengers cryosleep changes deck know Chi^2 test change significant.can return visual understanding relationship categorical variables whenever want check relationship metric makes sense. example, can see differences Side variables didn’t fall outside 5% p-value, like HomePlanet, seem relatively small.","code":"\nmy_mosaic <- function(dtab, i) {\n  png(filename = paste0(\"Mosaic\", i, \".png\"))\n  vcd::mosaic(dtab, pop = FALSE, highlighting = TRUE, highlighting_fill = colorspace::rainbow_hcl,\n            margins = unit(c(6, 1, 1, 8), \"lines\"),\n            labeling = vcd::labeling_border(rot_labels = c(90, 0, 0, 0), just_labels = c(\"left\", \"right\", \"center\", \"right\"),\n                                       offset_varnames = unit(c(3, 1, 1, 4), \"lines\")), keep_aspect_ratio = FALSE)\n  dev.off()\n}\n\nmy_tables <- list(\n  base::table(HomePlanet = train4$HomePlanet, Destination = train4$Destination),\n  base::table(HomePlanet = train4$HomePlanet, CryoSleep = train4$CryoSleep),\n  base::table(HomePlanet = train4$HomePlanet, Deck = train4$Deck),\n  base::table(HomePlanet = train4$HomePlanet, Side = train4$Side),\n  base::table(HomePlanet = train4$HomePlanet, VIP = train4$VIP),\n  \n  base::table(Destination = train4$Destination, CryoSleep = train4$CryoSleep),\n  base::table(Destination = train4$Destination, Deck = train4$Deck),\n  base::table(Destination = train4$Destination, Side = train4$Side),\n  base::table(Destination = train4$Destination, VIP = train4$VIP),\n  \n  base::table(CryoSleep = train4$CryoSleep, Deck = train4$Deck),\n  base::table(CryoSleep = train4$CryoSleep, Side = train4$Side),\n  base::table(CryoSleep = train4$CryoSleep, VIP = train4$VIP),\n  \n  base::table(Deck = train4$Deck, Side = train4$Side),\n  base::table(Deck = train4$Deck, VIP = train4$VIP),\n  \n  base::table(Side = train4$Side, VIP = train4$VIP)\n)\n\nmosaic_plots <- map2(.x = my_tables, .y = 1:length(my_tables), .f = \\(t, it) my_mosaic(dtab = t, i = it))\n\nplotsToSVG <- map(1:length(mosaic_plots), .f = \\(i) paste0(\"Mosaic\", i, \".png\"))\nslickR::slickR(plotsToSVG, height = \"480px\", width = \"672px\") +\n  slickR::settings(slidesToShow = 1)"},{"path":"exploration-of-categorical-variables.html","id":"correspondence-analysis","chapter":"4 Exploration of categorical variables","heading":"4.3 Correspondence Analysis","text":"Correspondence analysis uses residuals contingency table two variables compared frequency table variables ’ve unrelated . 4.2 shows correspondence analysis Destination Deck.\nFigure 4.2: Correspondance analysis Destination Deck variables.\nway read plot follows:horizontal vertical components show amount Chi^2 statistic respective component accounts . Another way think amount information original variables explained either horizontal vertical direction. eigenvalues inside parenthesis gives us sense component important. example, horizontal axis much important cases.horizontal vertical components show amount Chi^2 statistic respective component accounts . Another way think amount information original variables explained either horizontal vertical direction. eigenvalues inside parenthesis gives us sense component important. example, horizontal axis much important cases.distance origin either direction tells us rare particular category . example, Deck T five passengers ’s rare.distance origin either direction tells us rare particular category . example, Deck T five passengers ’s rare.Categories variable close together (particularly along horizontal axis case) suggest similarities categories. example, decks -C seem similar variation.Categories variable close together (particularly along horizontal axis case) suggest similarities categories. example, decks -C seem similar variation.Categories different variables close together suggest dependence especially distance origin large. trick see strength relationship imagine straight line one category orgin category. greater distance smaller angle, stronger relationship. example, draw line deck origin Cancri, get relatively long line tiny angle.Categories different variables close together suggest dependence especially distance origin large. trick see strength relationship imagine straight line one category orgin category. greater distance smaller angle, stronger relationship. example, draw line deck origin Cancri, get relatively long line tiny angle.can check insights make sense mosaic plot . example, decks -C seem unusually high proportion passengers travel Cancri decks D-G seem common travellers Trappist although relationship moderate.\nFigure 4.3: Correspondance analysis HomePlanet Deck variables.\nHomePlanet Deck, see strong relationship Europa decks -C well Earth deck G. might make sense create feature decks -C pooled single category potentially reduce noise data.","code":"\nplot(FactoMineR::CA(base::table(Deck = train4$Deck, Destination = train4$Destination), graph = FALSE), \n     title = \"Destination and Deck\")\nplot(FactoMineR::CA(base::table(Deck = train4$Deck, HomePlanet = train4$HomePlanet), graph = FALSE), \n     title = \"HomePlanet and Deck\")"},{"path":"exploration-of-categorical-variables.html","id":"relationships-between-categorical-variables-and-the-outcome","chapter":"4 Exploration of categorical variables","heading":"4.4 Relationships between categorical variables and the outcome","text":"far, ’ve looked relationships categorical predictors related response. can explore binomial proportion test (code heavily borrowed Kuhn Johnson). sense similar chi^2 test compares actual proportions within group given expected proportion, case proportion transported total number passengers.\nFigure 4.4: categorical variables plotted binomial plots confidence intervals. red dotted line shows average probability response across entire training set.\nsee lot interesting details let us focus features relate passenger group.\nFigure 4.5: PassengerGroupSize plotted binomial plot confidence intervals. red dotted line shows average probability response across entire training set.\n4.5 suggests might difference passengers travel alone don’t. Perhaps binary feature tracks whether passenger travels solo within group benefit. Also, perhaps large groups less likely transported, although don’t much data .\nFigure 4.6: DestinationsPerGroup plotted binomial plot confidence intervals. red dotted line shows average probability response across entire training set.\n4.6 suggests might slight difference groups passengers travelled destination didn’t. differences small, presence feature might improve models.Let’s create function add new features ’ve discovered data.Let’s explore VIP variables bit . ?? suggests chances transported higher passengers group another passenger VIP ticket time VIP ticket seems reduce chance transported.\nFigure 4.7: closer look VIP variable relation response.\n\nFigure 4.8: closer look VIP variable relation response.\nBased can see 4.9, passengers groups one passenger VIP-ticket likely transported. can related fact VIP-passengers spend amenities , turn, might increase chances transported.explore effects numerical variables next chapter.\nFigure 4.9: closer look VIP VIPsPerGroup response.\n","code":"\nresponse_rate = mean(train4$Transported == \"True\")\n\nmy_binom <- function(df, t) { # Test whether the differences in proportions (of transported) are significant\n  p <- infer::prop_test(x = df, response = !!sym(t))\n  df2 <- df %>%\n    mutate(Lower = p$lower_ci, \n           Upper = p$upper_ci,\n           Proportion = sum(!!sym(t) == \"True\") / length(!!sym(t)))\n  return(df2)\n}\n\nmy_binom_plot <- function(df, v, t) { # Applies the proportion test to different categories of a variable and plots the results\n  p <- df %>%\n    group_split(!!sym(v)) %>%\n    map(.x = ., .f = \\(df) my_binom(df, t)) %>%\n    bind_rows() %>%\n    mutate(my_var = reorder(!!sym(v), Proportion)) %>%\n    ggplot(., aes(x = my_var, y = Proportion)) +\n    geom_errorbar(aes(ymin = 1 - Lower, ymax = 1 - Upper), width = .1) +\n    geom_point() +\n    geom_hline(yintercept = response_rate, col = \"red\", alpha = .8, lty = 4) + \n    scale_y_continuous(breaks = seq(0, 1, 0.1), limits = c(0, 1)) +\n    labs(x = \"\", title = v)\n  \n  return(p)\n}\nsave_plot <- function(p, i) {\n  ggsave(filename = paste0(\"Binomial\", i, \".png\"), plot = p)\n}\n\nbinom_plots <- train4 %>%\n  select(where(is.character), where(is.factor)) %>%\n  summarise(across(.cols = -c(Cabin, Name, LastName, PassengerId, Transported, PassengerGroup), .fns = list)) %>%\n  imap(.f = ~my_binom_plot(train4, .y, \"Transported\"))\n\nplotsToSVG <- walk2(.x = binom_plots, .y = seq_along(binom_plots), .f = save_plot)\nplotsToSVG <- map(1:length(binom_plots), .f = \\(i) paste0(\"Binomial\", i, \".png\"))\nslickR::slickR(plotsToSVG, height = \"480px\", width = \"672px\") +\n  slickR::settings(slidesToShow = 1)\nmy_binom_plot(train4, \"PassengerGroupSize\", \"Transported\")\nmy_binom_plot(train4, \"DestinationsPerGroup\", \"Transported\")\nadd_grp_features <- function(df) {\n  res <- df %>%\n  mutate(Solo = if_else(PassengerGroupSize == 1, 1, 0),\n         LargeGroup = if_else(as.integer(PassengerGroupSize) > 7, 1, 0),\n         TravelTogether = if_else(DestinationsPerGroup == 1, 1, 0))\n}\n\ntrain5 <- add_grp_features(train4)\nmy_binom_plot(train5, \"VIP\", \"Transported\")\nmy_binom_plot(train5, \"VIPsPerGroup\", \"Transported\")\ng1 <- train5 %>%\n  select(VIP, Transported, VIPsPerGroup) %>%\n  filter(VIPsPerGroup == 2) %>%\n  ggplot(., mapping = aes(x = VIP, fill = Transported)) +\n  geom_bar() +\n  geom_text(aes(by = VIP), stat = \"prop\", position = position_stack(vjust = 0.5)) +\n  facet_wrap(~VIPsPerGroup, labeller = \"label_both\") +\n  labs(title = \"Two VIPsPerGroup\", x = \"VIP\", y = \"Number of passengers\") + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1), text = element_text(size = 10), plot.title = element_text(size = 10))\n\ng2 <- train5 %>%\n  select(VIP, Transported) %>%\n  ggplot(., mapping = aes(x = VIP, fill = Transported)) +\n  geom_bar() +\n  geom_text(aes(by = VIP), stat = \"prop\", position = position_stack(vjust = 0.5)) +\n  labs(title = \"VIP share total\", x = \"VIP\", y = \"Number of passengers\") + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1), text = element_text(size = 10), plot.title = element_text(size = 10))\n\n(g1 + g2) + \n  plot_layout(guides = \"collect\", axis_titles = \"collect\")"},{"path":"exploration-of-numerical-variables.html","id":"exploration-of-numerical-variables","chapter":"5 Exploration of numerical variables","heading":"5 Exploration of numerical variables","text":"’ve far focused categorical variables now turn numerical variables see affect response.","code":""},{"path":"exploration-of-numerical-variables.html","id":"name-variable","chapter":"5 Exploration of numerical variables","heading":"5.1 Name variable","text":"First, however, let’s transform name variable numerical values. can assume rank order names based fact variable distribution uniform (see 2.1). use simple encoding convert names numbers.Let’s also explore count values names visually see look response.\nFigure 5.1: Exploration counts LastName variable LastNameCount number passengers total share last name LastNamesPerGroup number passengers share last name within group.\nseem difference good news since allows us get use name variable original form didn’t seem much variance. Hopefully features also prove beneficial.","code":"\nencode_cat_to_numeric <- function(x) {\n  x <- factor(x, ordered = FALSE)\n  x <- unclass(x)\n  return(x)\n}\n\nadd_name_features <- function(df) {\n  res <- df %>%\n  mutate(LastNameAsNumber = encode_cat_to_numeric(LastName)) %>%\n  add_count(x = ., LastNameAsNumber, name = \"LastNameCount\") %>%\n  mutate(across(.cols = c(PassengerGroup, LastNameAsNumber), .fns = as.integer))\n}\n\ntrain6 <- add_name_features(train5)\ndf1 <- train6 %>%\n  select(Transported, LastNamesPerGroup) %>%\n  group_by(LastNamesPerGroup, Transported) %>%\n  summarise(count = n()) %>%\n  mutate(perc = count / sum(count))\n\ng1 <- ggplot(data = df1) +\n  geom_bar(aes(x = LastNamesPerGroup, y = perc*100, fill = Transported), stat = \"identity\") +\n  labs(y = \"Percent transported\")\n\ndf2 <- train6 %>%\n  select(Transported, LastNameCount) %>%\n  group_by(LastNameCount, Transported) %>%\n  summarise(count = n()) %>%\n  mutate(perc = count / sum(count))\n\ng2 <- ggplot(data = df2) +\n  geom_bar(aes(x = LastNameCount, y = perc*100, fill = Transported), stat = \"identity\") +\n  labs(y = \"Percent transported\")\n\ng1 + g2 + plot_layout(guides = \"collect\", axis_titles = \"collect_y\")"},{"path":"exploration-of-numerical-variables.html","id":"age-cabinnumber-and-lastnames","chapter":"5 Exploration of numerical variables","heading":"5.2 Age, CabinNumber and LastNames","text":"Kuhn Johnson propose smoothing function using gam mgcv-package fit generalized additive models numeric variables response. ’ve modified function somewhat.\nFigure 5.2: General additive model (GAM) plots Age smoothed confidence intervals. red dotted line shows average probability response across entire training set.\nPassengers younger 15 years age seem higher chance transported, effect significant younger passenger . rest passengers, age doesn’t seem matter much.\nFigure 5.3: General additive model (GAM) plots PassengerGroup smoothed confidence intervals. red dotted line shows average probability response across entire training set.\nPassenger groups seem effect although variable correlated CabinNumber , turn, likely correlated Deck might explain differences\nFigure 5.4: General additive model (GAM) plots CabinNumber smoothed confidence intervals. red dotted line shows average probability response across entire training set.\nCabinNumber shows effect look Deck numbers seem make biggest difference, see cabins decks F G.\nFigure 5.5: General additive model (GAM) plots LastNameAsNumber smoothed confidence intervals. red dotted line shows average probability response across entire training set.\nLastName seemes effect.","code":"\nmy_smooth_plot <- function(d, v) {\n  TransportedRate <- mean(d$Transported == \"True\")\n  \n  my_df <- d %>% \n    select(!!sym(v), Transported) %>%\n    arrange(!!sym(v))\n  \n  my_df_small <- my_df %>%\n    distinct(!!sym(v))\n\n  gam_model <- mgcv::gam(as.formula(paste(\"Transported\", \"~ s(\", v, \")\")), data = my_df, family = binomial())\n \n  my_df_small <- my_df_small %>%\n    mutate(\n      link = predict(gam_model, my_df_small, type = \"link\"),\n      se = predict(gam_model, my_df_small, type = \"link\", se.fit = TRUE)$se.fit,\n      upper = link + qnorm(.975) * se,\n      lower = link - qnorm(.975) * se,\n      lower = binomial()$linkinv(lower),\n      upper = binomial()$linkinv(upper),\n      probability = binomial()$linkinv(link)\n    )\n  \n  g <- ggplot(my_df_small, aes(x = !!sym(v))) + \n        geom_line(aes(y = probability)) + \n        geom_ribbon(aes(ymin = lower, ymax = upper), fill = \"grey\", alpha = .5) + \n        geom_hline(yintercept = TransportedRate, col = \"red\", alpha = .8, lty = 4)  + \n        scale_y_continuous(breaks = seq(0, 1, 0.1), limits = c(0, 1)) +\n        labs(x = v, title = paste(\"Smoothed\", v))\n\n  return(g)\n}\n\nmy_smooth_plot(train6, \"Age\")\nmy_smooth_plot(train6, \"PassengerGroup\")\nmy_smooth_plot(train6, \"CabinNumber\")\nmy_smooth_plot(train6, \"LastNameAsNumber\")"},{"path":"exploration-of-numerical-variables.html","id":"amenities","chapter":"5 Exploration of numerical variables","heading":"5.3 Amenities","text":"amenity variables zero might reflect fact passenger cryosleep let’s filter .\nFigure 5.6: General additive model (GAM) plots RoomService smoothed confidence intervals. red dotted line shows average probability response across entire training set.\n\nFigure 5.7: General additive model (GAM) plots VRDeck smoothed confidence intervals. red dotted line shows average probability response across entire training set.\n\nFigure 5.8: General additive model (GAM) plots Spa smoothed confidence intervals. red dotted line shows average probability response across entire training set.\nPassengers spent credits RoomService, Spa VRDeck seem less likely transported.\nFigure 5.9: General additive model (GAM) plots ShoppingMall smoothed confidence intervals. red dotted line shows average probability response across entire training set.\n\nFigure 5.10: General additive model (GAM) plots FoodCourt smoothed confidence intervals. red dotted line shows average probability response across entire training set.\nPassengers spent credits FoodCourt ShoppingMall seem instead likely transported\nFigure 5.11: General additive model (GAM) plots TotalSpent smoothed confidence intervals. red dotted line shows average probability response across entire training set.\nTotalSpent variable probably smooths effects individual amenities might reduce model performance. prepared remove models.","code":"\nmy_smooth_plot(train6 %>% filter(CryoSleep == \"False\"), \"RoomService\")\nmy_smooth_plot(train6 %>% filter(CryoSleep == \"False\"), \"VRDeck\")\nmy_smooth_plot(train6 %>% filter(CryoSleep == \"False\"), \"Spa\")\nmy_smooth_plot(train6 %>% filter(CryoSleep == \"False\"), \"ShoppingMall\")\nmy_smooth_plot(train6 %>% filter(CryoSleep == \"False\"), \"FoodCourt\")\nmy_smooth_plot(train6 %>% filter(CryoSleep == \"False\"), \"TotalSpent\")"},{"path":"exploration-of-numerical-variables.html","id":"correlations","chapter":"5 Exploration of numerical variables","heading":"5.4 Correlations","text":"can see new numerical variables correlate highly others.PassengerGroup CabinNumber relatively correlated cause problems models sensitive correlated variables. consider using one variable see one gives better results.","code":"\ntrain6 %>%\n  select(Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, CabinNumber, LastNameAsNumber, PassengerGroup) %>%\n  DataExplorer::plot_correlation(., type = \"continuous\", geom_text_args = list(size = 10), \n                   theme_config = list(text = element_text(size = 15), axis.text.x = element_text(angle = 90)))"},{"path":"interactions.html","id":"interactions","chapter":"6 Interactions","heading":"6 Interactions","text":"far ’ve created features single variables effects two variables together? help models added interaction effect example HomePlanet Destination create new feature HomeDestination? interactions?previous competition Titanic 1912, sex passenger mattered (women likely survive) ticket class mattered (first class likely survive) interaction “woman first class” almost 100% survival interaction improved model (remember correctly). space odyssey, want discover interactions exist variables .course, won’t know extent improvement test interaction effects various models. models inherently discover interactions (like tree-models) addition interaction effects might matter. might matter models might least improve computation time.","code":""},{"path":"interactions.html","id":"visual-exploration-of-interactions","chapter":"6 Interactions","heading":"6.1 Visual exploration of interactions","text":"Since categorical variables, let’s visualize possible interactions. use default glm logistic regression model formula Transported ~ (.)^2 amounts Outcome ~ Variable_1 + Variable_2 + Variable_1 x Variable_2Parallel lines indicate significant interaction effects lines cross indicate potential significant interactions. ’ll highlight interactions .\nFigure 6.1: Interaction CryoSleep HomePlanet\ninteraction CryoSleep Deck suggests passengers E G decks seem less likely transported cryosleep reverse true passengers decks D F.\nFigure 6.2: Interaction Deck Side\ninteraction Deck Side, however, seems show minor effect, .","code":"\nplot_simple_int <- function(df, v1, v2) {\n  tmp_vars <- c(\"Transported\", v1, v2)\n  tmp_model <- glm(Transported ~ (.)^2, data = df[, tmp_vars], family = binomial())\n  p <- interactions::cat_plot(tmp_model, pred = {{ v1 }}, modx = {{ v2 }}, geom = \"line\", colors = c25,\n                       main.title = paste(v1, \"and\", v2))\n  return(p)\n}\n\npreds_cat <- c(\"CryoSleep\", \"HomePlanet\", \"Destination\", \"VIP\", \"Deck\", \"Side\")\npairs <- combn(preds_cat, 2, simplify = FALSE)\nc25 <- c(\"dodgerblue2\", \"#E31A1C\", \"green4\", \"#6A3D9A\", \"#FF7F00\", \"black\", \"gold1\", \"skyblue2\", \"#FB9A99\", \"palegreen2\", \"#CAB2D6\",\n  \"#FDBF6F\", \"gray70\", \"khaki2\", \"maroon\", \"orchid1\", \"deeppink1\", \"blue1\", \"steelblue4\", \"darkturquoise\", \"green1\", \"yellow4\",\n  \"yellow3\", \"darkorange4\", \"brown\") # Had to add extra colours because I couldn't get `cat_plot` to work with defaults\n\nsave_plot <- function(p, i) {\n  ggsave(filename = paste0(\"PairInt\", i, \".png\"), plot = p)\n}\n\npair_int_plots <- pairs %>%\n  map(.x = ., .f = \\(vp) plot_simple_int(train5, vp[1], vp[2]))\n\nplotsToSVG <- walk2(.x = pair_int_plots, .y = seq_along(pair_int_plots), .f = save_plot)\nplotsToSVG <- map(1:length(pair_int_plots), .f = \\(i) paste0(\"PairInt\", i, \".png\"))\nslickR::slickR(plotsToSVG, height = \"480\", width = \"672\") +\n  slickR::settings(slidesToShow = 1)\nplot_simple_int(train6, \"CryoSleep\", \"HomePlanet\")\nplot_simple_int(train6, \"Deck\", \"Side\")"},{"path":"interactions.html","id":"interaction-significance-with-only-variable-pairs-and-their-interactions","chapter":"6 Interactions","heading":"6.2 Interaction significance with only variable pairs and their interactions","text":"One problem visual approach don’t know interaction effects real - sense represent true relationship outcome - ’re called false positives - , random effects happened present dataset. example, fact passengers Earth seem less likely transported even cryosleep reflect true aspect spacetime anomaly caused transportation another dimension pattern just something happened time pure chance? Another way think : Spaceship Titanic pass spacetime anomaly thousand times, pattern persist?explore , must validate effects cross validation. still use simple glm model model response pair variables compare similar model includes interaction term. use accuracy metric comparison.p-value test tells us whether effects interaction terms large enough considered non-random. ’ve used cutoff 5% without adjustment. can problematic since tests run 5% cutoff, higher chance finding interactions effect purely chance (fact, 15 pairs 5% cutoff, chances getting false false positive increase 5% 54%). look adjustment methods pvalues take account later.now, can conclude 3 variable pairs seem improve model performance compared model without interaction effects. One CryoSleep Destination visual inspection earlier, effect doesn’t seem large, especially compared ones ’ve seen.","code":"\ncompare_models_1way <- function(a, b, metric = a$metric[1], ...) { # A customized compare_models function from caret that allows for\n  mods <- list(a, b)                                               # a custom t.test adjustment in the diff-function\n  rs <- resamples(mods)\n  diffs <- diff(rs, metric = metric[1], adjustment = \"none\", ...)\n  res <- diffs$statistics[[1]][[1]]\n  return(res)\n}\n\npair_model <- function(df, v1, v2) { # Model without interactions with only two variables\n  tmp_vars <- c(\"Transported\", v1, v2)\n  set.seed(8584)\n  m <- train(Transported ~ ., data = df[, tmp_vars], preProc = NULL, method = \"glm\", metric = \"accuracy\", trControl = ctrl)\n  return(m)\n}\n\npair_int_model <- function(df, v1, v2) { # Model without interactions with only two variables\n  tmp_vars <- c(\"Transported\", v1, v2)\n  set.seed(8584)\n  m <- train(Transported ~ (.)^2, data = df[, tmp_vars], preProc = NULL, method = \"glm\", metric = \"accuracy\", trControl = ctrl)\n  return(m)\n}\n\npreds_cat <- c(\"CryoSleep\", \"HomePlanet\", \"Destination\", \"VIP\", \"Deck\", \"Side\")\npairs <- combn(preds_cat, 2, simplify = FALSE)\npairs_cols <- combn(preds_cat, 2, simplify = TRUE) %>%\n  t() %>%\n  as.data.frame()\n\nctrl <- trainControl(method = \"repeatedcv\", repeats = 5, classProbs = TRUE, summaryFunction = prSummary)\n\nmy_cluster <- makeCluster(detectCores() - 1, type = 'SOCK')\nregisterDoSNOW(my_cluster)\n\nno_int_mods <- pairs %>%\n  map(.x = ., .f = \\(vp) pair_model(train5, vp[1], vp[2]))\n\nint_mods <- pairs %>%\n  map(.x = ., .f = \\(vp) pair_int_model(train5, vp[1], vp[2]))\n\nstopCluster(my_cluster)\n\nno_int_acc <- no_int_mods %>%\n  list_flatten(.) %>%\n  map(.x = ., .f = \\(m) getTrainPerf(m)[1, \"TrainPrecision\"]) %>%\n  list_c(.)\n\ndiff_res <- map2(.x = no_int_mods, .y = int_mods, .f = \\(m1, m2) compare_models_1way(m1, m2, alternative = \"greater\"))\ndiff_res2 <-\n  data.frame(Improvement = map_dbl(.x = diff_res, .f = \\(est) est$estimate),\n             Pvalue = map_dbl(.x = diff_res, .f = \\(p) p$p.value)) %>%\n  bind_cols(., Accuracy = no_int_acc) %>%\n  bind_cols(pairs_cols, .)\n\ndiff_res2 %>% \n  filter(Pvalue <= 0.05) %>%\n  arrange(desc(Improvement))\n#>            V1          V2  Improvement       Pvalue\n#> 1   CryoSleep Destination 0.2691091011 2.497225e-13\n#> 2  HomePlanet        Deck 0.1043066266 1.551826e-04\n#> 3         VIP        Deck 0.0017354191 2.474699e-02\n#> 4 Destination        Side 0.0008895738 1.775294e-03\n#>    Accuracy\n#> 1 0.6718461\n#> 2 0.6836372\n#> 3 0.5790599\n#> 4 0.5773259"},{"path":"interactions.html","id":"interaction-significance-with-entire-model-and-pairwise-interactions","chapter":"6 Interactions","heading":"6.3 Interaction significance with entire model and pairwise interactions","text":"’ve looked variable pairs interactions absence variables previous section interactions contribute model variables? improvements still persist become correlated variable effects lessen even introduce noise data?Now ’ve added variables models, see 5 interaction effects statistically significant without p-value adjustment look resampled p-values. Kuhn Johnson write:\n> interactions discovered included broader model contains (perhaps correlated) predictors, importance model may diminished. (…) might reduce number predictors considered important (since residual degrees freedom smaller) discovered interactions likely reliably important larger model.adjust values, single interaction effect remains CryoSleep HomePlanet. stricter adjustment Boneferroni multiplies p-values number tests (15 case) less strict adjustment FDR sorts p-values ascending order multiplies one total number tests divided p-value’s position (lowest value gets multiplied 15/1 case, second lowest 14/2 ). case interaction, holds even stricter Bonferroni adjustment implies effect relatively strong.mean shouldn’t include interaction effects model? . can still try add ones ’ve discovered far later stage evaluate features select.","code":"\nmy_split <- initial_split(train5, prop = 0.8)\nint_train <- training(my_split) %>%\n  mutate(across(.cols = c(PassengerGroup), .fns = as.integer)) %>%\n  select(-c(PassengerId, Cabin, Name, LastName, PassengerCount, HomePlanetsPerGroup))\n\nnorm_ctrl <- trainControl(method = \"repeatedcv\", repeats = 5, classProbs = TRUE, summaryFunction = prSummary)\nnorm_rec <- recipe(Transported ~ ., data = int_train)\n\n# my_cluster <- makeCluster(detectCores() - 1, type = 'SOCK')\n# registerDoSNOW(my_cluster)\n# \n# set.seed(8584)\n# norm_m <- train(norm_rec, data = int_train, method = \"glm\", metric = \"accuracy\", trControl = norm_ctrl)\n# \n# norm_m_acc <- getTrainPerf(norm_m)[1, \"TrainPrecision\"]\n# save(norm_m_acc, file = \"GLM model accuracy performance without interactions.RData\")\n# \n# preds_cat <- c(\"CryoSleep\", \"HomePlanet\", \"Destination\", \"VIP\", \"Deck\", \"Side\")\n# pairs_cols <- combn(preds_cat, 2, simplify = TRUE) %>%\n#   t() %>%\n#   as.data.frame()\n# pairs <- combn(preds_cat, 2, simplify = FALSE)\n# \n# int_ctrl <- trainControl(method = \"repeatedcv\", repeats = 5, classProbs = TRUE, summaryFunction = prSummary)\n# int_form <- map(.x = pairs, .f = \\(vp) formula(paste0(\"~\", vp[1], \":\", vp[2]))) # Map over pairs of vars to create int formulas\n# int_function <- function(rec, f) {\n#   ir <- rec %>%\n#     step_interact(!!f)\n#   return(ir)\n# }\n# int_rec <- map(.x = int_form, .f = \\(form) int_function(norm_rec, f = form))\n# \n# set.seed(8584)\n# int_m <- map(.x = int_rec, .f = \\(r) train(r, data = int_train, method = \"glm\", metric = \"accuracy\", trControl = int_ctrl))\n# \n# int_m_acc <- map_dbl(.x = int_m, .f = \\(m) getTrainPerf(m)[1, \"TrainPrecision\"])\n# save(int_m_acc, file = \"GLM model accuracy performance with interactions.RData\")\n# \n# anova_res <- map2(.x = list(norm_m), .y = int_m, .f = \\(m1, m2) anova(m1$finalModel, m2$finalModel, test = \"Chisq\")[2, 'Pr(>Chi)'])\n# save(anova_res, file = \"Anova without interactions.RData\")\n# \n# diff_all_res <- map2(.x = int_m, .y = list(norm_m), .f = \\(m1, m2) compare_models_1way(m1, m2, alternative = \"greater\"))\n# save(diff_all_res, file = \"Comparisson between GLM model with and without interactions.RData\")\n# \n# stopCluster(my_cluster)\n\nload(\"GLM model accuracy performance without interactions.RData\")\nload(\"GLM model accuracy performance with interactions.RData\")\nload(\"Comparisson between GLM model with and without interactions.RData\")\nload(\"Anova without interactions.RData\")\n\ndiff_all_res2 <-\n  data.frame(Improvement = map_dbl(.x = diff_all_res, .f = \\(est) est$estimate),\n             Resampled_Pvalue = map_dbl(.x = diff_all_res, .f = \\(p) p$p.value),\n             Traditional_Pvalue = map_dbl(.x = anova_res, .f = \\(p) p)) %>%\n  bind_cols(., No_Int_Accuracy = norm_m_acc) %>%\n  bind_cols(., With_Int_Accuracy = int_m_acc) %>%\n  bind_cols(pairs_cols, .)\n\ndiff_all_res2_adj <- diff_all_res2 %>%\n  mutate(Resampled_pvalue_FDR = p.adjust(Resampled_Pvalue, method = \"fdr\"),\n         Traditional_pvalue_FDR = p.adjust(Traditional_Pvalue, method = \"fdr\"),\n         Resampled_pvalue_Bon = p.adjust(Resampled_Pvalue, method = \"bonferroni\"),\n         Traditional_pvalue_Bon = p.adjust(Traditional_Pvalue, method = \"bonferroni\"))\n\ndiff_all_res2_adj %>% filter(Resampled_Pvalue <= 0.05) %>% arrange(desc(Improvement))\n#>          V1         V2 Improvement Resampled_Pvalue\n#> 1 CryoSleep       Deck 0.005205615     4.011748e-02\n#> 2 CryoSleep HomePlanet 0.003771645     3.656686e-15\n#>   Traditional_Pvalue No_Int_Accuracy With_Int_Accuracy\n#> 1       1.241712e-40       0.8070313         0.8049513\n#> 2       3.508244e-24       0.8070313         0.8062025\n#>   Resampled_pvalue_FDR Traditional_pvalue_FDR\n#> 1         2.499718e-01           1.862567e-39\n#> 2         5.485029e-14           2.631183e-23\n#>   Resampled_pvalue_Bon Traditional_pvalue_Bon\n#> 1         6.017622e-01           1.862567e-39\n#> 2         5.485029e-14           5.262366e-23\ndiff_all_res2_adj %>% filter(Resampled_pvalue_FDR <= 0.2) %>% arrange(desc(Improvement))\n#>          V1         V2 Improvement Resampled_Pvalue\n#> 1 CryoSleep HomePlanet 0.003771645     3.656686e-15\n#>   Traditional_Pvalue No_Int_Accuracy With_Int_Accuracy\n#> 1       3.508244e-24       0.8070313         0.8062025\n#>   Resampled_pvalue_FDR Traditional_pvalue_FDR\n#> 1         5.485029e-14           2.631183e-23\n#>   Resampled_pvalue_Bon Traditional_pvalue_Bon\n#> 1         5.485029e-14           5.262366e-23\ndiff_all_res2_adj %>% filter(Resampled_pvalue_Bon <= 0.2) %>% arrange(desc(Improvement))\n#>          V1         V2 Improvement Resampled_Pvalue\n#> 1 CryoSleep HomePlanet 0.003771645     3.656686e-15\n#>   Traditional_Pvalue No_Int_Accuracy With_Int_Accuracy\n#> 1       3.508244e-24       0.8070313         0.8062025\n#>   Resampled_pvalue_FDR Traditional_pvalue_FDR\n#> 1         5.485029e-14           2.631183e-23\n#>   Resampled_pvalue_Bon Traditional_pvalue_Bon\n#> 1         5.485029e-14           5.262366e-23\nint_vars_very_imp <- diff_all_res2_adj %>% \n  filter(Resampled_pvalue_Bon <= 0.2) %>% \n  select(V1, V2) %>%\n  mutate(ForFormula = str_c(\"starts_with('\", V1, \"'):starts_with('\", V2, \"')\")) %>%\n  mutate(RevFormula = str_c(\"starts_with('\", V2, \"'):starts_with('\", V1, \"')\"))\n\nint_vars_maybe_imp <- diff_all_res2_adj %>% \n  filter(Resampled_Pvalue <= 0.05) %>% \n  select(V1, V2) %>%\n  mutate(ForFormula = str_c(\"starts_with('\", V1, \"'):starts_with('\", V2, \"')\")) %>%\n  mutate(RevFormula = str_c(\"starts_with('\", V2, \"'):starts_with('\", V1, \"')\"))"},{"path":"interactions.html","id":"all-interaction-effects","chapter":"6 Interactions","heading":"6.4 All interaction effects","text":"Let’s turn comprehensive interaction analysis ’ll use penalized regression see whether interactions prove beneficial model. use glmnet-function first fit model without interactions model interactions tune different parameters.\\(\\lambda\\) parameter controls penalty applied different variables maximize accuracy \\(\\alpha\\) controls weight applied penalty function \\(\\alpha = 1\\) signifies Lasso penalty penalty applied absolute regression coefficient \\(\\alpha = 0\\) signifies Ridge penalty penalty applied squared coefficient. case, glmnet model allows us tune mix factors gives best accuracy.\nFigure 6.3: Tuning results Lasso/Ridge regression without interaction effects.\nWithout interaction effects, regression model favoured pure Lasso regression (\\(\\alpha = 1\\)) penalty \\(\\lambda = 0.00016\\). important variables (highest coefficients) case Deck, VRDeck, Spa, HomePlanet Cryosleep. final model used 33 35 variables (two dropped TotalSpent feature TravellingSolo) resampled accuracy without interactions around 0.79.look best coefficients penalized regression model, several.\nFigure 6.4: Tuning results Lasso/Ridge regression interaction effects\nregression model still favours pure Lasso (\\(\\alpha = 1\\)) regression higher penalty \\(\\lambda = 0.0025\\). 348 total variables interaction effects, selected 107 best accuracy slightly 0.80.important variables almost interaction effects CryoSleep & Deck, HomePlanet & CryoSleep . go back visualizations, see Deck = F seem effect CryoSleep changes False True true change homeplanets, well.original variables made top ten list VRDeck, Spa Cryosleep. smoothed graphs, saw credits spent VRDeck significantly reduced chances transported also makes sense.Let’s add discoveries important variable list.","code":"\ndf_pen <- train6 %>%\n  mutate(across(.cols = c(PassengerGroupSize, Solo, LargeGroup, TravelTogether, tidyselect::ends_with(\"PerGroup\")),\n                .fns = as.integer))\n\nset.seed(8584)\nmy_pen_split <- initial_split(df_pen, prop = 0.8)\npen_train <- training(my_pen_split)\nmy_pen_folds <- vfold_cv(pen_train, v = 10, repeats = 5)\n\nmy_vars <- data.frame(Variables = names(pen_train)) %>%\n  mutate(Roles = if_else(Variables %in% c(\"PassengerId\", \"Cabin\", \"Name\", \"LastName\", \"PassengerCount\", \"HomePlanetsPerGroup\"),\n                         \"id\", \"predictor\"),\n         Roles = if_else(Variables == \"Transported\", \"outcome\", Roles))\n\npen_rec <- recipe(x = pen_train, vars = my_vars$Variables, roles = my_vars$Roles) %>%\n  step_zv() %>%\n  step_normalize(Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, TotalSpent, CabinNumber, LastNameAsNumber, PassengerGroup,\n                 TotalSpentPerGroup) %>%\n  step_dummy(all_nominal_predictors())\n\nmy_acc <- metric_set(accuracy)\npen_ctrl <- control_grid(verbose = TRUE, save_pred = TRUE, save_workflow = TRUE)\npen_grid <- expand.grid(mixture = seq(.2, 1, by = .2), penalty = 10^seq(-4, -1, by = 0.1))\n\nglmnet_mod <- logistic_reg(penalty = tune(), mixture = tune()) %>%\n  set_engine(\"glmnet\")\n\n# my_cluster <- makeCluster(detectCores() - 1, type = 'SOCK')\n# registerDoSNOW(my_cluster)\n# \n# system.time({\n#   set.seed(8584)\n#   pen_tune <- glmnet_mod %>%\n#     tune_grid(pen_rec, resamples = my_pen_folds, metrics = my_acc, control = pen_ctrl, grid = pen_grid)\n# })\n# \n# save(pen_tune, file = \"Penalized regression without interactions.RData\")\n# \n# stopCluster(my_cluster)\n# unregister()\n\nload(\"Penalized regression without interactions.RData\")\npen_best <- fit_best(pen_tune, verbose = TRUE)\npen_coef <- pen_best %>%\n  tidy() %>%\n  filter(estimate != 0) %>%\n  filter(term != \"(Intercept)\") %>%\n  pull(term)\n\nshow_best(pen_tune, metric = \"accuracy\", n = 20) %>%\n  mutate(mixture = as.factor(round(mixture, 2))) %>%\n  ggplot(aes(x = penalty, y = mean, label = mixture, colour = mixture)) +\n  geom_line() +\n  geom_point() +\n  scale_x_log10() +\n  labs(title = \"Tune results without interactions\", x = \"Lambda penalty\", y = \"Resample accuracy\", colour = \"Alpha\")\nint_vars <- pen_coef %>%\n  combn(., 2, simplify = FALSE)\n\n# Map over pairs of vars to create int formula\nint_formula <- map_chr(.x = int_vars, .f = \\(vp) paste0(\"starts_with('\", vp[1], \"'):starts_with('\", vp[2], \"')\")) %>%\n  str_flatten(., collapse = \"+\") %>%\n  paste(\"~\", .) %>%\n  as.formula(.)\n\npen_int_rec <- recipe(x = pen_train, vars = my_vars$Variables, roles = my_vars$Roles) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_interact(int_formula) %>%\n  step_zv(all_predictors()) %>%\n  step_nzv(all_predictors()) %>%\n  step_normalize(Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, TotalSpent, CabinNumber, LastNameAsNumber, PassengerGroup,\n                 TotalSpentPerGroup)\n\n# my_cluster <- makeCluster(detectCores() - 1, type = 'SOCK')\n# registerDoSNOW(my_cluster)\n# clusterExport(cl = my_cluster, \"int_formula\")\n# \n# set.seed(8584)\n# pen_int_tune <- glmnet_mod %>%\n#   tune_grid(pen_int_rec, resamples = my_pen_folds, metrics = my_acc, control = pen_ctrl, grid = pen_grid)\n# \n# save(pen_int_tune, file = \"Penalized regression with interactions.RData\")\n# \n# stopCluster(my_cluster)\n# unregister()\n\nload(\"Penalized regression with interactions.RData\")\npen_int_best <- fit_best(pen_int_tune, verbose = TRUE)\npen_int_coef <- pen_int_best %>%\n  tidy()\nshow_best(pen_int_tune, metric = \"accuracy\", n = 20) %>%\n  mutate(mixture = as.factor(round(mixture, 2))) %>%\n  ggplot(aes(x = penalty, y = mean, label = mixture, colour = mixture)) +\n  geom_line() +\n  geom_point() +\n  scale_x_log10() +\n  labs(title = \"Tune results with interactions\", x = \"Lambda penalty\", y = \"Resample accuracy\", colour = \"Alpha\")\npen_int_top <- pen_int_coef %>%\n  select(-penalty) %>%\n  filter(estimate != 0) %>%\n  arrange(desc(abs(estimate))) %>%\n  slice(1:10)\n\npen_int_top\n#> # A tibble: 10 × 2\n#>    term                               estimate\n#>    <chr>                                 <dbl>\n#>  1 CryoSleep_True_x_Deck_F               2.31 \n#>  2 HomePlanet_Europa_x_CryoSleep_True    1.95 \n#>  3 HomePlanet_Mars_x_Deck_F              1.17 \n#>  4 LastNamesPerGroup_x_Deck_C            0.977\n#>  5 Destination_TRAPPIST.1e_x_Deck_E     -0.841\n#>  6 CryoSleep_True_x_Deck_B               0.825\n#>  7 CryoSleep_True                        0.558\n#>  8 Deck_B_x_Side_S                       0.554\n#>  9 HomePlanet_Mars_x_CryoSleep_True      0.518\n#> 10 HomePlanet_Europa_x_Side_S            0.444\nint_vars_very_imp2 <- pen_int_top %>%\n  filter(str_detect(term, \"_x_\")) %>%\n  mutate(V1 = str_split_i(term, \"_\", 1),\n         V2 = str_split_i(term, \"_\", -2),\n         ForFormula = str_c(\"starts_with('\", V1, \"'):starts_with('\", V2, \"')\"),\n         RevFormula = str_c(\"starts_with('\", V2, \"'):starts_with('\", V1, \"')\")) %>%\n  select(V1, V2, ForFormula, RevFormula) %>%\n  bind_rows(int_vars_very_imp, .)\n\nint_vars_very_imp2\n#>                   V1         V2\n#> 1          CryoSleep HomePlanet\n#> 2          CryoSleep       Deck\n#> 3         HomePlanet  CryoSleep\n#> 4         HomePlanet       Deck\n#> 5  LastNamesPerGroup       Deck\n#> 6        Destination       Deck\n#> 7          CryoSleep       Deck\n#> 8               Deck       Side\n#> 9         HomePlanet  CryoSleep\n#> 10        HomePlanet       Side\n#>                                              ForFormula\n#> 1    starts_with('CryoSleep'):starts_with('HomePlanet')\n#> 2          starts_with('CryoSleep'):starts_with('Deck')\n#> 3    starts_with('HomePlanet'):starts_with('CryoSleep')\n#> 4         starts_with('HomePlanet'):starts_with('Deck')\n#> 5  starts_with('LastNamesPerGroup'):starts_with('Deck')\n#> 6        starts_with('Destination'):starts_with('Deck')\n#> 7          starts_with('CryoSleep'):starts_with('Deck')\n#> 8               starts_with('Deck'):starts_with('Side')\n#> 9    starts_with('HomePlanet'):starts_with('CryoSleep')\n#> 10        starts_with('HomePlanet'):starts_with('Side')\n#>                                              RevFormula\n#> 1    starts_with('HomePlanet'):starts_with('CryoSleep')\n#> 2          starts_with('Deck'):starts_with('CryoSleep')\n#> 3    starts_with('CryoSleep'):starts_with('HomePlanet')\n#> 4         starts_with('Deck'):starts_with('HomePlanet')\n#> 5  starts_with('Deck'):starts_with('LastNamesPerGroup')\n#> 6        starts_with('Deck'):starts_with('Destination')\n#> 7          starts_with('Deck'):starts_with('CryoSleep')\n#> 8               starts_with('Side'):starts_with('Deck')\n#> 9    starts_with('CryoSleep'):starts_with('HomePlanet')\n#> 10        starts_with('Side'):starts_with('HomePlanet')"},{"path":"interactions.html","id":"tree-model-interactions","chapter":"6 Interactions","heading":"6.5 Tree model interactions","text":"Max Kuhn also offer another method discovering interaction effects use tree-based model like random forest. algorithm doesn’t evaluate effects interactions rank original variables based importance can used model interaction effects second stage.\nFigure 6.5: Variable importance based ranger tree-model\ntree model heavily favours amenity variables well variables penalized model didn’t consider important enough. saw, however, numerical analysis, many amenities large effects response perhaps interactions , well? Based ranking tree model, well exploration earlier, let’s add additional interactions might interesting explore.","code":"\ndf_tree <- train6 %>%\n  mutate(across(.cols = c(PassengerGroupSize, Solo, LargeGroup, TravelTogether, tidyselect::ends_with(\"PerGroup\")),\n                .fns = as.integer))\n\nset.seed(8584)\nmy_tree_split <- initial_split(df_tree, prop = 0.8)\ntree_train <- training(my_tree_split)\n\nmy_vars <- data.frame(Variables = names(tree_train)) %>%\n  mutate(Roles = if_else(Variables %in% c(\"PassengerId\", \"Cabin\", \"Name\", \"LastName\", \"PassengerCount\", \"HomePlanetsPerGroup\"),\n                         \"id\", \"predictor\"),\n         Roles = if_else(Variables == \"Transported\", \"outcome\", Roles))\n\ntree_rec <- recipe(x = tree_train, vars = my_vars$Variables, roles = my_vars$Roles) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, TotalSpent, CabinNumber, LastNameAsNumber, PassengerGroup,\n                 TotalSpentPerGroup) %>%\n  step_dummy(all_nominal_predictors())\n\ntree_baked <- tree_rec %>%\n  prep() %>%\n  bake(new_data = NULL) %>%\n  select(-c(Cabin, Name, LastName, PassengerCount, HomePlanetsPerGroup))\n\n# rf_mod <- ranger::ranger(Transported ~ . -PassengerId, data = tree_baked, num.trees = 1000, importance = \"impurity\", \n#                  num.threads = detectCores() - 1, seed = 8584)\n# \n# save(rf_mod, file = \"Ranger tree model variable importance.RData\")\nload(\"Ranger tree model variable importance.RData\")\n\nrf_imp <- tibble(Predictor = names(rf_mod$variable.importance),\n                 Importance = unname(rf_mod$variable.importance))\nggplot(rf_imp, aes(x = reorder(Predictor, Importance), y = Importance)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  xlab(\"\")\ntree_int_vars <- rf_imp %>%\n  filter(Importance > 150) %>%\n  pull(Predictor) %>%\n  combn(., 2, simplify = TRUE) %>%\n  t() %>%\n  as.data.frame(.) %>%\n  mutate(V1 = str_split_i(V1, \"_\", 1),\n         V2 = str_split_i(V2, \"_\", 1),\n         ForFormula = str_c(\"starts_with('\", V1, \"'):starts_with('\", V2, \"')\"),\n         RevFormula = str_c(\"starts_with('\", V2, \"'):starts_with('\", V1, \"')\"))"},{"path":"feature-selection-and-elimination.html","id":"feature-selection-and-elimination","chapter":"7 Feature selection and elimination","heading":"7 Feature selection and elimination","text":"Now created several new features discovered many interaction effects, must explore potential improve models don’t. Kuhn Johnsson recommend several methods feature selection elimination ’re going explore .","code":""},{"path":"feature-selection-and-elimination.html","id":"recursive-feature-elimination","chapter":"7 Feature selection and elimination","heading":"7.1 Recursive feature elimination","text":"method create model includes features resampling process remove sets variables test improves performance. start smaller set interactions since process computationally demanding.\nFigure 7.1: Estimated performance recursive feature elimination based number variables included\n\nFigure 7.2: Correlation first set optimal variables RFE model\nsee several variables relatively high correlation introducing noise model. Let’s add step remove correlated variables see affects performance.\nFigure 7.3: Estimated performance recursive feature elimination based number variables. Correlated variables removed.\n’m sure results contain variables 55 tuning max model similar performance one without removal correlations still considers 45 variables optimal.final run, can also add interactions considered based results tree model optimal set variables see improve model.\nFigure 7.4: Estimated performance recursive feature elimination interactions. Correlated variables removed.\n’m sure results contain variables 55 tuning max model similar performance one without removal correlations still considers 45 variables optimal.Since ’m sure got results 170 variables , let’s test set variables randomForest see results consistent.accuracy closer 80% within values sets variables.","code":"\ntrain7 <- train6 %>%\n  select(-c(PassengerCount, HomePlanetsPerGroup))\n\nset.seed(8584)\nrfe_split <- initial_split(train7, prop = 0.8)\nrfe_train <- training(rfe_split)\n\nmany_stats <- function(data, lev = levels(data$obs), model = NULL) {\n    c(twoClassSummary(data = data, lev = levels(data$obs), model),\n      prSummary(data = data, lev = levels(data$obs), model),\n      mnLogLoss(data = data, lev = levels(data$obs), model),\n      defaultSummary(data = data, lev = levels(data$obs), model))\n}\n\nrfe_funcs <- caret::rfFuncs\nrfe_funcs$summary <- many_stats\n\nrfe_vars <- data.frame(Variables = names(train7)) %>%\n  mutate(Roles = if_else(Variables %in% c(\"PassengerId\", \"Cabin\", \"Name\", \"LastName\"), \"id\", \"predictor\"),\n         Roles = if_else(Variables == \"Transported\", \"outcome\", Roles))\n\nint_formula <- int_vars_very_imp2 %>%\n  select(ForFormula, RevFormula) %>%\n  unlist() %>%\n  unname() %>%\n  str_flatten(., collapse = \"+\") %>%\n  str_c(\"~\", .) %>%\n  as.formula(.)\n\nrfe_rec <- recipe(x = rfe_train, vars = rfe_vars$Variables, roles = rfe_vars$Roles) %>%\n  step_normalize(Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, TotalSpent, CabinNumber, LastNameAsNumber, PassengerGroup,\n                 TotalSpentPerGroup) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_interact(int_formula) %>%\n  step_zv(all_predictors())\n\nrfe_bake <- rfe_rec %>% prep() %>% bake(new_data = NULL)\n\nrfe_sizes <- seq(5, 55, 5)\nrfe_ctrl <- rfeControl(method = \"repeatedcv\", repeats = 5, functions = rfe_funcs, returnResamp = \"all\", verbose = FALSE)\n\n## This part takes a while to run, even in parallel, so save the results\n# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')\n# registerDoSNOW(my_cluster)\n# \n# snow::clusterEvalQ(my_cluster, library(recipes))\n# snow::clusterExport(my_cluster, \"int_formula\")\n# \n# set.seed(8584)\n# rfe_acc <- rfe(rfe_rec, data = rfe_train, sizes = rfe_sizes, rfeControl = rfe_ctrl, metric = \"Accuracy\", ntree = 1000)\n# \n# save(rfe_acc, file = \"Recursive feature elimination with interactions.RData\")\n# \n# stopCluster(my_cluster)\n# unregister()\n\nload(\"Recursive feature elimination with interactions.RData\")\nrfe_acc$results %>%\n  ggplot(data = ., aes(x = Variables, y = Accuracy)) +\n  geom_point() +\n  geom_line()\nrfe_bake %>% \n  select(rfe_acc[[\"optVariables\"]]) %>%\n  DataExplorer::plot_correlation(., type = \"continuous\", geom_text_args = list(size = 10), \n                   theme_config = list(text = element_text(size = 8), axis.text.x = element_text(angle = 90)))\nrfe_rec_corr <- recipe(x = rfe_train, vars = rfe_vars$Variables, roles = rfe_vars$Roles) %>%\n  step_normalize(Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, TotalSpent, CabinNumber, LastNameAsNumber, PassengerGroup,\n                 TotalSpentPerGroup) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_interact(int_formula) %>%\n  step_zv(all_predictors()) %>%\n  step_corr(all_numeric_predictors(), threshold = 0.5)\n  \nrfe_bake_corr <- rfe_rec_corr %>% prep() %>% bake(new_data = NULL)\n\n## This part takes a while to run, even in parallel so save the results\n# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')\n# registerDoSNOW(my_cluster)\n# \n# snow::clusterEvalQ(my_cluster, library(recipes))\n# snow::clusterExport(my_cluster, \"int_formula\")\n# \n# set.seed(8584)\n# rfe_acc_corr <- rfe(rfe_rec_corr, data = rfe_train, sizes = rfe_sizes, rfeControl = rfe_ctrl, metric = \"Accuracy\", ntree = 1000)\n# \n# save(rfe_acc_corr, file = \"Recursive feature elimination with interactions removed correlated.RData\")\n# \n# stopCluster(my_cluster)\n# unregister()\n\nload(\"Recursive feature elimination with interactions removed correlated.RData\")\nrfe_acc_corr$results %>%\n  ggplot(data = ., aes(x = Variables, y = Accuracy)) +\n  geom_point() +\n  geom_line()\nrfe_opt_vars1 <- rfe_acc_corr[[\"optVariables\"]]\nrfe_opt_vars1\n#>  [1] \"VRDeck\"                                 \n#>  [2] \"Spa\"                                    \n#>  [3] \"RoomService\"                            \n#>  [4] \"CryoSleep_True\"                         \n#>  [5] \"CryoSleep_True_x_Deck_F\"                \n#>  [6] \"FoodCourt\"                              \n#>  [7] \"Side_S\"                                 \n#>  [8] \"Age\"                                    \n#>  [9] \"HomePlanet_Mars_x_Deck_E\"               \n#> [10] \"PassengerGroup\"                         \n#> [11] \"HomePlanet_Europa_x_Side_S\"             \n#> [12] \"CryoSleep_True_x_Deck_B\"                \n#> [13] \"Deck_E_x_Destination_PSO.J318.5.22\"     \n#> [14] \"CryoSleep_True_x_Deck_C\"                \n#> [15] \"CryoSleep_True_x_Deck_D\"                \n#> [16] \"HomePlanet_Mars_x_Side_S\"               \n#> [17] \"Deck_G_x_Destination_TRAPPIST.1e\"       \n#> [18] \"Destination_TRAPPIST.1e\"                \n#> [19] \"CryoSleepsPerGroup_X2_x_HomePlanet_Mars\"\n#> [20] \"Deck_B_x_Destination_TRAPPIST.1e\"       \n#> [21] \"Deck_C_x_Destination_TRAPPIST.1e\"       \n#> [22] \"PassengerGroupSize_X4\"                  \n#> [23] \"HomePlanet_Europa_x_Deck_E\"             \n#> [24] \"DestinationsPerGroup_X2\"                \n#> [25] \"ShoppingMall\"                           \n#> [26] \"PassengerGroupSize_X3\"                  \n#> [27] \"Deck_F_x_Destination_PSO.J318.5.22\"     \n#> [28] \"Deck_C_x_LastNamesPerGroup_X2\"          \n#> [29] \"PassengerGroupSize_X5\"                  \n#> [30] \"CabinsPerGroup_X2\"                      \n#> [31] \"LastNameCount\"                          \n#> [32] \"PassengerGroupSize_X6\"                  \n#> [33] \"PassengerGroupSize_X7\"                  \n#> [34] \"Deck_C_x_DestinationsPerGroup_X2\"       \n#> [35] \"Deck_G_x_DestinationsPerGroup_X3\"       \n#> [36] \"Deck_G_x_DestinationsPerGroup_X2\"       \n#> [37] \"Deck_G_x_LastNamesPerGroup_X2\"          \n#> [38] \"Deck_G_x_Destination_PSO.J318.5.22\"     \n#> [39] \"CabinsPerGroup_X3\"                      \n#> [40] \"Deck_E_x_LastNamesPerGroup_X3\"          \n#> [41] \"PassengerGroupSize_X2\"                  \n#> [42] \"CryoSleep_True_x_Deck_E\"                \n#> [43] \"Deck_F_x_DestinationsPerGroup_X2\"       \n#> [44] \"Deck_E_x_LastNamesPerGroup_X2\"          \n#> [45] \"Deck_E_x_DestinationsPerGroup_X2\"\n\nrfe_opt_vars2 <- data.frame(term = rfe_opt_vars1) %>%\n  filter(str_detect(term, \"_x_\")) %>%\n  mutate(V1 = str_split_i(term, \"_\", 1),\n         V2 = str_split_i(term, \"_\", -2),\n         ForFormula = str_c(\"starts_with('\", V1, \"'):starts_with('\", V2, \"')\"),\n         RevFormula = str_c(\"starts_with('\", V2, \"'):starts_with('\", V1, \"')\")) %>%\n  select(V1, V2, ForFormula, RevFormula)\nset.seed(8584)\nrfe_split_all <- initial_split(train7, prop = 0.8)\nrfe_train_all <- training(rfe_split_all)\n\nmany_stats <- function(data, lev = levels(data$obs), model = NULL) {\n    c(twoClassSummary(data = data, lev = levels(data$obs), model),\n      prSummary(data = data, lev = levels(data$obs), model),\n      mnLogLoss(data = data, lev = levels(data$obs), model),\n      defaultSummary(data = data, lev = levels(data$obs), model))\n}\n\nrfe_funcs <- caret::rfFuncs\nrfe_funcs$summary <- many_stats\n\nrfe_vars <- data.frame(Variables = names(train7)) %>%\n  mutate(Roles = if_else(Variables %in% c(\"PassengerId\", \"Cabin\", \"Name\", \"LastName\"), \"id\", \"predictor\"),\n         Roles = if_else(Variables == \"Transported\", \"outcome\", Roles))\n\nint_formula <- bind_rows(rfe_opt_vars2, tree_int_vars) %>%\n  select(ForFormula, RevFormula) %>%\n  unlist() %>%\n  unname() %>%\n  unique(.) %>%\n  str_flatten(., collapse = \"+\") %>%\n  str_c(\"~\", .) %>%\n  as.formula(.)\n\nrfe_rec_all <- recipe(x = rfe_train_all, vars = rfe_vars$Variables, roles = rfe_vars$Roles) %>%\n  step_normalize(Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, TotalSpent, CabinNumber, LastNameAsNumber, PassengerGroup,\n                 TotalSpentPerGroup) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_interact(int_formula) %>%\n  step_zv(all_predictors()) %>%\n  step_corr(all_numeric_predictors(), threshold = 0.5)\n\nrfe_bake_all <- rfe_rec_all %>% prep() %>% bake(new_data = NULL)\n\nrfe_sizes_all <- seq(5, 100, 5)\nrfe_ctrl <- rfeControl(method = \"repeatedcv\", repeats = 5, functions = rfe_funcs, returnResamp = \"all\", verbose = FALSE)\n\n## This part takes a while to run, even in parallel so save the results\n\n# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')\n# registerDoSNOW(my_cluster)\n# \n# snow::clusterEvalQ(my_cluster, library(recipes))\n# snow::clusterExport(my_cluster, \"int_formula\")\n# \n# system.time({\n#   set.seed(8584)\n#   rfe_acc_all <- rfe(rfe_rec_all, data = rfe_train_all, sizes = rfe_sizes_all, rfeControl = rfe_ctrl, metric = \"Accuracy\", \n#                      ntree = 1000)\n# })\n# save(rfe_acc_all, file = \"Recursive feature elimination final.RData\")\n# \n# stopCluster(my_cluster)\n# unregister()\n\nload(\"Recursive feature elimination final.RData\")\nrfe_acc_all$results %>%\n  ggplot(data = ., aes(x = Variables, y = Accuracy)) +\n  geom_point() +\n  geom_line()\nrfe_acc_avg <- rfe_acc_all$results$Accuracy\n\nrfe_vars_best <- rfe_acc_all[[\"optVariables\"]]\nrfe_vars_best2 <- data.frame(term = rfe_vars_best) %>%\n  filter(str_detect(term, \"_x_\")) %>%\n  mutate(V1 = str_split_i(term, \"_\", 1),\n         V2_tmp = str_split_i(term, \"_x_\", 2),\n         V2 = str_split_i(V2_tmp, \"_\", 1),\n         ForFormula = str_c(\"starts_with('\", V1, \"'):starts_with('\", V2, \"')\"),\n         RevFormula = str_c(\"starts_with('\", V2, \"'):starts_with('\", V1, \"')\")) %>%\n  select(V1, V2, ForFormula, RevFormula)\nrfe_vars_best_weird <- rfe_acc_all$variables %>%\n  filter(Variables == 170) %>%\n  select(var) %>%\n  rename(term = var) %>%\n  filter(str_detect(term, \"_x_\")) %>%\n  mutate(V1 = str_split_i(term, \"_\", 1),\n         V2_temp = str_split_i(term, \"_x_\", 2),\n         V2 = str_split_i(V2_temp, \"_\", 1),\n         ForFormula = str_c(\"starts_with('\", V1, \"'):starts_with('\", V2, \"')\"),\n         RevFormula = str_c(\"starts_with('\", V2, \"'):starts_with('\", V1, \"')\")) %>%\n  select(V1, V2, ForFormula, RevFormula)\n\nint_form_weird <- rfe_vars_best_weird %>%\n  select(ForFormula, RevFormula) %>%\n  unlist() %>%\n  unname() %>%\n  unique(.) %>%\n  str_flatten(., collapse = \"+\") %>%\n  str_c(\"~\", .) %>%\n  as.formula(.)\n\nrfe_rec_all_weird <- recipe(x = rfe_train_all, vars = rfe_vars$Variables, roles = rfe_vars$Roles) %>%\n  step_normalize(Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, TotalSpent, CabinNumber, LastNameAsNumber, PassengerGroup,\n                 TotalSpentPerGroup) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_interact(int_form_weird) %>%\n  step_zv(all_predictors()) %>%\n  step_corr(all_numeric_predictors(), threshold = 0.5)\n\nrf_mod <- rand_forest(trees = 1000, min_n = 5) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"randomForest\")\n\nrfe_all_weird_wf <- workflow() %>%\n  add_recipe(rfe_rec_all_weird) %>%\n  add_model(rf_mod)\n\n# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')\n# registerDoSNOW(my_cluster)\n# \n# snow::clusterEvalQ(my_cluster, library(recipes))\n# snow::clusterExport(my_cluster, \"int_formula\")\n# \n# system.time({\n#   set.seed(8584)\n#   rfe_acc_all_weird <- fit(rfe_all_weird_wf, rfe_train_all)\n# })\n# \n# save(rfe_acc_all_weird, file = \"Weird RFE results.RData\")\n# \n# stopCluster(my_cluster)\n# unregister()\n\nload(\"Weird RFE results.RData\")\nrfe_acc_all_weird"},{"path":"feature-selection-and-elimination.html","id":"simulated-annealing","chapter":"7 Feature selection and elimination","heading":"7.2 Simulated annealing","text":"\nFigure 7.5: Estimated performance internal resamples simulated annealing.\n\nFigure 7.6: Estimated performance external resamples simulated annealing.\n\nFigure 7.7: Estimated performance best resample simulated annealing.\nresults simulated annealing indicate much lower accuracy overall due fact Naive Bayes model doesn’t perform well randomForest recursive feature elimination procedure. optimal set variables 73 similar RFE process. Let’s save final model comparison.compare best variables RFE process SA process, can see one overlap don’t.","code":"\nset.seed(8584)\nsa_split <- initial_split(train7, prop = 0.8)\nsa_train <- training(sa_split)\n\nmany_stats <- function(data, lev = levels(data$obs), model = NULL) {\n    c(twoClassSummary(data = data, lev = levels(data$obs), model),\n      prSummary(data = data, lev = levels(data$obs), model),\n      mnLogLoss(data = data, lev = levels(data$obs), model),\n      defaultSummary(data = data, lev = levels(data$obs), model))\n}\n\nmy_vars <- data.frame(Variables = names(train7)) %>%\n  mutate(Roles = if_else(Variables %in% c(\"PassengerId\", \"Cabin\", \"Name\", \"LastName\"), \"id\", \"predictor\"),\n         Roles = if_else(Variables == \"Transported\", \"outcome\", Roles))\n\nint_formula <- bind_rows(rfe_opt_vars2, tree_int_vars) %>%\n  select(ForFormula, RevFormula) %>%\n  unlist() %>%\n  unname() %>%\n  unique(.) %>%\n  str_flatten(., collapse = \"+\") %>%\n  str_c(\"~\", .) %>%\n  as.formula(.)\n\nsa_rec <- recipe(x = sa_train, vars = my_vars$Variables, roles = my_vars$Roles) %>%\n  step_normalize(Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, TotalSpent, CabinNumber, LastNameAsNumber, PassengerGroup,\n                 TotalSpentPerGroup) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_interact(int_formula) %>%\n  step_zv(all_predictors()) %>%\n  step_corr(all_numeric_predictors(), threshold = 0.5)\n\nsa_bake <- sa_rec %>%\n  prep() %>%\n  bake(new_data = NULL)\n\nsa_funcs <- caretSA\nsa_funcs$fitness_extern <- many_stats\nsa_funcs$initial <- function(vars, prob = 0.50, ...) {\n  sort(sample.int(vars, size = floor(vars * prob) + 1))\n}\n\n# Arguments for klaR::NaiveBayes that is used to fit the SA model. Adjust is used for the kernel density estimation\nsa_grid <- data.frame(mtry = 5)\n\n# Inner control\nsa_ctrl_inner <- trainControl(method = \"boot\", p = 0.90, number = 1, summaryFunction = many_stats, classProbs = TRUE, \n                        allowParallel = FALSE)\n\n# Outer control for SA\nsa_ctrl_outer <- safsControl(method = \"cv\", metric = c(internal = \"Accuracy\", external = \"Accuracy\"), \n                       maximize = c(internal = TRUE, external = TRUE), functions = sa_funcs, improve = 20, returnResamp = \"all\",\n                       verbose = FALSE, allowParallel = TRUE)\n\n# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')\n# registerDoSNOW(my_cluster)\n# clusterExport(cl = my_cluster, \"int_formula\")\n# \n# # Run simulated annealing with RandomForest\n# system.time({\n#   set.seed(8485)\n#   sim_anneal_50_pct <- safs(sa_rec, data = sa_train, iters = 500, safsControl = sa_ctrl_outer, method = \"rf\", tuneGrid = sa_grid,\n#                             trControl = sa_ctrl_inner, metric = \"Accuracy\")\n# })\n# \n# save(sim_anneal_50_pct, file = \"Simulated annealing with 50% inital.RData\")\n# \n# stopCluster(my_cluster)\n# unregister()\n\nload(\"Simulated annealing with 50% inital.RData\")\nsim_ann_50_int <- sim_anneal_50_pct$internal\nsim_ann_50_int2 <- sim_ann_50_int %>%\n  group_by(Iter) %>%\n  summarise(Accuracy = sum(Accuracy) / length(unique(sim_ann_50_int$Resample))) %>%\n  ungroup() %>%\n  mutate(Resample = \"Averaged\") %>%\n  bind_rows(sim_ann_50_int, .) %>%\n  mutate(colour_grp = if_else(Resample == \"Averaged\", \"yes\", \"no\"))\nsim_ann_50_int_avg <- sim_ann_50_int2 %>% filter(Resample == \"Averaged\") %>% select(Iter, Accuracy)\n  \nggplot(sim_ann_50_int2, aes(x = Iter, y = Accuracy, colour = colour_grp)) +\n  geom_point() +\n  facet_wrap(~Resample) +\n  lims(y = c(0, NA)) +\n  theme(legend.position = \"none\")\nsim_ann_50_ext <- sim_anneal_50_pct$external\nsim_ann_50_ext2 <- sim_ann_50_ext %>%\n  group_by(Iter) %>%\n  summarise(Accuracy = sum(Accuracy) / length(unique(sim_ann_50_int$Resample))) %>%\n  ungroup() %>%\n  mutate(Resample = \"Averaged\") %>%\n  bind_rows(sim_ann_50_int, .) %>%\n  mutate(colour_grp = if_else(Resample == \"Averaged\", \"yes\", \"no\"))\n\nsim_ann_50_ext_avg <- sim_ann_50_ext2 %>% filter(Resample == \"Averaged\") %>% select(Iter, Accuracy)\next_int_corr <- round(cor(sim_ann_50_int_avg$Accuracy, sim_ann_50_ext_avg$Accuracy), 2)\n\nggplot(mapping = aes(x = Iter, y = Accuracy)) +\n  geom_point(data = sim_ann_50_int_avg, aes(colour = \"red\")) +\n  geom_point(data = sim_ann_50_ext_avg, aes(colour = \"black\")) +\n  geom_label(data = sim_ann_50_ext_avg, x = 5, y = 0.7, label = str_c(\"Corr: \", ext_int_corr)) +\n  labs(colour = \"Estimate\") +\n  scale_colour_hue(labels = c(\"Internal\", \"External\"))\nsim_ann_50_final <- sim_anneal_50_pct$sa\nsim_ann_50_final2 <- data.frame(Iter = sim_ann_50_final[[\"internal\"]]$Iter, Accuracy = sim_ann_50_final[[\"internal\"]]$Accuracy,\n                                Subset_Size = unlist(lapply(sim_ann_50_final[[\"subsets\"]], length))) %>%\n  pivot_longer(-Iter)\n\nggplot(sim_ann_50_final2, aes(x = Iter, y = value)) +\n  geom_point() +\n  facet_wrap(~name, nrow = 2, ncol = 1, scales = \"free_y\")\nsa_vars_best <- sim_anneal_50_pct[[\"optVariables\"]]\nsa_vars_best2 <- data.frame(term = sa_vars_best) %>%\n  filter(str_detect(term, \"_x_\")) %>%\n  mutate(V1 = str_split_i(term, \"_\", 1),\n         V2 = str_split_i(term, \"_\", -2),\n         ForFormula = str_c(\"starts_with('\", V1, \"'):starts_with('\", V2, \"')\"),\n         RevFormula = str_c(\"starts_with('\", V2, \"'):starts_with('\", V1, \"')\")) %>%\n  select(V1, V2, ForFormula, RevFormula)\nbest_vars_rfe_sa <- intersect(rfe_vars_best, sa_vars_best)\nbest_vars_rfe_sa\n#>  [1] \"RoomService_x_VRDeck\"                 \n#>  [2] \"RoomService_x_Spa\"                    \n#>  [3] \"Deck_G_x_Destination_TRAPPIST.1e\"     \n#>  [4] \"ShoppingMall_x_CabinNumber\"           \n#>  [5] \"FoodCourt_x_LastNameAsNumber\"         \n#>  [6] \"Deck_E_x_HomePlanet_Mars\"             \n#>  [7] \"VRDeck_x_PassengerGroup\"              \n#>  [8] \"RoomService_x_PassengerGroup\"         \n#>  [9] \"FoodCourt_x_ShoppingMall\"             \n#> [10] \"ShoppingMall_x_PassengerGroup\"        \n#> [11] \"ShoppingMall_x_Spa\"                   \n#> [12] \"Age_x_Spa\"                            \n#> [13] \"CryoSleep_True_x_TotalSpentPerGroup\"  \n#> [14] \"FoodCourt_x_Spa\"                      \n#> [15] \"ShoppingMall_x_VRDeck\"                \n#> [16] \"RoomService_x_LastNameAsNumber\"       \n#> [17] \"PassengerGroup_x_TotalSpentPerGroup\"  \n#> [18] \"Age_x_VRDeck\"                         \n#> [19] \"LastNameAsNumber\"                     \n#> [20] \"TotalSpentPerGroup_x_LastNameAsNumber\"\n#> [21] \"PassengerGroup_x_LastNameAsNumber\"    \n#> [22] \"Age_x_RoomService\"                    \n#> [23] \"Age_x_TotalSpentPerGroup\"             \n#> [24] \"Age_x_LastNameAsNumber\"               \n#> [25] \"HomePlanet_Mars_x_Side_S\"             \n#> [26] \"Destination_TRAPPIST.1e\"              \n#> [27] \"Deck_C_x_Destination_TRAPPIST.1e\""},{"path":"feature-selection-and-elimination.html","id":"genetic-algorithm","chapter":"7 Feature selection and elimination","heading":"7.3 Genetic algorithm","text":"\nFigure 7.8: Estimated performance internal resamples genetic algorithm.\n\nFigure 7.9: Estimated performance external resamples genetic algorithm.\n","code":"\n# set.seed(8584)\n# ga_split <- initial_split(train7, prop = 0.8)\n# ga_train <- training(ga_split)\n# \n# many_stats <- function(data, lev = levels(data$obs), model = NULL) {\n#     c(twoClassSummary(data = data, lev = levels(data$obs), model),\n#       prSummary(data = data, lev = levels(data$obs), model),\n#       mnLogLoss(data = data, lev = levels(data$obs), model),\n#       defaultSummary(data = data, lev = levels(data$obs), model))\n# }\n# \n# my_vars <- data.frame(Variables = names(train7)) %>%\n#   mutate(Roles = if_else(Variables %in% c(\"PassengerId\", \"Cabin\", \"Name\", \"LastName\"), \"id\", \"predictor\"),\n#          Roles = if_else(Variables == \"Transported\", \"outcome\", Roles))\n# \n# int_formula <- bind_rows(rfe_opt_vars2, tree_int_vars) %>%\n#   select(ForFormula, RevFormula) %>%\n#   unlist() %>%\n#   unname() %>%\n#   unique(.) %>%\n#   str_flatten(., collapse = \"+\") %>%\n#   str_c(\"~\", .) %>%\n#   as.formula(.)\n# \n# ga_rec <- recipe(x = ga_train, vars = my_vars$Variables, roles = my_vars$Roles) %>%\n#   step_dummy(all_nominal_predictors()) %>%\n#   step_interact(int_formula) %>%\n#   step_zv(all_predictors())\n# \n# ga_bake <- ga_rec %>%\n#   prep() %>%\n#   bake(new_data = NULL)\n# \n# ga_funcs <- caretGA\n# ga_funcs$fitness_extern <- many_stats\n# ga_funcs$initial <- function(vars, popSize, ...)  {\n#   x <- matrix(NA, nrow = popSize, ncol = vars)\n#   probs <- seq(0.1, 0.90, length = popSize)\n#   for (i in 1:popSize) {\n#     x[i, ] <- \n#       sample(0:1, replace = TRUE, size = vars, prob = c(probs[i], 1 - probs[i]))\n#   }\n#   var_count <- apply(x, 1, sum)\n#   if (any(var_count == 0)) {\n#     for (i in which(var_count == 0)) {\n#       p <- sample(1:length(vars), size = 2)\n#       x[i, p] <- 1\n#     }\n#   }\n#   return(x)\n# }\n# \n# # Inner control\n# ga_ctrl_inner <- trainControl(method = \"boot\", p = 0.90, number = 1, summaryFunction = many_stats, classProbs = TRUE, \n#                         allowParallel = FALSE)\n# \n# # Outer control for SA\n# ga_ctrl_outer <- gafsControl(method = \"cv\", metric = c(internal = \"Accuracy\", external = \"Accuracy\"), \n#                        maximize = c(internal = TRUE, external = TRUE), functions = ga_funcs, returnResamp = \"all\",\n#                        verbose = FALSE, allowParallel = TRUE)\n# \n# # Arguments for klaR::NaiveBayes that is used to fit the SA model. Adjust is used for the kernel density estimation\n# ga_grid <- data.frame(mtry = 5)\n\n# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')\n# registerDoSNOW(my_cluster)\n# clusterExport(cl = my_cluster, \"int_formula\")\n# \n# system.time({\n#   set.seed(8584)\n#   ga_acc <- gafs(ga_rec, data = ga_train, iters = 2, gafsControl = ga_ctrl_outer, method = \"rf\", tuneGrid = ga_grid,\n#                    trControl = ga_ctrl_inner, metric = \"Accuracy\", ntree = 1000)\n# })\n# \n# save(ga_acc, file = \"Genetic algorithm feature selection.RData\")\n# \n# stopCluster(my_cluster)\n# unregister()\n\nload(\"Genetic algorithm feature selection.RData\")\nga_internal <- ga_acc$internal\nga_internal2 <- ga_internal %>%\n  group_by(Iter) %>%\n  summarise(Accuracy = sum(Accuracy) / length(unique(ga_internal$Resample))) %>%\n  ungroup() %>%\n  mutate(Resample = \"Averaged\") %>%\n  bind_rows(ga_internal, .) %>%\n  mutate(colour_grp = if_else(Resample == \"Averaged\", \"yes\", \"no\"))\n\nga_internal_avg <- ga_internal2 %>% filter(Resample == \"Averaged\") %>% select(Iter, Accuracy)\n  \nggplot(ga_internal2, aes(x = Iter, y = Accuracy, colour = colour_grp)) +\n  geom_point() +\n  facet_wrap(~Resample) +\n  lims(y = c(0, NA)) +\n  theme(legend.position = \"none\")\nga_external <- ga_acc$external\nga_external2 <- ga_external %>%\n  group_by(Iter) %>%\n  summarise(Accuracy = sum(Accuracy) / length(unique(ga_external$Resample))) %>%\n  ungroup() %>%\n  mutate(Resample = \"Averaged\") %>%\n  bind_rows(ga_external, .) %>%\n  mutate(colour_grp = if_else(Resample == \"Averaged\", \"yes\", \"no\"))\n\nga_external_avg <- ga_external2 %>% filter(Resample == \"Averaged\") %>% select(Iter, Accuracy)\n\nggplot(mapping = aes(x = Iter, y = Accuracy)) +\n  geom_point(data = ga_internal_avg, aes(colour = \"red\")) +\n  geom_point(data = ga_external_avg, aes(colour = \"black\")) +\n  geom_label(data = ga_external_avg, x = 5, y = 0.7, label = str_c(\"Corr: \", ext_int_corr)) +\n  labs(colour = \"Estimate\") +\n  scale_colour_hue(labels = c(\"Internal\", \"External\"))"},{"path":"feature-selection-and-elimination.html","id":"check-performance-with-random-subsets","chapter":"7 Feature selection and elimination","heading":"7.4 Check performance with random subsets","text":"\nFigure 7.10: Comparison different feature selection results random subset result.\ncan see recursive feature elimination process outperforms random variable subset. test significance RFE results best results random subset, p-values confirm difference significant.","code":"\n# my_subset_size <- length(sim_anneal_50_pct$optVariables)\n# my_vars <- sa_bake %>%\n#   select(-c(\"PassengerId\", \"Cabin\", \"Name\", \"LastName\", \"Transported\")) %>%\n#   names(.)\n# map_seq <- 1:(length(my_vars)/2)\n# \n# rand_subset <- map(map_seq, .f = \\(x) sample(my_vars, my_subset_size))\n# rand_data <- map(rand_subset, .f = \\(x) sa_bake %>% dplyr::select(Transported, x))\n# rand_rec <- map(rand_data, .f = \\(x) recipe(Transported ~ ., data = x))\n# \n# subset_ctrl <- trainControl(method = \"cv\", classProbs = TRUE, summaryFunction = many_stats)\n# subset_grid <- data.frame(mtry = 5)\n\n# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')\n# registerDoSNOW(my_cluster)\n# \n# system.time({\n#   subset_model <- map2(.x = rand_rec, .y = rand_data, \n#                        .f = \\(rec, df) train(rec, data = df, method = \"rf\", tuneGrid = subset_grid, trControl = subset_ctrl,\n#                                              metric = \"Accuracy\"))\n# })\n# \n# subset_perf <- map(subset_model, .f = \\(m) getTrainPerf(m))\n#   \n# save(subset_perf, file = \"Rf random subsets performance.RData\")\n# rm(rand_subset, rand_data, rand_rec, subset_model)\n# \n# stopCluster(my_cluster)\n# unregister()\n\nload(\"Rf random subsets performance.RData\")\nrfe_acc_avg2 <- rfe_acc_avg %>%\n  enframe()\nrf_random_avg <- map_dbl(subset_perf, .f = \\(x) x$TrainAccuracy)\nrf_random_avg2 <- rf_random_avg %>% enframe()\n\nggplot() +\n  geom_point(data = rfe_acc_avg2, aes(x = name, y = value, colour = \"RFE\")) +\n  geom_point(data = rf_random_avg2, aes(x = name, y = value, colour = \"Random\")) +\n  geom_point(data = sim_ann_50_ext_avg, aes(x = Iter, y = Accuracy, colour = \"SA\")) +\n  geom_point(data = ga_internal_avg, aes(x = Iter, y = Accuracy, colour = \"GA\")) +\n  scale_colour_manual(values = c(\"RFE\" = \"green\", \"Random\" = \"darkgrey\", \"GA\" = \"red\", \"SA\" = \"orange\")) +\n  labs(x = \"Iterations\", y = \"Accuracy\", colour = \"Method\") +\n  lims(x = c(0, 100))\n#> Warning: Removed 400 rows containing missing values\n#> (`geom_point()`).\nwilcox.test(rfe_acc_avg, sort(rf_random_avg, decreasing = TRUE)[1:25], paired = TRUE)\n#> \n#>  Wilcoxon signed rank exact test\n#> \n#> data:  rfe_acc_avg and sort(rf_random_avg, decreasing = TRUE)[1:25]\n#> V = 272, p-value = 0.002255\n#> alternative hypothesis: true location shift is not equal to 0\nt.test(rfe_acc_avg, sort(rf_random_avg, decreasing = TRUE)[1:25], paired = TRUE)\n#> \n#>  Paired t-test\n#> \n#> data:  rfe_acc_avg and sort(rf_random_avg, decreasing = TRUE)[1:25]\n#> t = 1.9414, df = 24, p-value = 0.06404\n#> alternative hypothesis: true mean difference is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.0002307976  0.0075469039\n#> sample estimates:\n#> mean difference \n#>     0.003658053"},{"path":"final-model-exploration.html","id":"final-model-exploration","chapter":"8 Final model exploration","heading":"8 Final model exploration","text":"Let’s summarise far:variable set second level interactions think offer best accuracy can handle computational powerA variable set second level interactions think offer best accuracy can handle computational powerA set tuning parameters several models think likely optimalA set tuning parameters several models think likely optimalNow must decide model use. many options limit three: GLM, RandomForest Lasso/Ridge.\nget models, let’s first summarise preprocessing training data ’ve done.","code":""},{"path":"final-model-exploration.html","id":"summarise-preprocess","chapter":"8 Final model exploration","heading":"8.1 Summarise preprocess","text":"","code":"\n# Create group variables, seperate CabinNumber, Deck and Side from Cabin\ntrain2 <- useful_features(train)\n\n# Replace structurally missing NA\ntrain3 <- my_na_replace(train2)\ntrain3 <- useful_features(train3)\n\n# KNN impute remaining missing values\ntrain3_for_knn <- train3 %>%\n  mutate(across(.cols = where(is.factor), .fns = as.character))\n\nvars_to_impute <- c(\"HomePlanet\", \"CryoSleep\", \"Destination\", \"Age\", \"VIP\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\",\n                    \"Spa\", \"VRDeck\", \"Deck\", \"Side\", \"CabinNumber\", \"LastName\")\nvars_for_imputing <- c(\"HomePlanet\", \"CryoSleep\", \"Destination\", \"Age\", \"VIP\", \"RoomService\", \"FoodCourt\",\n                              \"ShoppingMall\", \"Spa\", \"VRDeck\", \"PassengerGroup\", \"Deck\", \"Side\", \"CabinNumber\",\n                              \"PassengerGroupSize\", \"DestinationsPerGroup\", \"CabinsPerGroup\",\n                              \"CryoSleepsPerGroup\", \"VIPsPerGroup\", \"LastNamesPerGroup\")\n\ntrain3_noNA <- train3_for_knn[complete.cases(train3_for_knn),]\n  \nknn_impute_rec <- recipe(Transported ~ ., data = train3_noNA) %>%\n  step_normalize(Age, CabinNumber, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck) %>%\n  step_impute_knn(recipe = ., all_of(vars_to_impute), impute_with = imp_vars(all_of(vars_for_imputing)), neighbors = 5) \n\nset.seed(8584)\nknn_impute_prep <- knn_impute_rec %>% prep(strings_as_factors = FALSE)\n\nset.seed(8584)\nknn_impute_bake <- bake(knn_impute_prep, new_data = train3_for_knn)\n\nknn_impute_res <- knn_impute_bake %>%\n  mutate(across(.cols = c(Age, CabinNumber, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck),\n                .fns = ~ rev_normalization(.x, knn_impute_prep)))\n\n# Fixed KNN imputation where structural missing rules were broken\nfixed_knn <- fix_knn(knn_impute_res)\ntrain4 <- useful_features2(fixed_knn)\n\n# Add new features we've discovered from our visual exploration\ntrain5 <- add_grp_features(train4)\ntrain6 <- add_name_features(train5)\n\n# Get our variables in order for modelling\ntrain7 <- train6 %>%\n  select(-c(Cabin, Name, LastName, PassengerCount, HomePlanetsPerGroup)) %>%\n  mutate(across(.cols = c(PassengerGroupSize, Solo, LargeGroup, TravelTogether, tidyselect::ends_with(\"PerGroup\")),\n                .fns = as.integer))\n\nmy_vars <- data.frame(Variables = names(train7)) %>%\n  mutate(Roles = if_else(Variables %in% c(\"PassengerId\"), \"id\", \"predictor\"),\n         Roles = if_else(Variables == \"Transported\", \"outcome\", Roles))\n\n# final_interactions <- rfe_vars_best2 %>%\n#   select(ForFormula, RevFormula) %>%\n#   unlist() %>%\n#   unname() %>%\n#   unique(.) %>%\n#   str_flatten(., collapse = \"+\") %>%\n#   str_c(\"~\", .) %>%\n#   as.formula(.)\n# \n# save(final_interactions, file = \"Final interactions.RData\")\nload(\"Final interactions.RData\")\n\nset.seed(8584)\nfinal_split <- initial_split(train7, prop = 0.8)\nfinal_train <- training(final_split)\nfinal_test <- testing(final_split)\nfinal_folds <- vfold_cv(final_train, v = 10, repeats = 5)\n\nfinal_rec_int <- recipe(x = final_train, vars = my_vars$Variables, roles = my_vars$Roles) %>%\n  step_normalize(Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, TotalSpent, CabinNumber, LastNameAsNumber, PassengerGroup,\n                 TotalSpentPerGroup) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_interact(final_interactions) %>%\n  step_zv(all_predictors()) %>%\n  step_corr(all_numeric_predictors(), threshold = 0.5)\n\nfinal_rec <- recipe(x = final_train, vars = my_vars$Variables, roles = my_vars$Roles) %>%\n  step_normalize(Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, TotalSpent, CabinNumber, LastNameAsNumber, PassengerGroup,\n                 TotalSpentPerGroup) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors()) %>%\n  step_corr(all_numeric_predictors(), threshold = 0.5)"},{"path":"final-model-exploration.html","id":"glm","chapter":"8 Final model exploration","heading":"8.2 GLM","text":"results GLM model aren’t great close ones expected resampling feature evaluation process.","code":"\nglm_final_bake <- final_rec %>%\n  prep() %>%\n  bake(new_data = NULL)\n\nmy_acc <- metric_set(accuracy)\npen_ctrl <- control_grid(verbose = TRUE, save_pred = TRUE, save_workflow = TRUE)\npen_grid <- expand.grid(mixture = seq(.2, 1, by = .2), penalty = 10^seq(-4, -1, by = 0.1))\n\nglm_final_mod <- logistic_reg() %>%\n  set_engine(\"glm\", family = \"binomial\")\n\nglm_final_wf <- workflow() %>%\n  add_recipe(final_rec) %>%\n  add_model(glm_final_mod)\n\nset.seed(8584)\nglm_final_fit <- fit(glm_final_wf, final_train)\n#> Warning: glm.fit: fitted probabilities numerically 0 or 1\n#> occurred\n\nglm_final_pred <- bind_cols(Actual = final_test$Transported, predict(glm_final_fit, final_test))\nglm_final_acc_int <- accuracy(data = glm_final_pred, truth = Actual, estimate = .pred_class)\nglm_final_acc_int\n#> # A tibble: 1 × 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy binary         0.791"},{"path":"final-model-exploration.html","id":"glmnet-lassoridge","chapter":"8 Final model exploration","heading":"8.3 GLMNET Lasso/Ridge","text":"","code":"\nbest_glm_params <- select_best(pen_int_tune)\n\nglmnet_final_mod <- logistic_reg(penalty = pluck(best_glm_params$penalty), mixture = pluck(best_glm_params$mixture)) %>%\n  set_engine(\"glmnet\", family = \"binomial\")\n\nglmnet_final_wf <- glm_final_wf %>%\n  update_model(glmnet_final_mod)\n\nset.seed(8584)\nglmnet_final_fit <- fit(glmnet_final_wf, final_train)\n\nglmnet_final_pred <- bind_cols(Actual = final_test$Transported, predict(glmnet_final_fit, final_test))\nglmnet_final_acc <- accuracy(data = glmnet_final_pred, truth = Actual, estimate = .pred_class)\nglmnet_final_acc\n#> # A tibble: 1 × 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy binary         0.787"},{"path":"final-model-exploration.html","id":"randomforest","chapter":"8 Final model exploration","heading":"8.4 RandomForest","text":"","code":"\nmy_acc <- metric_set(accuracy)\nrf_ctrl <- control_grid(verbose = TRUE, save_pred = TRUE, save_workflow = TRUE)\nrf_grid <- expand.grid(trees = c(500, 2000, 10000), min_n = 5)\n\nrf_final_mod_tune <- rand_forest(trees = tune(), min_n = tune()) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"randomForest\")\n\n# my_cluster <- makeCluster(detectCores() - 1, type = 'SOCK')\n# registerDoSNOW(my_cluster)\n# clusterExport(cl = my_cluster, \"final_interactions\")\n# \n# system.time({\n#   set.seed(8584)\n#   rf_final_tune <- rf_final_mod_tune %>%\n#     tune_grid(final_rec, resamples = final_folds, metrics = my_acc, control = rf_ctrl, grid = rf_grid)\n# })\n# \n# save(rf_final_tune, file = \"Final randomForest tune.RData\")\n# \n# stopCluster(my_cluster)\n# unregister()\n\nload(\"Final randomForest tune.RData\")\n\nshow_best(rf_final_tune, metric = \"accuracy\", n = 20) %>%\n  ggplot(aes(x = trees, y = mean)) +\n  geom_line() +\n  geom_point() +\n  scale_x_log10() +\n  labs(title = \"Tune results for RandomForest\", x = \"Number of trees\", y = \"Resample accuracy\")\nrf_final_mod <- rand_forest(trees = 2000, min_n = 5) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"randomForest\")\n\nrf_final_wf <- glm_final_wf %>%\n  update_model(rf_final_mod)\n\nset.seed(8584)\nrf_final_fit <- fit(rf_final_wf, final_train)\n\nrf_final_pred <- bind_cols(Actual = final_test$Transported, predict(rf_final_fit, final_test))\nrf_final_acc <- accuracy(data = rf_final_pred, truth = Actual, estimate = .pred_class)\nrf_final_acc\n#> # A tibble: 1 × 3\n#>   .metric  .estimator .estimate\n#>   <chr>    <chr>          <dbl>\n#> 1 accuracy binary         0.799"}]
