# Exploration of categorical variables
## Chi-squared tests
The traditional statistical test between two categorical values is the chi\^2 test, which compares the actual frequencies of each combination of categorical variables to a perfectly independent frequency (a frequency that would occur if each combination had the same ratio as the overall ratio for one variable). The null hypothesis for the chi\^2 test is that there is no relationship between the variables.

I use a set of helper functions to iterate over the different categorical variables and then summarize the results in tables.
```{r chi-tests, warning = FALSE}
my_chisq_test <- function(var1, var2) { # Test and extract Chi^2 statistics
  res <- rstatix::chisq_test(x = var1, y = var2)
  ex_res <- c(chi = res$statistic, pval = res$p)
  return(ex_res)
}

chi_sqr_res <- train4 %>%
  select(HomePlanet, Destination, Deck, Side, CryoSleep, VIP, LastName) %>%
  summarise(chi_stats = map(.x = across(everything()), .f = \(x) map(.x = across(everything()), .f = \(y) my_chisq_test(x, y))))

chi_sqr_res2 <- chi_sqr_res %>%
  unnest() %>%
  unnest_wider(col = everything()) %>%
  rename(chi_sqr = `chi.X-squared`, p_value = pval) %>%
  bind_cols(expand.grid(Var1 = c("HomePlanet", "Destination", "Deck", "Side", "CryoSleep", "VIP", "LastName"), 
                        Var2 = c("HomePlanet", "Destination", "Deck", "Side", "CryoSleep", "VIP", "LastName")), .)

chi_sqr_res2 %>%
  select(Var1, Var2, chi_sqr, p_value) %>%
  filter(p_value > 0.05 & Var1 != Var2) %>%
  distinct(chi_sqr, .keep_all = TRUE) %>%
  arrange(as.character(Var1), desc(p_value))

signif_chi <- chi_sqr_res2 %>%
  select(Var1, Var2, chi_sqr, p_value) %>%
  filter(p_value <= 0.05 & Var1 != Var2) %>%
  distinct(chi_sqr, .keep_all = TRUE) %>%
  arrange(as.character(Var1), p_value)

signif_chi
```

If we use the typical 95% confidence interval as a cut-off point for comparison, we can see that only two pairs of variables have p-values higher than 5%, which means that these variables are independent of each other. All other variables are related in some way. One way we can explore this further is to visualize the levels of the categorical variables in mosaic plots.

## Mosaic plots
Here I use a custom function from Kuhn's and Johnsson's code to improve the default mosaic plot and plot the categorical variables against each other with the help of contigency tables for each pair. I've left out LastName since it has too many levels.
```{r mosaic, fig.cap = "Mosaic plots to visualize the relationships between categorical variables"}
my_mosaic <- function(dtab) { 
  mosaic(dtab, pop = FALSE, highlighting = TRUE, highlighting_fill = rainbow_hcl, margins = unit(c(6, 1, 1, 8), "lines"),
         labeling = labeling_border(rot_labels = c(90, 0, 0, 0), just_labels = c("left", "right", "center",  "right"),
                                    offset_varnames = unit(c(3, 1, 1, 4), "lines")), 
         keep_aspect_ratio = FALSE)
}

# Produces too many plots at once but it's useful to see all interactions at once
# train4 %>% 
#   select(HomePlanet, Destination, Deck, Side, VIP, CryoSleep, Transported) %>%
#   mutate(walk(.x = across(everything()), .f = \(x) walk(.x = across(everything()), .f = \(y) my_mosaic(base::table(x, y)))))

my_mosaic(base::table(Destination = train4$Destination, Deck = train4$Deck))
```

If we take a look at some of the variable pairs, we can see that the height of the coloured bars changes when we move across horisontally. For example, the share of passengers that are in cryosleep changes with the deck and since the p-value from the Chi\^2 test is very low, the change is significant. This is harder to see visually for CryoSleep and Side and this is also reflected in the p-value that is almost at our threshold of 5%. 

We also see that destinations are clearly connected to homeplanets which we might want to use to create a feature HomeDestination that perhaps adds predictive power to the model. Whether it does will depend on whether a specific HomeDestination is more important to why the passenger was transported than HomePlanet and Destination by themselves.

## Correspondence Analysis
Correspondence analysis uses the residuals from the contingency table of two variables compared to the frequency table the variables would've had if they were unrelated to 
Kuhn and Johnson also recommend the `CA` function from the `FactoMineR`-package that performs a correspondence analysis which maps the levels of the categorical variables into a two-dimensional space. It is similar to a principal component analysis.

```{r correspondense-analysis, results = "hide", fig.cap = "Correspondance analysis of categorical variables that have more than two categories"}
plot(CA(base::table(Deck = train4$Deck, Destination = train4$Destination), graph = FALSE), title = "Destination and Deck")
plot(CA(base::table(Deck = train4$Deck, HomePlanet = train4$HomePlanet), graph = FALSE), title = "HomePlanet and Deck")
```

The way to read these plots is as follows:
- The horizontal and vertical components show the amount of the Chi\^2 statistic that each respective component accounts for. Another way to think of this is what amount of information from the original variables is explained by either the horizontal or vertical direction. The eigenvalues inside the parenthesis gives us a sense of which component is more important. For example, the horizontal axis is much more important in both our cases.
- The distance from the origin in either direction tells us how rare the particular category is. For example, Deck T has only five passengers so it's very rare.
- Categories from the same variable that are close together (particularly along the horizontal axis in our case) suggest similarities between the categories. For example, decks A-C seem to have a similar variation.
- Categories from different variables that are close together suggest a dependence and especially if the distance from the origin is large. A trick to see the strength of the relationship is to imagine a straight line from one category to the orgin and then to the other category. The greater the distance and the smaller the angle, the stronger the relationship. For example, if we draw a line from deck A to the origin and then to Cancri, we get a relatively long line and a small angle. 

We can check if these insights make sense in our mosaic plot above. For example, decks A-C do seem to an unusually high proportion of passengers that travel to Cancri, compared to (what we can visually imagine) is the average for the entire dataset. Decks D-G do seem to be more common for travellers to Trappist but the relationship is relatively moderate (again, compared to the average). 

What is the main takeaway? It might make sense to either pool some groups, like decks A-C into a single category to reduce the noise our models need to deal with. 

## Relationships between categorical variables and the outcome
So far, we've only look at relationships between the categoric predictors but not how they are related to the response. We can explore this with a binominal proportion test (again, the code is heavily borrowed from Kuhn and Johnson). This is in a sense similar to the chi\^2 test in that it compares the actual proportions within a group to some given or expected proportion, in this case the proportion of those that were transported from the total number of passengers.
```{r binominal, warning = FALSE, results = "hide", fig.cap = "Binominal distribution plots for categorical variables and the average response"}
response_rate = mean(train4$Transported == "True")

my_binom <- function(df, t) { # Test whether the differences in proportions (of transported) are significant
  p <- infer::prop_test(x = df, response = !!sym(t))
  df2 <- df %>%
    mutate(Lower = p$lower_ci, 
           Upper = p$upper_ci,
           Proportion = sum(!!sym(t) == "True") / length(!!sym(t)))
  return(df2)
}

my_binom_plot <- function(df, v, t) { # Applies the proportion test to different categories of a variable and plots the results
  df %>%
    group_split(!!sym(v)) %>%
    map(.x = ., .f = \(df) my_binom(df, t)) %>%
    bind_rows() %>%
    mutate(my_var = reorder(!!sym(v), Proportion)) %>%
    ggplot(., aes(x = my_var, y = Proportion)) +
    geom_errorbar(aes(ymin = 1 - Lower, ymax = 1 - Upper), width = .1) +
    geom_point() +
    geom_hline(yintercept = response_rate, col = "red", alpha = .35, lty = 2) + 
    scale_y_continuous(breaks = seq(0, 1, 0.1), limits = c(0, 1)) +
    theme(axis.text = element_text(size = 8)) +
    labs(x = "", title = v)
}

train4 %>%
  select(PassengerGroupSize, DestinationsPerGroup, CabinsPerGroup, VIPsPerGroup, LastNamesPerGroup, Transported) %>%
  summarise(across(.cols = -Transported, .fns = list)) %>%
  iwalk(~print(my_binom_plot(train4, .y, "Transported")))
```

We're already explored the core variables so here I want to focus on some of the simple features we've derived by counting categories per group.
1. PassengerGroupSize suggests that there might be a difference between passengers who travel alone and those who don't. We might also want to create a variable for large groups > 7 although this might overfit the data since only around 1% of passengers would be in this category in the train data.
2. DestinationsPerGroup suggests that there might be some slight difference between groups of passengers that all travelled to the same destination and those that didn't.
3. CabinsPerGroup doesn't seem to add much value
4. VIPsPerGroup suggests that the chances of being transported are higher for passengers who are in a group where another passenger has a VIP ticket. We'll explore this a bit more below since it seems to go against what we can see from the VIP variable where having a VIP ticket seems to reduce the chance of transportation.
5. CabinsPerGroup doesn't seem to add much value
```{r new-features}
train5 <- train4 %>%
  mutate(Solo = if_else(PassengerGroup == 1, 1, 0),
         LargeGroup = if_else(as.integer(PassengerGroup) > 7, 1, 0),
         TravelTogether = if_else(DestinationsPerGroup == 1, 1, 0))
```
\
Let's explore the VIP variables a bit more.
```{r VIP-variable, warning = FALSE, results = "hide"}
train5 %>%
  select(VIP, VIPsPerGroup, Transported) %>%
  summarise(across(.cols = -Transported, .fns = list)) %>%
  iwalk(~print(my_binom_plot(train4, .y, "Transported")))

g1 <- train5 %>%
  select(CryoSleep, Transported, VIPsPerGroup) %>%
  ggplot(., mapping = aes(x = CryoSleep, fill = Transported)) +
  geom_bar() +
  geom_text(aes(by = CryoSleep), stat = "prop", position = position_stack(vjust = 0.5)) +
  facet_wrap(~VIPsPerGroup, labeller = "label_both") +
  labs(title = "CryoSleep share with different VIPsPerGroup", x = "CryoSleep", y = "Number of passengers") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

g2 <- train5 %>%
  select(CryoSleep, Transported) %>%
  ggplot(., mapping = aes(x = CryoSleep, fill = Transported)) +
  geom_bar() +
  geom_text(aes(by = CryoSleep), stat = "prop", position = position_stack(vjust = 0.5)) +
  labs(title = "CryoSleep share of total data", x = "CryoSleep", y = "Number of passengers") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

g3 <- train5 %>%
  filter(VIP == "True") %>%
  select(CryoSleep, Transported) %>%
  ggplot(., mapping = aes(x = CryoSleep, fill = Transported)) +
  geom_bar() +
  geom_text(aes(by = CryoSleep), stat = "prop", position = position_stack(vjust = 0.5)) +
  labs(title = "CryoSleep share for those with VIP", x = "CryoSleep", y = "Number of passengers") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

g1 + g2 + g3 + plot_layout(guides = "collect")
```

The proportion of people transported seems to be higher for groups with more than a single VIP-ticket even when we control for CryoSleep. On the other hand, passengers with VIP-tickets seem slightly less likely to be transported which suggests that we should keep both variables for modelling.