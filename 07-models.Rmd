# Final model tuning {#chapter-8}
The exploration of the data has so far yielded a set of features, including second level interactions, that we think offers the best accuracy. We've tested them through several feature selection processes and have gotten some insight on this models performance through resampling evaluations.

Now we must decide which models, or assembly of models, we want to use for our final prediction. There are many different model engines that can handle classification problems, with different pros and cons. Here is a list of the ones we will be using:

```{r modelinfo, echo=FALSE, cache=TRUE}
modelinfo <- readxl::read_xlsx("Extra/Modelinfo.xlsx")

knitr::kable(modelinfo) %>%
  kableExtra::kable_styling(bootstrap_options = "striped")
```

## Complete preprocess
```{r functions, include=FALSE}
useful_features <- function(x) {
  x2 <- x %>%
    mutate(PassengerGroup = str_split_i(PassengerId, "_", 1),
           LastName = word(Name, -1),
           Deck = str_split_i(Cabin, "/", 1),
           CabinNumber = str_split_i(Cabin, "/", 2),
           Side = str_split_i(Cabin, "/", 3),
           TotalSpent = RoomService + FoodCourt + ShoppingMall + Spa + VRDeck,
           PassengerCount = 1) %>% # I added this to use as a count variable in visualizations
    group_by(PassengerGroup) %>%
    add_count(PassengerGroup, name = "PassengerGroupSize") %>%
    mutate(HomePlanetsPerGroup = n_distinct(HomePlanet, na.rm = TRUE),
           DestinationsPerGroup = n_distinct(Destination, na.rm = TRUE),
           CabinsPerGroup = n_distinct(Cabin, na.rm = TRUE),
           TotalSpentPerGroup = sum(TotalSpent, na.rm = TRUE),
           CryoSleepsPerGroup = n_distinct(CryoSleep, na.rm = TRUE),
           VIPsPerGroup = n_distinct(VIP, na.rm = TRUE),
           LastNamesPerGroup = n_distinct(LastName, na.rm = TRUE)) %>%
    ungroup() %>%
    mutate(across(.cols = c(HomePlanet, CryoSleep, Destination, VIP, Deck, Side, HomePlanetsPerGroup,
                            PassengerGroupSize, DestinationsPerGroup, CabinsPerGroup, CryoSleepsPerGroup, VIPsPerGroup,
                            LastNamesPerGroup, PassengerGroup, PassengerId),
                  .fns = as.factor)) %>%
    mutate(across(.cols = c(CabinNumber, Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck),
                  .fns = as.integer))
  return(x2)
}

my_na_replace <- function(d) {
  d2 <- d %>%
    # Replace HomePlanet for passengers in groups where the homeplanet is known from the other passengers
    group_by(PassengerGroup) %>% 
    fill(HomePlanet, .direction = "downup") %>% 
    
    # Replace Cabin by group cabin for groups with group count > 1. Update the Deck, CabinNumber and Side variables.
    mutate(Cabin2 = Cabin) %>%
    fill(data = ., Cabin2, .direction = "downup") %>%
    ungroup() %>%
    mutate(Cabin = if_else(is.na(Cabin) & PassengerGroupSize != 1, Cabin2, Cabin),
           Deck = str_split_i(Cabin, "/", 1),
           CabinNumber = str_split_i(Cabin, "/", 2),
           Side = str_split_i(Cabin, "/", 3)) %>%
    select(-Cabin2) %>%
    
    # Replace HomePlanet if the passenger is housed on a dedicated Deck
    mutate(HomePlanet = if_else(is.na(HomePlanet) & Deck == "G", "Earth", HomePlanet),
           HomePlanet = if_else(is.na(HomePlanet) & Deck %in% c("A", "B", "C"), "Europa", HomePlanet)) %>%
    
    # Replace all of VIPs from Earth to FALSE
    mutate(VIP = if_else(is.na(VIP) & HomePlanet == "Earth", "False", VIP)) %>%
    
    # Replace amenities with zero if CryoSleep is TRUE or if Age <= 12
    # Replace CryoSleep with FALSE if the passenger has spent credits
    mutate(across(.cols = c(RoomService, FoodCourt, ShoppingMall, Spa, VRDeck), 
                  .fns = ~ if_else(condition = CryoSleep == "True" | Age <= 12, true = 0, false = .x, missing = .x)),
           CryoSleep = if_else(TotalSpent > 0 & is.na(CryoSleep), "False", CryoSleep))
  return(d2)
}

useful_features2 <- function(x) {
  x2 <- x %>%
    mutate(TotalSpent = RoomService + FoodCourt + ShoppingMall + Spa + VRDeck) %>%
    group_by(PassengerGroup) %>%
    add_count(PassengerGroup, name = "PassengerGroupSize") %>%
    mutate(HomePlanetsPerGroup = n_distinct(HomePlanet, na.rm = TRUE),
           DestinationsPerGroup = n_distinct(Destination, na.rm = TRUE),
           CabinsPerGroup = n_distinct(Cabin, na.rm = TRUE),
           TotalSpentPerGroup = sum(TotalSpent, na.rm = TRUE),
           CryoSleepsPerGroup = n_distinct(CryoSleep, na.rm = TRUE),
           VIPsPerGroup = n_distinct(VIP, na.rm = TRUE),
           LastNamesPerGroup = n_distinct(LastName, na.rm = TRUE)) %>%
    ungroup() %>%
    mutate(across(.cols = c(HomePlanet, CryoSleep, Destination, VIP, Deck, Side, HomePlanetsPerGroup,
                            PassengerGroupSize, DestinationsPerGroup, CabinsPerGroup, CryoSleepsPerGroup, VIPsPerGroup,
                            LastNamesPerGroup, PassengerGroup, PassengerId),
                  .fns = as.factor)) %>%
    mutate(across(.cols = c(CabinNumber, Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck),
                  .fns = as.integer))
  return(x2)
}

fix_knn <- function(df) {
  amenities_summary <- df %>%
    filter(Age > 12 & TotalSpent > 0) %>%
    summarise(mean_age = round(mean(Age), 0), .by = c(CryoSleep, Deck))
  
  wrong_age <- df %>%
    filter(Age <= 12 & TotalSpent > 0) %>%
    left_join(., amenities_summary, by = c("CryoSleep", "Deck")) %>%
    select(PassengerId, mean_age)
  
  wrong_planet <- df %>%
    filter(HomePlanetsPerGroup != 1) %>%
    select(PassengerId, PassengerGroup, HomePlanet) %>%
    group_by(PassengerGroup) %>%
    mutate(HomePlanet_correct = first(HomePlanet)) %>%
    ungroup() %>%
    select(PassengerId, HomePlanet_correct)
  
  wrong_deck <- df %>%
    filter(Deck %in% c("A", "B", "C", "G")) %>%
    mutate(Deck_correct = case_when(Deck %in% c("A", "B", "C") & HomePlanet == "Earth" ~ "G",
                                    Deck %in% c("A", "B", "C") & HomePlanet == "Mars" ~ "F",
                                    Deck == "G" & HomePlanet == "Mars" ~ "F",
                                    Deck == "G" & HomePlanet == "Europa" ~ "C", .default = Deck)) %>%
    select(PassengerId, Deck_correct)
  
  res <- df %>%
  left_join(., wrong_age, by = "PassengerId") %>%
  left_join(., wrong_planet, by = "PassengerId") %>%
  left_join(., wrong_deck, by = "PassengerId") %>%
  mutate(Age = if_else(Age <= 12 & TotalSpent > 0, mean_age, Age),
         RoomService = if_else(CryoSleep == "True", 0, RoomService),
         FoodCourt = if_else(CryoSleep == "True", 0, FoodCourt),
         ShoppingMall = if_else(CryoSleep == "True", 0, ShoppingMall),
         Spa = if_else(CryoSleep == "True", 0, Spa),
         VRDeck = if_else(CryoSleep == "True", 0, VRDeck),
         HomePlanet = if_else(HomePlanetsPerGroup != 1, HomePlanet_correct, HomePlanet),
         Deck_correct = coalesce(Deck_correct, Deck),
         Deck = Deck_correct) %>%
  select(-mean_age, -HomePlanet_correct, -Deck_correct)
  
  return(res)
}

add_grp_features <- function(df) {
  res <- df %>%
  mutate(Solo = if_else(PassengerGroupSize == 1, 1, 0),
         LargeGroup = if_else(as.integer(PassengerGroupSize) > 7, 1, 0),
         TravelTogether = if_else(DestinationsPerGroup == 1, 1, 0))
}

encode_cat_to_numeric <- function(x) {
  x <- factor(x, ordered = FALSE)
  x <- unclass(x)
  return(x)
}

add_name_features <- function(df) {
  res <- df %>%
  mutate(LastNameAsNumber = encode_cat_to_numeric(LastName)) %>%
  add_count(x = ., LastNameAsNumber, name = "LastNameCount") %>%
  mutate(across(.cols = c(PassengerGroup, LastNameAsNumber), .fns = as.integer))
}

rev_normalization <- function(v, rec) { # Custom function that will "unnormalise" numeric values inside mutate(across())
  tidy_rec <- tidy(rec, number = 1)
  v2 <- v * filter(tidy_rec, terms == cur_column() & statistic == "sd")$value + 
    filter(tidy_rec, terms == cur_column() & statistic == "mean")$value
  v3 <- round(v2, 0)
  return(v3)
}

bin_for_zero <- function(df) {
  res <- df %>% mutate(ZeroRoomService = if_else(CryoSleep == "False" & RoomService == 0, 1, 0),
                       ZeroFoodCourt = if_else(CryoSleep == "False" & FoodCourt == 0, 1, 0),
                       ZeroShoppingMall = if_else(CryoSleep == "False" & ShoppingMall == 0, 1, 0),
                       ZeroSpa = if_else(CryoSleep == "False" & Spa == 0, 1, 0),
                       ZeroVRDeck = if_else(CryoSleep == "False" & VRDeck == 0, 1, 0))
  return(res)
}
```

Let's summarise the entire preprocess for the training data before we get to our models.

```{r summarise-preprocess, echo=FALSE}
train <- read_csv("train.csv", na = "", col_types = "ccfccnfnnnnncf")

# Create group variables, seperate CabinNumber, Deck and Side from Cabin
train2 <- useful_features(train)

# Replace structurally missing NA
train3 <- my_na_replace(train2)
train3 <- useful_features(train3)

# KNN impute remaining missing values
train3_for_knn <- train3 %>%
  mutate(across(.cols = where(is.factor), .fns = as.character))

vars_to_impute <- c("HomePlanet", "CryoSleep", "Destination", "Age", "VIP", "RoomService", "FoodCourt", "ShoppingMall",
                    "Spa", "VRDeck", "Deck", "Side", "CabinNumber", "LastName")
vars_for_imputing <- c("HomePlanet", "CryoSleep", "Destination", "Age", "VIP", "RoomService", "FoodCourt",
                              "ShoppingMall", "Spa", "VRDeck", "PassengerGroup", "Deck", "Side", "CabinNumber",
                              "PassengerGroupSize", "DestinationsPerGroup", "CabinsPerGroup",
                              "CryoSleepsPerGroup", "VIPsPerGroup", "LastNamesPerGroup")

train3_noNA <- train3_for_knn[complete.cases(train3_for_knn),]
  
knn_impute_rec <- recipe(Transported ~ ., data = train3_noNA) %>%
  step_normalize(Age, CabinNumber, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck) %>%
  step_impute_knn(recipe = ., all_of(vars_to_impute), impute_with = imp_vars(all_of(vars_for_imputing)), neighbors = 5) 

set.seed(8584)
knn_impute_prep <- knn_impute_rec %>% prep(strings_as_factors = FALSE)

set.seed(8584)
knn_impute_bake <- bake(knn_impute_prep, new_data = train3_for_knn)

knn_impute_res <- knn_impute_bake %>%
  mutate(across(.cols = c(Age, CabinNumber, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck),
                .fns = ~ rev_normalization(.x, knn_impute_prep)))

# Fixed KNN imputation where structural missing rules were broken
fixed_knn <- fix_knn(knn_impute_res)
train4 <- useful_features2(fixed_knn)

# Add new features we've discovered from our visual exploration
train5 <- add_grp_features(train4)
train6 <- add_name_features(train5)
train7 <- bin_for_zero(train6)

# Get our variables in order for modelling
final_df <- train7 %>%
  select(-c(PassengerCount, HomePlanetsPerGroup, Cabin, Name, LastName)) %>%
  mutate(across(.cols = c(PassengerGroupSize, ends_with("PerGroup")), .fns = as.integer),
         Transported = as.factor(Transported))

my_vars <- data.frame(Variables = names(final_df)) %>%
  mutate(Roles = if_else(Variables %in% c("PassengerId"), "id", "predictor"),
         Roles = if_else(Variables == "Transported", "outcome", Roles))

load("Extra/Best interactions.RData")

int_formula <- pen_int_vars %>%
  select(ForFormula, RevFormula) %>%
  unlist() %>%
  unname() %>%
  str_flatten(., collapse = "+") %>%
  str_c("~", .) %>%
  as.formula(.)

load("Extra/Best variables.RData")
best_vars2 <- c(best_vars, "ZeroRoomService", "ZeroFoodCourt", "ZeroShoppingMall", "ZeroSpa", "ZeroVRDeck")

vars_to_normalize <- c("Age", "RoomService", "FoodCourt", "ShoppingMall", "Spa", "VRDeck", "PassengerGroup", "CabinNumber",
                       "TotalSpent", "TotalSpentPerGroup", "LastNameAsNumber")

selector_rec <- recipe(x = final_df, vars = my_vars$Variables, roles = my_vars$Roles) %>%
  step_normalize(all_of(!!vars_to_normalize)) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(int_formula) %>%
  step_zv(all_predictors()) %>%
  step_select(all_outcomes(), contains("_x_"), all_of(matches(str_c(best_vars2, collapse = "|"))), skip = TRUE)

final_df2 <- selector_rec %>%
  prep() %>%
  bake(new_data = NULL)

set.seed(8584)
final_split <- initial_split(final_df2, prop = 0.8)
final_train <- training(final_split)
final_test <- testing(final_split)
final_folds <- bootstraps(final_train, times = 25)

final_rec <- recipe(Transported ~ ., data = final_train)
```

## Logistic regression - GLM
```{r glm, cache=TRUE, message=FALSE, warning=FALSE, fig.cap="Tuning results for the GLM model."}
glm_final_mod <- logistic_reg() %>%
  set_engine("glm", family = "binomial")

glm_final_wf <- workflow() %>%
  add_recipe(final_rec) %>%
  add_model(glm_final_mod)

glm_control <- control_resamples(save_pred = TRUE, save_workflow = TRUE)

# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# snow::clusterExport(cl = my_cluster, "int_formula")
# 
# system.time({
#   set.seed(8584)
#   glm_final_fit_resamples <- fit_resamples(glm_final_wf, final_folds, control = glm_control)
# })
# 
# save(glm_final_fit_resamples, file = "Extra/Final GLM tune.RData")
# 
# snow::stopCluster(my_cluster)
# unregister()

load("Extra/Final GLM tune.RData")

bind_preds <- function(df, t) {
  res <- map(.x = t, .f = \(x) df %>% mutate("Pred_{{x}}" := if_else(.pred_True < x, "False", "True"), .keep = "none"))
  res <- bind_cols(df, res, .name_repair = "unique_quiet")
  return(res)
}

calc_acc <- function(df) {
  res <- df %>% mutate(across(.cols = starts_with("Pred"), .fns = as.factor),
                       across(.cols = starts_with("Pred"), .fns = ~accuracy_vec(truth = df$Transported, estimate = .x)))
  return(res)
}

glm_resample_pred <- collect_predictions(glm_final_fit_resamples, summarize = TRUE)

glm_thresholds <- seq(0.1, 0.9, 0.05)

glm_resample_acc <- glm_resample_pred %>%
  select(Transported, .pred_False, .pred_True) %>%
  bind_preds(., glm_thresholds) %>%
  calc_acc(.) %>%
  summarise(across(.cols = starts_with("Pred"), .fns = mean)) %>%
  pivot_longer(everything()) %>%
  mutate(name = as.numeric(str_split_i(name, "_", 2)))

glm_p1 <- ggplot(glm_resample_acc, aes(x = name, y = value)) +
  geom_point() +
  labs(x = "Probability threshold", y = "Accuracy") +
  theme(legend.position = "none", axis.text.x = element_text(angle = 90)) +
  scale_x_continuous(breaks = glm_thresholds)

glm_thresholds2 <- seq(0.4, 0.5, 0.01)

glm_resample_acc2 <- glm_resample_pred %>%
  select(Transported, .pred_False, .pred_True) %>%
  bind_preds(., glm_thresholds2) %>%
  calc_acc(.) %>%
  summarise(across(.cols = starts_with("Pred"), .fns = mean)) %>%
  pivot_longer(everything()) %>%
  mutate(name = as.numeric(str_split_i(name, "_", 2)))

glm_p2 <- ggplot(glm_resample_acc2, aes(x = name, y = value)) +
  geom_point() +
  labs(x = "Probability threshold", y = "Accuracy") +
  theme(legend.position = "none", axis.text.x = element_text(angle = 90)) +
  scale_x_continuous(breaks = glm_thresholds2)

glm_p1 + glm_p2 + plot_layout(axis_titles = "collect_y")

glm_final_fit <- fit(glm_final_wf, final_train)
glm_final_pred <- predict(glm_final_fit, final_test, type = "prob") %>%
  mutate(Prediction = as.factor(if_else(.pred_True < 0.46, "False", "True")))
glm_final_acc <- accuracy_vec(final_test$Transported, glm_final_pred$Prediction)
```

Based on resamples, it seems that the best threshold for the probability cutoff is 0.46 instead of the normal 0.5. The best accuracy we seem able to get is just above 0.8 with the glm-model.

## Penalized logistic regression - GLMNET
```{r glmnet, cache=TRUE, message=FALSE, warning=FALSE, fig.cap="Tuning results for the GLMNET model."}
glmnet_final_mod <- logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet", family = "binomial")

glmnet_final_wf <- glm_final_wf %>%
  update_model(glmnet_final_mod)

my_acc <- metric_set(accuracy)
glmnet_ctrl <- control_grid(save_pred = TRUE, save_workflow = TRUE)
glmnet_grid <- expand.grid(mixture = 1, penalty = seq(1e-07, 1e-04, 1e-06))

# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# 
# system.time({
#   set.seed(8584)
#   glmnet_final_tune <- glmnet_final_wf %>%
#     tune_grid(object = ., resamples = final_folds, metrics = my_acc, control = glmnet_ctrl, grid = glmnet_grid)
# })
# 
# save(glmnet_final_tune, file = "Extra/Final glmnet tune.RData")
# 
# snow::stopCluster(my_cluster)
# unregister()

load("Extra/Final glmnet tune.RData")

show_best(glmnet_final_tune, metric = "accuracy", n = 20) %>%
  mutate(mixture = as.factor(round(mixture, 2))) %>%
  ggplot(aes(x = penalty, y = mean, label = mixture, colour = mixture)) +
  geom_line() +
  geom_point() +
  labs(title = "Final tune results", x = "Lambda penalty", y = "Resample accuracy", colour = "Alpha")

set.seed(8584)
glmnet_final_fit <- fit_best(glmnet_final_tune)

glmnet_final_pred <- predict(glmnet_final_fit, final_test, type = "class")
glmnet_final_acc <- accuracy_vec(final_test$Transported, glmnet_final_pred$.pred_class)
```
## Generalized additive models - GAM
```{r gam-final}
gam_final_mod <- gen_additive_mod(select_features = FALSE) %>%
  set_engine("mgcv") %>%
  set_mode("classification")

f <- reformulate(setdiff(colnames(final_train), "Transported"), response = "Transported")

set.seed(8584)
gam_final_fit <- fit(object = gam_final_mod, formula = f, data = final_train)

gam_final_pred <- predict(gam_final_fit, final_test, type = "class")
gam_final_acc <- accuracy_vec(final_test$Transported, gam_final_pred$.pred_class)
```

## SVM
The parsnip package offers three different SVM engines.

**SVM_linear** defines a SVM model that uses a linear class boundary. For classification, the model tries to maximize the width of the margin between classes using a linear class boundary. There is a single tuning parameter (named *cost* in parsnip) that determines how strict misclassification should be treated. The stricter the cost, the greater risk of overfitting.

```{r svm-linear-final, cache=TRUE, message=FALSE, warning=FALSE, fig.cap="Tuning results for the SVM linear model."}
svm_linear_final_mod <- svm_linear(cost = tune()) %>%
  set_mode("classification") %>%
  set_engine("kernlab")

svm_linear_final_wf <- glm_final_wf %>%
  update_model(svm_linear_final_mod)

my_acc <- metric_set(accuracy)
svm_linear_ctrl <- control_grid(save_pred = TRUE, save_workflow = TRUE)
svm_linear_grid <- expand.grid(cost = seq(0, 2, 0.2))

# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# 
# system.time({
#   set.seed(8584)
#   svm_linear_final_tune <- svm_linear_final_wf %>%
#     tune_grid(object = ., resamples = final_folds, metrics = my_acc, control = svm_linear_ctrl, grid = svm_linear_grid)
# })
# 
# save(svm_linear_final_tune, file = "Extra/Final SVM linear tune.RData")
# 
# snow::stopCluster(my_cluster)
# unregister()

load("Extra/Final SVM linear tune.RData")

show_best(svm_linear_final_tune, metric = "accuracy", n = 20) %>%
  ggplot(aes(x = cost, y = mean)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = seq(0, 2, 0.2)) +
  labs(title = "Final tune results", x = "Cost penalty", y = "Resample accuracy")

set.seed(8584)
svm_linear_final_fit <- fit_best(svm_linear_final_tune)

svm_linear_final_pred <- predict(svm_linear_final_fit, final_test, type = "class")
svm_linear_final_acc <- accuracy_vec(final_test$Transported, svm_linear_final_pred$.pred_class)
```

**SVM_poly** model tries to maximize the width of the margin between classes using a polynomial class boundary. There are three tuning parameters in parsnip: *cost*, *degree* and *scale*. The cost-parameter is the same as for the linear model while the degree-parameter determines the degree of the polynomial. Lower degrees might find polynomial functions that underfit while higher might overfit. The scale factor is used to scale the data and isn't necessary if the predictors are already scaled.

```{r svm-poly-final, cache=TRUE, message=FALSE, warning=FALSE, fig.cap="Tuning results for the SVM poly model."}
svm_poly_final_mod <- svm_poly(cost = tune(), degree = tune(), scale_factor = tune()) %>%
  set_mode("classification") %>%
  set_engine("kernlab")

svm_poly_final_wf <- glm_final_wf %>%
  update_model(svm_poly_final_mod)

my_acc <- metric_set(accuracy)
svm_poly_ctrl <- control_grid(save_pred = TRUE, save_workflow = TRUE)
svm_poly_grid <- expand.grid(cost = c(0.2, 0.6, 1), degree = c(1, 2, 3, 4), scale_factor = 1)

# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# 
# system.time({
#   set.seed(8584)
#   svm_poly_final_tune <- svm_poly_final_wf %>%
#     tune_grid(object = ., resamples = final_folds, metrics = my_acc, control = svm_poly_ctrl, grid = svm_poly_grid)
# })
# 
# save(svm_poly_final_tune, file = "Extra/Final SVM polynomial tune.RData")
# 
# snow::stopCluster(my_cluster)
# unregister()

load("Extra/Final SVM polynomial tune.RData")

show_best(svm_poly_final_tune, metric = "accuracy", n = 20) %>%
  ggplot(aes(x = as.factor(cost), y = mean, colour = as.factor(degree))) +
  geom_point() +
  scale_y_continuous(breaks = seq(0.7, 0.8, 0.01)) +
  labs(title = "Final tune results", x = "Cost penalty", y = "Resample accuracy", colour = "Polynomial\ndegree")

set.seed(8584)
svm_poly_final_fit <- fit_best(svm_poly_final_tune)

svm_poly_final_pred <- predict(svm_poly_final_fit, final_test, type = "class")
svm_poly_final_acc <- accuracy_vec(final_test$Transported, svm_poly_final_pred$.pred_class)
```

**SVM_rbf** model stands for Radial Basis Function and tries to maximize the width of the margin between classes using a nonlinear class boundary. It is useful to predictors that overlap since it maps the data into an infinite dimension space. It has two tuning parameters: *cost* and *$\sigma$*. The cost-parameter is the same as for the other two SVM-models while the $\sigma$-parameter can be seen as a weighting parameter to adjust the influence of nearby data.

```{r svm-rbf-final, cache=TRUE, message=FALSE, warning=FALSE, fig.cap="Tuning results for the SVM rbf model."}
svm_rbf_final_mod <- svm_rbf(cost = tune()) %>%
  set_mode("classification") %>%
  set_engine("kernlab")

svm_rbf_final_wf <- glm_final_wf %>%
  update_model(svm_rbf_final_mod)

my_acc <- metric_set(accuracy)
svm_rbf_ctrl <- control_grid(save_pred = TRUE, save_workflow = TRUE)
svm_rbf_grid <- expand.grid(cost = c(0.2, 0.6, 1, 1.2, 1.8, 3, 5))

# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# 
# system.time({
#   set.seed(8584)
#   svm_rbf_final_tune <- svm_rbf_final_wf %>%
#     tune_grid(object = ., resamples = final_folds, metrics = my_acc, control = svm_rbf_ctrl, grid = svm_rbf_grid)
# })
# 
# save(svm_rbf_final_tune, file = "Extra/Final SVM rbf tune.RData")
# 
# snow::stopCluster(my_cluster)
# unregister()

load("Extra/Final SVM rbf tune.RData")

show_best(svm_rbf_final_tune, metric = "accuracy", n = 20) %>%
  ggplot(aes(x = cost, y = mean)) +
  geom_line() +
  geom_point() +
  labs(title = "Final tune results", x = "Cost penalty", y = "Resample accuracy")

set.seed(8584)
svm_rbf_final_fit <- fit_best(svm_rbf_final_tune)

svm_rbf_final_pred <- predict(svm_rbf_final_fit, final_test, type = "class")
svm_rbf_final_acc <- accuracy_vec(final_test$Transported, svm_rbf_final_pred$.pred_class)
```

## Naive Bayes
The Naive Bayes model has two tuning parameters in the parsnip package: *smoothness* and *Laplace*. The smoothness parameter controls the models class boundaries where lower values mean that the probabilities are more closely based on what we see in the training data while a higher value means that the probabilities are more smoothed (imagine points on a graph: low smoothness would draw a complex line through each point while high smoothness might draw a simple linear line - the best fit is probably somewhere in between). 
The Laplace-parameter can be used to add a value to the probability calculations to avoid zero probabilities in cases when variables have very low frequency values or when the test data has values not present in the training data.

```{r naive-bayes-final, cache=TRUE, message=FALSE, warning=FALSE, fig.cap="Tuning results for the Naive Bayes model."}
nb_final_mod <- naive_Bayes(smoothness = tune(), Laplace = tune()) %>%
  set_engine("klaR") %>%
  set_mode("classification")

nb_final_wf <- glm_final_wf %>%
  update_model(nb_final_mod)

my_acc <- metric_set(accuracy)
nb_ctrl <- control_grid(save_pred = TRUE, save_workflow = TRUE)
nb_grid <- expand.grid(smoothness = seq(0, 5, 0.5), Laplace = 1)

# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# 
# system.time({
#   set.seed(8584)
#   nb_final_tune <- nb_final_wf %>%
#     tune_grid(object = ., resamples = final_folds, metrics = my_acc, control = nb_ctrl, grid = nb_grid)
# })
# 
# save(nb_final_tune, file = "Extra/Final Naive Bayes tune.RData")
# 
# snow::stopCluster(my_cluster)
# unregister()

load("Extra/Final Naive Bayes tune.RData")

show_best(nb_final_tune, metric = "accuracy", n = 20) %>%
  ggplot(aes(x = smoothness, y = mean)) +
  geom_line() +
  geom_point() +
  labs(title = "Final tune results", x = "Cost penalty", y = "Resample accuracy")

set.seed(8584)
nb_final_fit <- fit_best(nb_final_tune)

nb_final_pred <- predict(nb_final_fit, final_test, type = "class")
nb_final_acc <- accuracy_vec(final_test$Transported, nb_final_pred$.pred_class)
```

## KNN
We've already used the K-Nearest Neighbours function to impute data in \@ref(chapter-3), which uses a distance measure to determine how close a new observations is to its nearest neighbours. The model has two tuning parameters in parsnip: *neighbors* and *weight_func*. The neighbors-parameter is used to decide how many neighbours should be used for similarity comparison while the weigh_func-parameter is used to set the kernel that is used for the distance measure between the neighbours. There are a variety of kernels to choose from:

*Rectangular*: This is also known as the uniform kernel. It gives equal weight to all neighbors within the window, effectively creating a binary situation where points are either in the neighborhood (and given equal weight) or not.
*Triangular*: This kernel assigns weights linearly decreasing from the center. It gives the maximum weight to the nearest neighbor and the minimum weight to the farthest neighbor within the window.
*Epanechnikov*: This kernel is parabolic with a maximum at the center, decreasing to zero at the windowâ€™s edge. It is often used because it minimizes the mean integrated square error.
*Biweight*: This is a smooth, bell-shaped kernel that gives more weight to the nearer neighbors.
*Triweight*: This is similar to the biweight but gives even more weight to the nearer neighbors.
*Cos*: This kernel uses the cosine of the distance to weight the neighbors.
*Inv*: This kernel gives weights as the inverse of the distance.
*Gaussian*: This kernel uses the Gaussian function to assign weights. It has a bell shape and does not compactly support, meaning it gives some weight to all points in the dataset, but the weight decreases rapidly as the distance increases.
*Rank*: This kernel uses the ranks of the distances rather than the distances themselves.

For our purposes, we probably want to use the rectangular kernel to keep it simple. I'm not sure if any weighting makes sense for our type of variables and response.

```{r knn-final, cache=TRUE, message=FALSE, warning=FALSE, fig.cap="Tuning results for the KNN model."}
knn_final_mod <- nearest_neighbor(neighbors = tune(), weight_func = "rectangular") %>%
  set_engine("kknn") %>%
  set_mode("classification")

knn_final_wf <- glm_final_wf %>%
  update_model(knn_final_mod)

my_acc <- metric_set(accuracy)
knn_ctrl <- control_grid(save_pred = TRUE, save_workflow = TRUE)
knn_grid <- expand.grid(neighbors = seq(3, 20, 1))

# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# 
# system.time({
#   set.seed(8584)
#   knn_final_tune <- knn_final_wf %>%
#     tune_grid(object = ., resamples = final_folds, metrics = my_acc, control = knn_ctrl, grid = knn_grid)
# })
# 
# save(knn_final_tune, file = "Extra/Final KNN tune.RData")
# 
# snow::stopCluster(my_cluster)
# unregister()

load("Extra/Final KNN tune.RData")

show_best(knn_final_tune, metric = "accuracy", n = 20) %>%
  ggplot(aes(x = neighbors, y = mean)) +
  geom_line() +
  geom_point() +
  labs(title = "Final tune results", x = "Cost penalty", y = "Resample accuracy")

set.seed(8584)
knn_final_fit <- fit_best(knn_final_tune)

knn_final_pred <- predict(knn_final_fit, final_test, type = "class")
knn_final_acc <- accuracy_vec(final_test$Transported, knn_final_pred$.pred_class)
```

## C5
The C5.0 model for decision trees for has only a single tuning parameter in parsnip: *min_n*. This parameter sets the minimal node size which is a lower boundary for how many values a node must have if it is to be split further.

```{r c5-final, cache=TRUE, message=FALSE, warning=FALSE, fig.cap="Tuning results for the C5.0 model."}
c5.0_final_mod <- decision_tree(min_n = tune()) %>%
  set_mode("classification") %>%
  set_engine("C5.0")

c5.0_final_wf <- glm_final_wf %>%
  update_model(c5.0_final_mod)

my_acc <- metric_set(accuracy)
c5.0_ctrl <- control_grid(save_pred = TRUE, save_workflow = TRUE)
c5.0_grid <- expand.grid(min_n = c(2, 3, 4, 5, 8, 12, 24, 48, 72, 96, 120, 144))

# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# 
# system.time({
#   set.seed(8584)
#   c5.0_final_tune <- c5.0_final_wf %>%
#     tune_grid(object = ., resamples = final_folds, metrics = my_acc, control = c5.0_ctrl, grid = c5.0_grid)
# })
# 
# save(c5.0_final_tune, file = "Extra/Final C5_0 tune.RData")
# 
# snow::stopCluster(my_cluster)
# unregister()

load("Extra/Final C5_0 tune.RData")

show_best(c5.0_final_tune, metric = "accuracy", n = 20) %>%
  ggplot(aes(x = min_n, y = mean)) +
  geom_line() +
  geom_point() +
  labs(title = "Final tune results", x = "Minimum values in leaf node", y = "Resample accuracy")

set.seed(8584)
c5.0_final_fit <- fit_best(c5.0_final_tune)

c5.0_final_pred <- predict(c5.0_final_fit, final_test, type = "class")
c5.0_final_acc <- accuracy_vec(final_test$Transported, c5.0_final_pred$.pred_class)
```

## RandomForest
This model has three tuning parameters: *mtry*, *trees* and *min_n*. The mtry-parameter sets the number of randomly selected predictors used to initiate each tree in the forest while the tree-parameter sets the maximum number of trees that are to be used. The min_n is the same as for the C5.0 model and decides the lowest number of values that must be present in a node so that it is allowed to be split.
We will leave the mtry-parameter to its default value.

```{r randomForest-tune, cache=TRUE, message=FALSE, warning=FALSE, fig.cap="Tuning results for the RandomForest model."}
my_min_n <- select_best(c5.0_final_tune)$min_n

rf_final_mod <- rand_forest(trees = tune(), min_n = !!my_min_n) %>%
  set_engine("randomForest") %>%
    set_mode("classification")

rf_final_wf <- glm_final_wf %>%
  update_model(rf_final_mod)

my_acc <- metric_set(accuracy)
rf_ctrl <- control_grid(save_pred = TRUE, save_workflow = TRUE, parallel_over = "resamples")
rf_grid <- expand.grid(trees = seq(300, 800, 100))

# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# 
# system.time({
#   set.seed(8584)
#   rf_final_tune <- rf_final_wf %>%
#     tune_grid(., resamples = final_folds, metrics = my_acc, control = rf_ctrl, grid = rf_grid)
# })
# 
# save(rf_final_tune, file = "Extra/Final randomForest tune.RData")
# 
# snow::stopCluster(my_cluster)
# unregister()

load("Extra/Final randomForest tune.RData")

show_best(rf_final_tune, metric = "accuracy", n = 20) %>%
  ggplot(aes(x = trees, y = mean)) +
  geom_line() +
  geom_point() +
  scale_x_log10() +
  labs(title = "Tune results for RandomForest", x = "Number of trees", y = "Resample accuracy")

set.seed(8584)
rf_final_fit <- fit_best(rf_final_tune)

rf_final_pred <- predict(rf_final_fit, final_test, type = "class")
rf_final_acc <- accuracy_vec(final_test$Transported, rf_final_pred$.pred_class)
```

## XGBoost
XGBoost is a very complicated tree-based model that uses boosting where each iteration, the results of the previous iteration are used as inputs to tweak the next ensamble of trees. It has 8 tuning parameters in the parsnip package: *tree_depth*, *trees*, *learn_rate*, *mtry*, *min_n*, *loss_reduction*, *sample_size* and *stop_iter*. We can use the best values from previous tree-models for some of these and leave others to their defaults. 

The ones we will tune is the tree depth, the learn-rate and the loss reduction. The tree depth sets the maxim number of splits within each tree. The learn-rate determines how large steps between boosting iterations the model takes. The larger the steps, the better the performance but the model may miss an important solution. The loss reduction determines the minimum loss that is required to further split a node of a tree. High values lead to less overfitting but also a higher risk of underfitting while low values increase the risk of overfitting.

```{r xgboost-final, cache=TRUE, message=FALSE, warning=FALSE, fig.cap="Tuning results for the XGBoost model."}
my_min_n <- select_best(c5.0_final_tune)$min_n
my_trees <- select_best(rf_final_tune)$trees

xgb_final_mod <- boost_tree(tree_depth = tune(), learn_rate = tune(), loss_reduction = tune(),
                            min_n = !!my_min_n, trees = !!my_trees) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

xgb_final_wf <- glm_final_wf %>%
  update_model(xgb_final_mod)

my_acc <- metric_set(accuracy)
xgb_ctrl <- control_grid(save_pred = TRUE, save_workflow = TRUE)
xgb_grid <- expand.grid(tree_depth = 3, learn_rate = seq(0, 0.2, 0.02), loss_reduction = seq(0, 1, 0.5))

# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# 
# system.time({
#   set.seed(8584)
#   xgb_final_tune <- xgb_final_wf %>%
#     tune_grid(., resamples = final_folds, metrics = my_acc, control = xgb_ctrl, grid = xgb_grid)
# })
# 
# save(xgb_final_tune, file = "Extra/Final XGBoost tune.RData")
# 
# snow::stopCluster(my_cluster)
# unregister()

load("Extra/Final XGBoost tune.RData")

show_best(xgb_final_tune, metric = "accuracy", n = 110) %>%
  ggplot(aes(x = learn_rate, y = mean, colour = as.factor(loss_reduction))) +
  geom_point() +
  geom_line() +
  labs(title = "Tune results for XGBoost", x = "Learn rate", y = "Resample accuracy", colour = "Loss\nreduction")

set.seed(8584)
xgb_final_fit <- fit_best(xgb_final_tune)

xgb_final_pred <- predict(xgb_final_fit, final_test, type = "class")
xgb_final_acc <- accuracy_vec(final_test$Transported, xgb_final_pred$.pred_class)
```

```{r remove-07, include=FALSE}
rm(list = setdiff(ls(), c("glm_final_acc", "glmnet_final_acc", "gam_final_acc", "svm_linear_final_acc", "svm_poly_final_acc", 
                          "svm_rbf_final_acc", "nb_final_acc", "knn_final_acc", "c5.0_final_acc", "rf_final_acc", "xgb_final_acc", 
                          "glm_final_fit", "glmnet_final_fit", "gam_final_fit", "svm_linear_final_fit", "svm_poly_final_fit", 
                          "svm_rbf_final_fit", "nb_final_fit", "knn_final_fit", "c5.0_final_fit", "rf_final_fit", "xgb_final_fit",
                          "useful_features", "my_na_replace", "useful_features2", "fix_knn", "add_grp_features",
                          "encode_cat_to_numeric", "add_name_features", "rev_normalization", "bin_for_zero")))
```