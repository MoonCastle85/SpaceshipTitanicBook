# Final model exploration
Let's summarise what we have so far:

1. A variable set with second level interactions that we think offer the best accuracy that I can handle with my computational power

2. A set of tuning parameters for several models that we think are likely to be optimal

Now we must decide which model to use. There are many options but I will limit myself to three: GLM, RandomForest and Lasso/Ridge.
Before we get to the models, let's first summarise all the preprocessing of the training data that we've done.

## Complete preprocess
```{r functions, include=FALSE}
useful_features <- function(x) {
  x2 <- x %>%
    mutate(PassengerGroup = str_split_i(PassengerId, "_", 1),
           LastName = word(Name, -1),
           Deck = str_split_i(Cabin, "/", 1),
           CabinNumber = str_split_i(Cabin, "/", 2),
           Side = str_split_i(Cabin, "/", 3),
           TotalSpent = RoomService + FoodCourt + ShoppingMall + Spa + VRDeck,
           PassengerCount = 1) %>% # I added this to use as a count variable in visualizations
    group_by(PassengerGroup) %>%
    add_count(PassengerGroup, name = "PassengerGroupSize") %>%
    mutate(HomePlanetsPerGroup = n_distinct(HomePlanet, na.rm = TRUE),
           DestinationsPerGroup = n_distinct(Destination, na.rm = TRUE),
           CabinsPerGroup = n_distinct(Cabin, na.rm = TRUE),
           TotalSpentPerGroup = sum(TotalSpent, na.rm = TRUE),
           CryoSleepsPerGroup = n_distinct(CryoSleep, na.rm = TRUE),
           VIPsPerGroup = n_distinct(VIP, na.rm = TRUE),
           LastNamesPerGroup = n_distinct(LastName, na.rm = TRUE)) %>%
    ungroup() %>%
    mutate(across(.cols = c(HomePlanet, CryoSleep, Destination, VIP, Transported, Deck, Side, HomePlanetsPerGroup,
                            PassengerGroupSize, DestinationsPerGroup, CabinsPerGroup, CryoSleepsPerGroup, VIPsPerGroup,
                            LastNamesPerGroup, PassengerGroup, PassengerId),
                  .fns = as.factor)) %>%
    mutate(across(.cols = c(CabinNumber, Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck),
                  .fns = as.integer))
  return(x2)
}

my_na_replace <- function(d) {
  d2 <- d %>%
    # Replace HomePlanet for passengers in groups where the homeplanet is known from the other passengers
    group_by(PassengerGroup) %>% 
    fill(HomePlanet, .direction = "downup") %>% 
    
    # Replace Cabin by group cabin for groups with group count > 1. Update the Deck, CabinNumber and Side variables.
    mutate(Cabin2 = Cabin) %>%
    fill(data = ., Cabin2, .direction = "downup") %>%
    ungroup() %>%
    mutate(Cabin = if_else(is.na(Cabin) & PassengerGroupSize != 1, Cabin2, Cabin),
           Deck = str_split_i(Cabin, "/", 1),
           CabinNumber = str_split_i(Cabin, "/", 2),
           Side = str_split_i(Cabin, "/", 3)) %>%
    select(-Cabin2) %>%
    
    # Replace HomePlanet if the passenger is housed on a dedicated Deck
    mutate(HomePlanet = if_else(is.na(HomePlanet) & Deck == "G", "Earth", HomePlanet),
           HomePlanet = if_else(is.na(HomePlanet) & Deck %in% c("A", "B", "C"), "Europa", HomePlanet)) %>%
    
    # Replace all of VIPs from Earth to FALSE
    mutate(VIP = if_else(is.na(VIP) & HomePlanet == "Earth", "False", VIP)) %>%
    
    # Replace amenities with zero if CryoSleep is TRUE or if Age <= 12
    # Replace CryoSleep with FALSE if the passenger has spent credits
    mutate(across(.cols = c(RoomService, FoodCourt, ShoppingMall, Spa, VRDeck), 
                  .fns = ~ if_else(condition = CryoSleep == "True" | Age <= 12, true = 0, false = .x, missing = .x)),
           CryoSleep = if_else(TotalSpent > 0 & is.na(CryoSleep), "False", CryoSleep))
  return(d2)
}

useful_features2 <- function(x) {
  x2 <- x %>%
    mutate(TotalSpent = RoomService + FoodCourt + ShoppingMall + Spa + VRDeck) %>%
    group_by(PassengerGroup) %>%
    add_count(PassengerGroup, name = "PassengerGroupSize") %>%
    mutate(HomePlanetsPerGroup = n_distinct(HomePlanet, na.rm = TRUE),
           DestinationsPerGroup = n_distinct(Destination, na.rm = TRUE),
           CabinsPerGroup = n_distinct(Cabin, na.rm = TRUE),
           TotalSpentPerGroup = sum(TotalSpent, na.rm = TRUE),
           CryoSleepsPerGroup = n_distinct(CryoSleep, na.rm = TRUE),
           VIPsPerGroup = n_distinct(VIP, na.rm = TRUE),
           LastNamesPerGroup = n_distinct(LastName, na.rm = TRUE)) %>%
    ungroup() %>%
    mutate(across(.cols = c(HomePlanet, CryoSleep, Destination, VIP, Transported, Deck, Side, HomePlanetsPerGroup,
                            PassengerGroupSize, DestinationsPerGroup, CabinsPerGroup, CryoSleepsPerGroup, VIPsPerGroup,
                            LastNamesPerGroup, PassengerGroup, PassengerId),
                  .fns = as.factor)) %>%
    mutate(across(.cols = c(CabinNumber, Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck),
                  .fns = as.integer))
  return(x2)
}

fix_knn <- function(df) {
  amenities_summary <- df %>%
    filter(Age > 12 & TotalSpent > 0) %>%
    summarise(mean_age = round(mean(Age), 0), .by = c(CryoSleep, Deck))
  
  wrong_age <- df %>%
    filter(Age <= 12 & TotalSpent > 0) %>%
    left_join(., amenities_summary, by = c("CryoSleep", "Deck")) %>%
    select(PassengerId, mean_age)
  
  wrong_planet <- df %>%
    filter(HomePlanetsPerGroup != 1) %>%
    select(PassengerId, PassengerGroup, HomePlanet) %>%
    group_by(PassengerGroup) %>%
    mutate(HomePlanet_correct = first(HomePlanet)) %>%
    ungroup() %>%
    select(PassengerId, HomePlanet_correct)
  
  wrong_deck <- df %>%
    filter(Deck %in% c("A", "B", "C", "G")) %>%
    mutate(Deck_correct = case_when(Deck %in% c("A", "B", "C") & HomePlanet == "Earth" ~ "G",
                                    Deck %in% c("A", "B", "C") & HomePlanet == "Mars" ~ "F",
                                    Deck == "G" & HomePlanet == "Mars" ~ "F",
                                    Deck == "G" & HomePlanet == "Europa" ~ "C", .default = Deck)) %>%
    select(PassengerId, Deck_correct)
  
  res <- df %>%
  left_join(., wrong_age, by = "PassengerId") %>%
  left_join(., wrong_planet, by = "PassengerId") %>%
  left_join(., wrong_deck, by = "PassengerId") %>%
  mutate(Age = if_else(Age <= 12 & TotalSpent > 0, mean_age, Age),
         RoomService = if_else(CryoSleep == "True", 0, RoomService),
         FoodCourt = if_else(CryoSleep == "True", 0, FoodCourt),
         ShoppingMall = if_else(CryoSleep == "True", 0, ShoppingMall),
         Spa = if_else(CryoSleep == "True", 0, Spa),
         VRDeck = if_else(CryoSleep == "True", 0, VRDeck),
         HomePlanet = if_else(HomePlanetsPerGroup != 1, HomePlanet_correct, HomePlanet),
         Deck_correct = coalesce(Deck_correct, Deck),
         Deck = Deck_correct) %>%
  select(-mean_age, -HomePlanet_correct, -Deck_correct)
  
  return(res)
}

add_grp_features <- function(df) {
  res <- df %>%
  mutate(Solo = if_else(PassengerGroupSize == 1, 1, 0),
         LargeGroup = if_else(as.integer(PassengerGroupSize) > 7, 1, 0),
         TravelTogether = if_else(DestinationsPerGroup == 1, 1, 0))
}

encode_cat_to_numeric <- function(x) {
  x <- factor(x, ordered = FALSE)
  x <- unclass(x)
  return(x)
}

add_name_features <- function(df) {
  res <- df %>%
  mutate(LastNameAsNumber = encode_cat_to_numeric(LastName)) %>%
  add_count(x = ., LastNameAsNumber, name = "LastNameCount") %>%
  mutate(across(.cols = c(PassengerGroup, LastNameAsNumber), .fns = as.integer))
}
```


```{r summarise-preprocess}
train <- read.csv("train.csv", na.strings = "")

# Create group variables, seperate CabinNumber, Deck and Side from Cabin
train2 <- useful_features(train)

# Replace structurally missing NA
train3 <- my_na_replace(train2)
train3 <- useful_features(train3)

# KNN impute remaining missing values
train3_for_knn <- train3 %>%
  mutate(across(.cols = where(is.factor), .fns = as.character))

vars_to_impute <- c("HomePlanet", "CryoSleep", "Destination", "Age", "VIP", "RoomService", "FoodCourt", "ShoppingMall",
                    "Spa", "VRDeck", "Deck", "Side", "CabinNumber", "LastName")
vars_for_imputing <- c("HomePlanet", "CryoSleep", "Destination", "Age", "VIP", "RoomService", "FoodCourt",
                              "ShoppingMall", "Spa", "VRDeck", "PassengerGroup", "Deck", "Side", "CabinNumber",
                              "PassengerGroupSize", "DestinationsPerGroup", "CabinsPerGroup",
                              "CryoSleepsPerGroup", "VIPsPerGroup", "LastNamesPerGroup")

train3_noNA <- train3_for_knn[complete.cases(train3_for_knn),]
  
knn_impute_rec <- recipe(Transported ~ ., data = train3_noNA) %>%
  step_normalize(Age, CabinNumber, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck) %>%
  step_impute_knn(recipe = ., all_of(vars_to_impute), impute_with = imp_vars(all_of(vars_for_imputing)), neighbors = 5) 

set.seed(8584)
knn_impute_prep <- knn_impute_rec %>% prep(strings_as_factors = FALSE)

set.seed(8584)
knn_impute_bake <- bake(knn_impute_prep, new_data = train3_for_knn)

knn_impute_res <- knn_impute_bake %>%
  mutate(across(.cols = c(Age, CabinNumber, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck),
                .fns = ~ rev_normalization(.x, knn_impute_prep)))

# Fixed KNN imputation where structural missing rules were broken
fixed_knn <- fix_knn(knn_impute_res)
train4 <- useful_features2(fixed_knn)

# Add new features we've discovered from our visual exploration
train5 <- add_grp_features(train4)
train6 <- add_name_features(train5)

# Get our variables in order for modelling
load("Best variables.RData")
final_df <- train6 %>%
  select(all_of(best_vars), Transported, PassengerId)

my_vars <- data.frame(Variables = names(final_df)) %>%
  mutate(Roles = if_else(Variables %in% c("PassengerId"), "id", "predictor"),
         Roles = if_else(Variables == "Transported", "outcome", Roles))


load("Final interactions.RData")

set.seed(8584)
final_split <- initial_split(final_df, prop = 0.8)
final_train <- training(final_split)
final_test <- testing(final_split)
final_folds <- vfold_cv(final_train, v = 10, repeats = 5)

final_rec_int <- recipe(x = final_train, vars = my_vars$Variables, roles = my_vars$Roles) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(final_interactions) %>%
  step_zv(all_predictors()) %>%
  step_corr(all_numeric_predictors(), threshold = 0.5)

final_rec <- recipe(x = final_train, vars = my_vars$Variables, roles = my_vars$Roles) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_corr(all_numeric_predictors(), threshold = 0.5)
```

## GLM
```{r glm, warning=FALSE}
glm_final_bake <- final_rec %>%
  prep() %>%
  bake(new_data = NULL)

my_acc <- metric_set(accuracy)
pen_ctrl <- control_grid(verbose = TRUE, save_pred = TRUE, save_workflow = TRUE)
pen_grid <- expand.grid(mixture = seq(.2, 1, by = .2), penalty = 10^seq(-4, -1, by = 0.1))

glm_final_mod <- logistic_reg() %>%
  set_engine("glm", family = "binomial")

glm_final_wf <- workflow() %>%
  add_recipe(final_rec) %>%
  add_model(glm_final_mod)

set.seed(8584)
glm_final_fit <- fit(glm_final_wf, final_train)
glm_fitted <- extract_fit_engine(glm_final_fit)

glm_final_pred <- predict(glm_final_fit, final_test)
glm_est_pred <- bind_cols(final_test[, c("PassengerId", "Transported")], glm_final_pred)
accuracy(data = glm_est_pred, truth = Transported, estimate = .pred_class)
```

```{r glm-without-int, warning=FALSE, message=FALSE, fig.cap="Residuals from the GLM model fitted without interactions."}
sjPlot::plot_model(glm_fitted, type = "resid")
```

We can see from the results of residuals in Figure \@ref(fig:glm-without-int) that the GLM model (red lines) does pretty well against the categorical variables but does poorly against the numerical ones. This is because these variables don't have a linear relationship to the response, as we've seen in our exploratory chapter of numerical variables. The accuracy is very low, 0.64.

## GLMNET Lasso/Ridge
```{r glmnet}
load("Penalized regression with interactions.RData")
best_glm_params <- select_best(pen_int_tune)

glmnet_final_mod <- logistic_reg(penalty = pluck(best_glm_params$penalty), mixture = pluck(best_glm_params$mixture)) %>%
  set_engine("glmnet", family = "binomial")

glmnet_final_wf <- glm_final_wf %>%
  update_model(glmnet_final_mod)

set.seed(8584)
glmnet_final_fit <- fit(glmnet_final_wf, final_train)
glmnet_fitted <- extract_fit_engine(glmnet_final_fit)

glmnet_final_pred <- predict(glmnet_final_fit, final_test, type = "class")
glmnet_est_pred <- bind_cols(final_test, glmnet_final_pred)
accuracy(data = glmnet_est_pred, truth = Transported, estimate = .pred_class)


```

```{r glmnet-without-int, warning=FALSE, message=FALSE, fig.cap="Residuals from the penalized Lasso/Ridge model fitted without interactions."}
plot(glmnet_fitted)
# sjPlot::plot_model(glmnet_fitted)
```

## RandomForest
```{r randomForest-tune}
my_acc <- metric_set(accuracy)
rf_ctrl <- control_grid(verbose = TRUE, save_pred = TRUE, save_workflow = TRUE)
rf_grid <- expand.grid(trees = c(500, 2000, 10000), min_n = 5)

rf_final_mod_tune <- rand_forest(trees = tune(), min_n = tune()) %>%
  set_mode("classification") %>%
  set_engine("randomForest")

# my_cluster <- makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# clusterExport(cl = my_cluster, "final_interactions")
# 
# system.time({
#   set.seed(8584)
#   rf_final_tune <- rf_final_mod_tune %>%
#     tune_grid(final_rec, resamples = final_folds, metrics = my_acc, control = rf_ctrl, grid = rf_grid)
# })
# 
# save(rf_final_tune, file = "Final randomForest tune.RData")
# 
# stopCluster(my_cluster)
# unregister()

load("Final randomForest tune.RData")

show_best(rf_final_tune, metric = "accuracy", n = 20) %>%
  ggplot(aes(x = trees, y = mean)) +
  geom_line() +
  geom_point() +
  scale_x_log10() +
  labs(title = "Tune results for RandomForest", x = "Number of trees", y = "Resample accuracy")
```
The results from the model tuning indicate that the improvement is very limited with more trees than somewhere around 2000 so I will assume this as the optimal. With tree based models, too much trees can overfit the data so it's a good idea not to

```{r randomForest}
rf_best_tune <- select_best(rf_final_tune)
rf_final_mod <- rand_forest(trees = rf_best_tune$trees, min_n = rf_best_tune$min_n) %>%
  set_mode("classification") %>%
  set_engine("randomForest")

rf_final_wf <- glm_final_wf %>%
  update_model(rf_final_mod)

set.seed(8584)
rf_final_fit <- fit(rf_final_wf, final_train)
rf_fitted <- extract_fit_engine(rf_final_fit)

rf_final_pred <- bind_cols(Actual = final_test$Transported, predict(rf_final_fit, final_test))
rf_final_acc <- accuracy(data = rf_final_pred, truth = Actual, estimate = .pred_class)
rf_final_acc

# sjPlot::plot_model(rf_fitted)
```








```{r remove-07, include=FALSE}
rm(train2, train3, train3_for_knn, vars_to_impute, vars_for_imputing, train3_noNA, knn_impute_rec, knn_impute_prep, knn_impute_bake,
   knn_impute_res, fixed_knn, train4, train5, train6, my_vars, final_interactions, final_split, final_train, final_folds)
```