# Final model exploration
Let's summarise what we have so far:

1. A variable set with second level interactions that we think offer the best accuracy that I can handle with my computational power

2. A set of tuning parameters for several models that we think are likely to be optimal

Now we must decide which model to use. There are many options but I will limit myself to three: GLM, RandomForest and Lasso/Ridge.
Before we get to the models, let's first summarise all the preprocessing of the training data that we've done.

## Complete preprocess
```{r functions, include=FALSE}
useful_features <- function(x) {
  x2 <- x %>%
    mutate(PassengerGroup = str_split_i(PassengerId, "_", 1),
           LastName = word(Name, -1),
           Deck = str_split_i(Cabin, "/", 1),
           CabinNumber = str_split_i(Cabin, "/", 2),
           Side = str_split_i(Cabin, "/", 3),
           TotalSpent = RoomService + FoodCourt + ShoppingMall + Spa + VRDeck,
           PassengerCount = 1) %>% # I added this to use as a count variable in visualizations
    group_by(PassengerGroup) %>%
    add_count(PassengerGroup, name = "PassengerGroupSize") %>%
    mutate(HomePlanetsPerGroup = n_distinct(HomePlanet, na.rm = TRUE),
           DestinationsPerGroup = n_distinct(Destination, na.rm = TRUE),
           CabinsPerGroup = n_distinct(Cabin, na.rm = TRUE),
           TotalSpentPerGroup = sum(TotalSpent, na.rm = TRUE),
           CryoSleepsPerGroup = n_distinct(CryoSleep, na.rm = TRUE),
           VIPsPerGroup = n_distinct(VIP, na.rm = TRUE),
           LastNamesPerGroup = n_distinct(LastName, na.rm = TRUE)) %>%
    ungroup() %>%
    mutate(across(.cols = c(HomePlanet, CryoSleep, Destination, VIP, Deck, Side, HomePlanetsPerGroup,
                            PassengerGroupSize, DestinationsPerGroup, CabinsPerGroup, CryoSleepsPerGroup, VIPsPerGroup,
                            LastNamesPerGroup, PassengerGroup, PassengerId),
                  .fns = as.factor)) %>%
    mutate(across(.cols = c(CabinNumber, Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck),
                  .fns = as.integer))
  return(x2)
}

my_na_replace <- function(d) {
  d2 <- d %>%
    # Replace HomePlanet for passengers in groups where the homeplanet is known from the other passengers
    group_by(PassengerGroup) %>% 
    fill(HomePlanet, .direction = "downup") %>% 
    
    # Replace Cabin by group cabin for groups with group count > 1. Update the Deck, CabinNumber and Side variables.
    mutate(Cabin2 = Cabin) %>%
    fill(data = ., Cabin2, .direction = "downup") %>%
    ungroup() %>%
    mutate(Cabin = if_else(is.na(Cabin) & PassengerGroupSize != 1, Cabin2, Cabin),
           Deck = str_split_i(Cabin, "/", 1),
           CabinNumber = str_split_i(Cabin, "/", 2),
           Side = str_split_i(Cabin, "/", 3)) %>%
    select(-Cabin2) %>%
    
    # Replace HomePlanet if the passenger is housed on a dedicated Deck
    mutate(HomePlanet = if_else(is.na(HomePlanet) & Deck == "G", "Earth", HomePlanet),
           HomePlanet = if_else(is.na(HomePlanet) & Deck %in% c("A", "B", "C"), "Europa", HomePlanet)) %>%
    
    # Replace all of VIPs from Earth to FALSE
    mutate(VIP = if_else(is.na(VIP) & HomePlanet == "Earth", "False", VIP)) %>%
    
    # Replace amenities with zero if CryoSleep is TRUE or if Age <= 12
    # Replace CryoSleep with FALSE if the passenger has spent credits
    mutate(across(.cols = c(RoomService, FoodCourt, ShoppingMall, Spa, VRDeck), 
                  .fns = ~ if_else(condition = CryoSleep == "True" | Age <= 12, true = 0, false = .x, missing = .x)),
           CryoSleep = if_else(TotalSpent > 0 & is.na(CryoSleep), "False", CryoSleep))
  return(d2)
}

useful_features2 <- function(x) {
  x2 <- x %>%
    mutate(TotalSpent = RoomService + FoodCourt + ShoppingMall + Spa + VRDeck) %>%
    group_by(PassengerGroup) %>%
    add_count(PassengerGroup, name = "PassengerGroupSize") %>%
    mutate(HomePlanetsPerGroup = n_distinct(HomePlanet, na.rm = TRUE),
           DestinationsPerGroup = n_distinct(Destination, na.rm = TRUE),
           CabinsPerGroup = n_distinct(Cabin, na.rm = TRUE),
           TotalSpentPerGroup = sum(TotalSpent, na.rm = TRUE),
           CryoSleepsPerGroup = n_distinct(CryoSleep, na.rm = TRUE),
           VIPsPerGroup = n_distinct(VIP, na.rm = TRUE),
           LastNamesPerGroup = n_distinct(LastName, na.rm = TRUE)) %>%
    ungroup() %>%
    mutate(across(.cols = c(HomePlanet, CryoSleep, Destination, VIP, Deck, Side, HomePlanetsPerGroup,
                            PassengerGroupSize, DestinationsPerGroup, CabinsPerGroup, CryoSleepsPerGroup, VIPsPerGroup,
                            LastNamesPerGroup, PassengerGroup, PassengerId),
                  .fns = as.factor)) %>%
    mutate(across(.cols = c(CabinNumber, Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck),
                  .fns = as.integer))
  return(x2)
}

fix_knn <- function(df) {
  amenities_summary <- df %>%
    filter(Age > 12 & TotalSpent > 0) %>%
    summarise(mean_age = round(mean(Age), 0), .by = c(CryoSleep, Deck))
  
  wrong_age <- df %>%
    filter(Age <= 12 & TotalSpent > 0) %>%
    left_join(., amenities_summary, by = c("CryoSleep", "Deck")) %>%
    select(PassengerId, mean_age)
  
  wrong_planet <- df %>%
    filter(HomePlanetsPerGroup != 1) %>%
    select(PassengerId, PassengerGroup, HomePlanet) %>%
    group_by(PassengerGroup) %>%
    mutate(HomePlanet_correct = first(HomePlanet)) %>%
    ungroup() %>%
    select(PassengerId, HomePlanet_correct)
  
  wrong_deck <- df %>%
    filter(Deck %in% c("A", "B", "C", "G")) %>%
    mutate(Deck_correct = case_when(Deck %in% c("A", "B", "C") & HomePlanet == "Earth" ~ "G",
                                    Deck %in% c("A", "B", "C") & HomePlanet == "Mars" ~ "F",
                                    Deck == "G" & HomePlanet == "Mars" ~ "F",
                                    Deck == "G" & HomePlanet == "Europa" ~ "C", .default = Deck)) %>%
    select(PassengerId, Deck_correct)
  
  res <- df %>%
  left_join(., wrong_age, by = "PassengerId") %>%
  left_join(., wrong_planet, by = "PassengerId") %>%
  left_join(., wrong_deck, by = "PassengerId") %>%
  mutate(Age = if_else(Age <= 12 & TotalSpent > 0, mean_age, Age),
         RoomService = if_else(CryoSleep == "True", 0, RoomService),
         FoodCourt = if_else(CryoSleep == "True", 0, FoodCourt),
         ShoppingMall = if_else(CryoSleep == "True", 0, ShoppingMall),
         Spa = if_else(CryoSleep == "True", 0, Spa),
         VRDeck = if_else(CryoSleep == "True", 0, VRDeck),
         HomePlanet = if_else(HomePlanetsPerGroup != 1, HomePlanet_correct, HomePlanet),
         Deck_correct = coalesce(Deck_correct, Deck),
         Deck = Deck_correct) %>%
  select(-mean_age, -HomePlanet_correct, -Deck_correct)
  
  return(res)
}

add_grp_features <- function(df) {
  res <- df %>%
  mutate(Solo = if_else(PassengerGroupSize == 1, 1, 0),
         LargeGroup = if_else(as.integer(PassengerGroupSize) > 7, 1, 0),
         TravelTogether = if_else(DestinationsPerGroup == 1, 1, 0))
}

encode_cat_to_numeric <- function(x) {
  x <- factor(x, ordered = FALSE)
  x <- unclass(x)
  return(x)
}

add_name_features <- function(df) {
  res <- df %>%
  mutate(LastNameAsNumber = encode_cat_to_numeric(LastName)) %>%
  add_count(x = ., LastNameAsNumber, name = "LastNameCount") %>%
  mutate(across(.cols = c(PassengerGroup, LastNameAsNumber), .fns = as.integer))
}

rev_normalization <- function(v, rec) { # Custom function that will "unnormalise" numeric values inside mutate(across())
  tidy_rec <- tidy(rec, number = 1)
  v2 <- v * filter(tidy_rec, terms == cur_column() & statistic == "sd")$value + 
    filter(tidy_rec, terms == cur_column() & statistic == "mean")$value
  v3 <- round(v2, 0)
  return(v3)
}
```


```{r summarise-preprocess, warning=FALSE, message=FALSE}
train <- read_csv("train.csv", na = "")

# Create group variables, seperate CabinNumber, Deck and Side from Cabin
train2 <- useful_features(train)

# Replace structurally missing NA
train3 <- my_na_replace(train2)
train3 <- useful_features(train3)

# KNN impute remaining missing values
train3_for_knn <- train3 %>%
  mutate(across(.cols = where(is.factor), .fns = as.character))

vars_to_impute <- c("HomePlanet", "CryoSleep", "Destination", "Age", "VIP", "RoomService", "FoodCourt", "ShoppingMall",
                    "Spa", "VRDeck", "Deck", "Side", "CabinNumber", "LastName")
vars_for_imputing <- c("HomePlanet", "CryoSleep", "Destination", "Age", "VIP", "RoomService", "FoodCourt",
                              "ShoppingMall", "Spa", "VRDeck", "PassengerGroup", "Deck", "Side", "CabinNumber",
                              "PassengerGroupSize", "DestinationsPerGroup", "CabinsPerGroup",
                              "CryoSleepsPerGroup", "VIPsPerGroup", "LastNamesPerGroup")

train3_noNA <- train3_for_knn[complete.cases(train3_for_knn),]
  
knn_impute_rec <- recipe(Transported ~ ., data = train3_noNA) %>%
  step_normalize(Age, CabinNumber, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck) %>%
  step_impute_knn(recipe = ., all_of(vars_to_impute), impute_with = imp_vars(all_of(vars_for_imputing)), neighbors = 5) 

set.seed(8584)
knn_impute_prep <- knn_impute_rec %>% prep(strings_as_factors = FALSE)

set.seed(8584)
knn_impute_bake <- bake(knn_impute_prep, new_data = train3_for_knn)

knn_impute_res <- knn_impute_bake %>%
  mutate(across(.cols = c(Age, CabinNumber, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck),
                .fns = ~ rev_normalization(.x, knn_impute_prep)))

# Fixed KNN imputation where structural missing rules were broken
fixed_knn <- fix_knn(knn_impute_res)
train4 <- useful_features2(fixed_knn)

# Add new features we've discovered from our visual exploration
train5 <- add_grp_features(train4)
train6 <- add_name_features(train5)

# Get our variables in order for modelling
final_df <- train6 %>%
  select(-c(PassengerCount, HomePlanetsPerGroup, Cabin, Name, LastName)) %>%
  mutate(across(.cols = c(PassengerGroupSize, ends_with("PerGroup")), .fns = as.integer),
         Transported = as.factor(Transported))

my_vars <- data.frame(Variables = names(final_df)) %>%
  mutate(Roles = if_else(Variables %in% c("PassengerId"), "id", "predictor"),
         Roles = if_else(Variables == "Transported", "outcome", Roles))

load("Extra/Best variables.RData")

set.seed(8584)
final_split <- initial_split(final_df, prop = 0.8)
final_train <- training(final_split)
final_test <- testing(final_split)
final_folds <- vfold_cv(final_train, v = 10, repeats = 5)

final_rec <- recipe(x = final_train, vars = my_vars$Variables, roles = my_vars$Roles) %>%
  step_scale(Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, PassengerGroup, CabinNumber, TotalSpent, TotalSpentPerGroup,
             LastNameAsNumber) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(int_formula) %>%
  step_zv(all_predictors()) %>%
  step_select(all_outcomes(), contains("_x_"), all_of(matches(str_c(rfe_best_vars, collapse = "|"))), skip = TRUE)
```

## GLM
```{r glm, warning=FALSE}
glm_final_bake <- final_rec %>%
  prep() %>%
  bake(new_data = NULL)

my_acc <- metric_set(accuracy)

glm_final_mod <- logistic_reg() %>%
  set_engine("glm", family = "binomial")

glm_final_wf <- workflow() %>%
  add_recipe(final_rec) %>%
  add_model(glm_final_mod)

set.seed(8584)
glm_final_fit <- fit(glm_final_wf, final_train)
glm_fitted <- extract_fit_engine(glm_final_fit)

glm_final_pred <- predict(glm_final_fit, final_test)
confusionMatrix(data = glm_final_pred$.pred_class, reference = final_test$Transported)
```

We can see from the results of residuals in Figure \@ref(fig:glm-without-int) that the GLM model (red lines) does pretty well against the categorical variables but does poorly against the numerical ones. This is because these variables don't have a linear relationship to the response, as we've seen in our exploratory chapter of numerical variables. The accuracy is very low, 0.64.

## GLMNET Lasso/Ridge
```{r glmnet}
load("Extra/Penalized regression with interactions.RData")
best_glm_params <- select_best(pen_int_tune)

glmnet_final_mod <- logistic_reg(penalty = pluck(best_glm_params$penalty), mixture = pluck(best_glm_params$mixture)) %>%
  set_engine("glmnet", family = "binomial")

glmnet_final_wf <- glm_final_wf %>%
  update_model(glmnet_final_mod)

set.seed(8584)
glmnet_final_fit <- fit(glmnet_final_wf, final_train)
glmnet_fitted <- extract_fit_engine(glmnet_final_fit)

glmnet_final_pred <- predict(glmnet_final_fit, final_test, type = "class")
confusionMatrix(data = glmnet_final_pred$.pred_class, reference = final_test$Transported)
```

## RandomForest
```{r randomForest-tune}
my_acc <- metric_set(accuracy)
rf_ctrl <- control_grid(verbose = TRUE, save_pred = TRUE, save_workflow = TRUE)
rf_grid <- expand.grid(trees = c(500, 2000, 10000), min_n = 5)

rf_final_mod_tune <- rand_forest(trees = tune(), min_n = tune()) %>%
  set_mode("classification") %>%
  set_engine("randomForest")

# my_cluster <- makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# clusterExport(cl = my_cluster, "final_interactions")
# 
# system.time({
#   set.seed(8584)
#   rf_final_tune <- rf_final_mod_tune %>%
#     tune_grid(final_rec, resamples = final_folds, metrics = my_acc, control = rf_ctrl, grid = rf_grid)
# })
# 
# save(rf_final_tune, file = "Final randomForest tune.RData")
# 
# stopCluster(my_cluster)
# unregister()

load("Extra/Final randomForest tune.RData")

show_best(rf_final_tune, metric = "accuracy", n = 20) %>%
  ggplot(aes(x = trees, y = mean)) +
  geom_line() +
  geom_point() +
  scale_x_log10() +
  labs(title = "Tune results for RandomForest", x = "Number of trees", y = "Resample accuracy")
```
The results from the model tuning indicate that the improvement is very limited with more trees than somewhere around 2000 so I will assume this as the optimal. With tree based models, too much trees can overfit the data so it's a good idea not to

```{r randomForest-final}
rf_best_tune <- select_best(rf_final_tune)

rf_final_mod <- rand_forest(trees = 2000, min_n = 5) %>%
  set_mode("classification") %>%
  set_engine("randomForest")

rf_final_wf <- glm_final_wf %>%
  update_model(rf_final_mod)

set.seed(8584)
rf_final_fit <- fit(rf_final_wf, final_train)
rf_fitted <- extract_fit_engine(rf_final_fit)

rf_final_pred <- predict(rf_final_fit, final_test)

confusionMatrix(data = rf_final_pred$.pred_class, reference = final_test$Transported)
```

# KNN

Rectangular: This is also known as the uniform kernel. It gives equal weight to all neighbors within the window, effectively creating a binary situation where points are either in the neighborhood (and given equal weight) or not.

Triangular: This kernel assigns weights linearly decreasing from the center. It gives the maximum weight to the nearest neighbor and the minimum weight to the farthest neighbor within the window.

Epanechnikov: This kernel is parabolic with a maximum at the center, decreasing to zero at the windowâ€™s edge. It is often used because it minimizes the mean integrated square error.

Biweight: This is a smooth, bell-shaped kernel that gives more weight to the nearer neighbors.

Triweight: This is similar to the biweight but gives even more weight to the nearer neighbors.

Cos: This kernel uses the cosine of the distance to weight the neighbors.

Inv: This kernel gives weights as the inverse of the distance.

Gaussian: This kernel uses the Gaussian function to assign weights. It has a bell shape and does not compactly support, meaning it gives some weight to all points in the dataset, but the weight decreases rapidly as the distance increases.

Rank: This kernel uses the ranks of the distances rather than the distances themselves.

Optimal: This kernel attempts to choose the best weighting function based on the data.

```{r knn-final}
knn_final_mod <- nearest_neighbor(neighbors = 5, weight_func = "optimal") %>%
  set_mode("classification") %>%
  set_engine("kknn")

knn_final_wf <- glm_final_wf %>%
  update_model(knn_final_mod)

set.seed(8584)
knn_final_fit <- fit(knn_final_wf, final_train)
knn_fitted <- extract_fit_engine(knn_final_fit)

knn_final_pred <- predict(knn_final_fit, final_test)

confusionMatrix(data = knn_final_pred$.pred_class, reference = final_test$Transported)
```

# SVM
svm_linear: This method defines a SVM model that uses a linear class boundary12. For classification, the model tries to maximize the width of the margin between classes using a linear class boundary. For regression, the model optimizes a robust loss function that is only affected by very large model residuals and uses a linear fit12.

svm_poly: This method defines a SVM model that uses a polynomial class boundary345. For classification, the model tries to maximize the width of the margin between classes using a polynomial class boundary. For regression, the model optimizes a robust loss function that is only affected by very large model residuals and uses polynomial functions of the predictors345.

svm_rbf: This method defines a SVM model that uses a nonlinear class boundary67. For classification, the model tries to maximize the width of the margin between classes using a nonlinear class boundary. For regression, the model optimizes a robust loss function that is only affected by very large model residuals and uses nonlinear functions of the predictors67.

```{r svm-final}
svm_final_mod <- svm_rbf(cost = 1, margin = 0.1) %>%
  set_mode("classification") %>%
  set_engine("kernlab")

svm_final_wf <- glm_final_wf %>%
  update_model(svm_final_mod)

set.seed(8584)
svm_final_fit <- fit(svm_final_wf, final_train)
svm_fitted <- extract_fit_engine(svm_final_fit)

svm_final_pred <- predict(svm_final_fit, final_test)

confusionMatrix(data = svm_final_pred$.pred_class, reference = final_test$Transported)
```

# XGBoost
```{r xgboost-final}
xgb_final_mod <- boost_tree(trees = 15) %>%
  set_mode("classification") %>%
  set_engine("xgboost")

xgb_final_wf <- glm_final_wf %>%
  update_model(xgb_final_mod)

set.seed(8584)
xgb_final_fit <- fit(xgb_final_wf, final_train)
xgb_fitted <- extract_fit_engine(xgb_final_fit)

xgb_final_pred <- predict(xgb_final_fit, final_test)

confusionMatrix(data = xgb_final_pred$.pred_class, reference = final_test$Transported)
```

# C5.0
```{r c5-0-final}
c50_final_mod <- boost_tree(trees = 15) %>%
  set_mode("classification") %>%
  set_engine("C5.0")

c50_final_wf <- glm_final_wf %>%
  update_model(c50_final_mod)

set.seed(8584)
c50_final_fit <- fit(c50_final_wf, final_train)
c50_fitted <- extract_fit_engine(c50_final_fit)

c50_final_pred <- predict(c50_final_fit, final_test)

confusionMatrix(data = c50_final_pred$.pred_class, reference = final_test$Transported)
```

# Naive Bayes
```{r naive-bayes-final}
nb_final_mod <- naive_Bayes() %>%
  set_engine("klaR")

nb_final_wf <- glm_final_wf %>%
  update_model(nb_final_mod)

set.seed(8584)
nb_final_fit <- fit(nb_final_wf, final_train)
nb_fitted <- extract_fit_engine(nb_final_fit)

nb_final_pred <- predict(nb_final_fit, final_test)

confusionMatrix(data = nb_final_pred$.pred_class, reference = final_test$Transported)
```

```{r rf-pred-kaggl, warning=FALSE, include=FALSE}
# test <- read_csv("test.csv", na = "", col_types = "cccccncnnnnnc")
# 
# # Create group variables, seperate CabinNumber, Deck and Side from Cabin
# test2 <- useful_features(test)
# 
# # Replace structurally missing NA
# test3 <- my_na_replace(test2)
# test3 <- useful_features(test3)
# 
# # KNN impute remaining missing values
# test3_for_knn <- test3 %>%
#   mutate(across(.cols = where(is.factor), .fns = as.character))
# 
# set.seed(8584)
# knn_impute_test <- bake(knn_impute_prep, new_data = test3_for_knn)
# 
# knn_impute_test_res <- knn_impute_test %>%
#   mutate(across(.cols = c(Age, CabinNumber, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck),
#                 .fns = ~ rev_normalization(.x, knn_impute_prep)))
# 
# # Fixed KNN imputation where structural missing rules were broken
# fixed_test_knn <- fix_knn(knn_impute_test_res)
# test4 <- useful_features2(fixed_test_knn)
# 
# # Add new features we've discovered from our visual exploration
# test5 <- add_grp_features(test4)
# test6 <- add_name_features(test5)
# 
# final_df_test <- test6 %>%
#   select(-c(PassengerCount, HomePlanetsPerGroup, Cabin, Name, LastName)) %>%
#   mutate(across(.cols = c(PassengerGroupSize, ends_with("PerGroup")), .fns = as.integer))
# 
# rf_kaggle_pred <- predict(rf_final_fit, final_df_test)
# rf_submission <- bind_cols(PassengerId = final_df_test$PassengerId, Transported = rf_kaggle_pred$.pred_class)
# 
# write_csv(rf_submission, file = "RF submission 2024-01-30 21_50.csv")
```

Our score on Kaggle was 0.7898 which is very close to our tuned values. This indicates that our process gives us accurate estimations and now we must consider how we can improve it or select a better model.

```{r remove-07, include=FALSE}
rm(train2, train3, train3_for_knn, vars_to_impute, vars_for_imputing, train3_noNA, knn_impute_rec, knn_impute_prep, knn_impute_bake,
   knn_impute_res, fixed_knn, train4, train5, train6, my_vars, final_interactions, final_split, final_train, final_folds)
```