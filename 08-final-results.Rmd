# Final results {#chapter-9}

```{r summarise-testdata, include=FALSE}
test <- read_csv("test.csv", na = "", col_types = "ccfccnfnnnnncf")

# Create group variables, seperate CabinNumber, Deck and Side from Cabin
test2 <- useful_features(test)

# Replace structurally missing NA
test3 <- my_na_replace(test2)
test3 <- useful_features(test3)

# KNN impute remaining missing values
test3_for_knn <- test3 %>%
  mutate(across(.cols = where(is.factor), .fns = as.character))

vars_to_impute <- c("HomePlanet", "CryoSleep", "Destination", "Age", "VIP", "RoomService", "FoodCourt", "ShoppingMall",
                    "Spa", "VRDeck", "Deck", "Side", "CabinNumber", "LastName")
vars_for_imputing <- c("HomePlanet", "CryoSleep", "Destination", "Age", "VIP", "RoomService", "FoodCourt",
                              "ShoppingMall", "Spa", "VRDeck", "PassengerGroup", "Deck", "Side", "CabinNumber",
                              "PassengerGroupSize", "DestinationsPerGroup", "CabinsPerGroup",
                              "CryoSleepsPerGroup", "VIPsPerGroup", "LastNamesPerGroup")

test3_noNA <- test3_for_knn[complete.cases(test3_for_knn),]
  
knn_impute_rec <- recipe(. ~ ., data = test3_noNA) %>%
  step_normalize(Age, CabinNumber, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck) %>%
  step_impute_knn(recipe = ., all_of(vars_to_impute), impute_with = imp_vars(all_of(vars_for_imputing)), neighbors = 5)

set.seed(8584)
knn_impute_prep <- knn_impute_rec %>% prep(strings_as_factors = FALSE)

set.seed(8584)
knn_impute_bake <- bake(knn_impute_prep, new_data = test3_for_knn)

knn_impute_res <- knn_impute_bake %>%
  mutate(across(.cols = c(Age, CabinNumber, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck),
                .fns = ~ rev_normalization(.x, knn_impute_prep)))

# Fixed KNN imputation where structural missing rules were broken
fixed_knn <- fix_knn(knn_impute_res)
test4 <- useful_features2(fixed_knn)

# Add new features we've discovered from our visual exploration
test5 <- add_grp_features(test4)
test6 <- add_name_features(test5)
test7 <- bin_for_zero(test6)

# Get our variables in order for modelling
final_test_df <- test7 %>%
  select(-c(PassengerCount, HomePlanetsPerGroup, Cabin, Name, LastName)) %>%
  mutate(across(.cols = c(PassengerGroupSize, ends_with("PerGroup")), .fns = as.integer))

my_vars <- data.frame(Variables = names(final_test_df)) %>%
  mutate(Roles = if_else(Variables %in% c("PassengerId"), "id", "predictor"))

load("Extra/Best interactions.RData")

int_formula <- pen_int_vars %>%
  select(ForFormula, RevFormula) %>%
  unlist() %>%
  unname() %>%
  str_flatten(., collapse = "+") %>%
  str_c("~", .) %>%
  as.formula(.)

load("Extra/Best variables.RData")
best_vars2 <- c(best_vars, "ZeroRoomService", "ZeroFoodCourt", "ZeroShoppingMall", "ZeroSpa", "ZeroVRDeck", "PassengerId")

vars_to_normalize <- c("Age", "RoomService", "FoodCourt", "ShoppingMall", "Spa", "VRDeck", "PassengerGroup", "CabinNumber",
                       "TotalSpent", "TotalSpentPerGroup", "LastNameAsNumber")

selector_rec <- recipe(x = final_test_df, vars = my_vars$Variables, roles = my_vars$Roles) %>%
  step_normalize(all_of(!!vars_to_normalize)) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(int_formula) %>%
  step_zv(all_predictors()) %>%
  step_mutate(Destination_PSO.J318.5.22_x_Deck_D = 0) %>%
  step_select(all_outcomes(), contains("_x_"), all_of(matches(str_c(best_vars2, collapse = "|"))), skip = TRUE)

final_test_df2 <- selector_rec %>%
  prep() %>%
  bake(new_data = NULL)
```

Let's make our final predictions using so far unseen data. After this, we cannot adjust the models anymore without risking data leakage. 

```{r submissions, cache=TRUE, warning=FALSE, eval=FALSE}
glm_ultimate_pred <- predict(glm_final_fit, final_test_df2, type = "prob") %>%
  mutate(Prediction = as.factor(if_else(.pred_True < 0.46, "False", "True")))
glm_submission <- bind_cols(PassengerId = final_test_df2$PassengerId, Transported = glm_ultimate_pred$Prediction)
write_csv(glm_submission, file = "Submissions/GLM submission 2024-02-13 19_40.csv")

glmnet_ultimate_pred <- predict(glmnet_final_fit, final_test_df2, type = "class")
glmnet_submission <- bind_cols(PassengerId = final_test_df2$PassengerId, Transported = glmnet_ultimate_pred$.pred_class)
write_csv(glmnet_submission, file = "Submissions/GLMNET submission 2024-02-13 19_40.csv")

gam_ultimate_pred <- predict(gam_final_fit, final_test_df2, type = "class")
gam_submission <- bind_cols(PassengerId = final_test_df2$PassengerId, Transported = gam_ultimate_pred$.pred_class)
write_csv(gam_submission, file = "Submissions/GAM submission 2024-02-13 19_40.csv")

svm_linear_ultimate_pred <- predict(svm_linear_final_fit, final_test_df2, type = "class")
svm_linear_submission <- bind_cols(PassengerId = final_test_df2$PassengerId, Transported = svm_linear_ultimate_pred$.pred_class)
write_csv(svm_linear_submission, file = "Submissions/SVM linear submission 2024-02-13 19_40.csv")

svm_poly_ultimate_pred <- predict(svm_poly_final_fit, final_test_df2, type = "class")
svm_poly_submission <- bind_cols(PassengerId = final_test_df2$PassengerId, Transported = svm_poly_ultimate_pred$.pred_class)
write_csv(svm_poly_submission, file = "Submissions/SVM poly submission 2024-02-13 19_40.csv")

svm_rbf_ultimate_pred <- predict(svm_rbf_final_fit, final_test_df2, type = "class")
svm_rbf_submission <- bind_cols(PassengerId = final_test_df2$PassengerId, Transported = svm_rbf_ultimate_pred$.pred_class)
write_csv(svm_rbf_submission, file = "Submissions/SVM rbf submission 2024-02-13 19_40.csv")

nb_ultimate_pred <- predict(nb_final_fit, final_test_df2, type = "class")
nb_submission <- bind_cols(PassengerId = final_test_df2$PassengerId, Transported = nb_ultimate_pred$.pred_class)
write_csv(nb_submission, file = "Submissions/NB submission 2024-02-13 19_40.csv")

knn_ultimate_pred <- predict(knn_final_fit, final_test_df2, type = "class")
knn_submission <- bind_cols(PassengerId = final_test_df2$PassengerId, Transported = knn_ultimate_pred$.pred_class)
write_csv(knn_submission, file = "Submissions/KNN submission 2024-02-13 19_40.csv")

c5.0_ultimate_pred <- predict(c5.0_final_fit, final_test_df2, type = "class")
c5.0_submission <- bind_cols(PassengerId = final_test_df2$PassengerId, Transported = c5.0_ultimate_pred$.pred_class)
write_csv(c5.0_submission, file = "Submissions/C5_0 submission 2024-02-13 19_40.csv")

rf_ultimate_pred <- predict(rf_final_fit, final_test_df2, type = "class")
rf_submission <- bind_cols(PassengerId = final_test_df2$PassengerId, Transported = rf_ultimate_pred$.pred_class)
write_csv(rf_submission, file = "Submissions/RF submission 2024-02-13 19_40.csv")

xgb_ultimate_pred <- predict(xgb_final_fit, final_test_df2, type = "class")
xgb_submission <- bind_cols(PassengerId = final_test_df2$PassengerId, Transported = xgb_ultimate_pred$.pred_class)
write_csv(xgb_submission, file = "Submissions/XGB submission 2024-02-13 19_40.csv")
```

Now we summarise both resampling and test results in a table.

```{r summary-acc, cache=TRUE}
modelresults <- readxl::read_xlsx("Extra/Modelresults.xlsx")
modelresults2 <- modelresults %>%
  mutate(`Resample Accuracy` = c(glm_final_acc, glmnet_final_acc, gam_final_acc, svm_linear_final_acc, svm_poly_final_acc,
                                 svm_rbf_final_acc, nb_final_acc, knn_final_acc, c5.0_final_acc, rf_final_acc, xgb_final_acc),
         `Test Data Accuracy` = c(0.79565, 0.79775, 0.79775, 0.79424, NA, 0.79658, 0.73182, 0.77671, 0.76852,
                                  0.78045, 0.78068)) %>%
  mutate(across(.cols = c(2:3), .fns = ~ round(.x, 3)))

knitr::kable(modelresults2, align = "lcc") %>%
  kableExtra::kable_styling(bootstrap_options = "striped")
```

We see that our resampling methods give reasonably accurate estimates of accuracy. Apart from the Naive Bayes model that has low performance, most other models do well with this data set. The explanation for the Naive Bayes underperformance is likely that it doesn't do so well on data that has been converted to dummy variables - it probably requires a different recipe to reach similar performance as the other models.

The simplest models seem to also have the best accuracy with the best two being penalized regression as well as general additive models. I was hoping to score above 80% but it seems as if more can be done with the data.

```{r remove-08, include=FALSE}
rm(list = setdiff(ls(), "modelresults2"))
```