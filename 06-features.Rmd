# Feature selection and elimination
Now that we have created several new features and discovered promising interaction effects, we must explore which of these have the potential to improve our models and which don't. Kuhn and Johnson recommend several methods for feature selection and elimination that we'll explore.

## Recursive feature elimination
Recursive feature elimination creates a model that includes all the features and through a systematized resampling process removes sets of variables to see whether it improves performance. The goal of the model is to converge on a set of predictor variables that produce the best results.

We'll use all of our original variables as well as the interaction effects we've identified in the previous chapter. 
```{r recurs-feat-no-corr, warning = FALSE, results='hide', message=FALSE, cache=TRUE}
rfe_df <- train6 %>%
  select(PassengerId, CryoSleep, HomePlanet, Destination, VIP, Deck, Side, Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck,
         CabinNumber, LastNameAsNumber, PassengerGroup, Transported)

set.seed(8584)
rfe_split <- initial_split(rfe_df, prop = 0.8)
rfe_train <- training(rfe_split)

rfe_vars <- data.frame(Variables = names(rfe_df)) %>%
  mutate(Roles = case_when(Variables == "PassengerId" ~ "id",
                           Variables == "Transported" ~ "outcome",
                           .default = "predictor"))

int_formula <- pen_int_vars %>%
  select(ForFormula, RevFormula) %>%
  unlist() %>%
  unname() %>%
  str_flatten(., collapse = "+") %>%
  str_c("~", .) %>%
  as.formula(.)

rfe_rec <- recipe(x = rfe_train, vars = rfe_vars$Variables, roles = rfe_vars$Roles) %>%
  step_normalize(Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, CabinNumber, LastNameAsNumber, PassengerGroup) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(int_formula) %>%
  step_zv(all_predictors())

rfe_bake <- rfe_rec %>% prep() %>% bake(new_data = NULL)

many_stats <- function(data, lev = levels(data$obs), model = NULL) {
    c(twoClassSummary(data = data, lev = levels(data$obs), model),
      prSummary(data = data, lev = levels(data$obs), model),
      mnLogLoss(data = data, lev = levels(data$obs), model),
      defaultSummary(data = data, lev = levels(data$obs), model))
}
rfe_funcs <- caret::caretFuncs
rfe_funcs$summary <- many_stats
rfe_funcs$fit <- function (x, y, first, last, ...) 
  train(x, y, trControl = trainControl(classProbs = TRUE), ...)
rfe_sizes <- c(5, 10, 15, 20)
rfe_ctrl <- rfeControl(method = "repeatedcv", repeats = 5, functions = rfe_funcs, returnResamp = "all", verbose = FALSE)

## This part takes a while to run, even in parallel, so save the results
# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# 
# snow::clusterEvalQ(my_cluster, library(recipes))
# snow::clusterExport(my_cluster, "int_formula")
# 
# system.time({
#   set.seed(8584)
#   rfe_acc <- rfe(rfe_rec, data = rfe_train, method = "glm", family = "binomial", metric = "Accuracy", sizes = rfe_sizes, 
#                  rfeControl = rfe_ctrl)
# })
# 
# save(rfe_acc, file = "Recursive feature elimination with interactions.RData")
# 
# stopCluster(my_cluster)
# unregister()

load("Recursive feature elimination with interactions.RData")
```

```{r recurs-feat-no-corr2, warning = FALSE, message=FALSE, fig.cap="Estimated performance with recursive feature elimination based on the number of variables that are included."}
rfe_acc$results %>%
  ggplot(data = ., aes(x = Variables, y = Accuracy)) +
  geom_point() +
  geom_line()
```
There is some strange behaviour that extends the number of variables beyond our specified sizes so we'll just ignore those results. What the results from Figure \@ref(fig:recurs-feat-no-corr2) indicate is that more variables seems to improve the model. However, some of the variables, including dummy variables and interactions, could be highly correlated which might skew the resulting feature set. 

```{r recurs-feat-no-corr3, fig.cap="Correlation between the first set of optimal variables from the RFE model."}
rfe_rec %>%
  prep() %>%
  bake(new_data = NULL) %>%
  select(rfe_acc[["optVariables"]]) %>%
  DataExplorer::plot_correlation(., type = "continuous", geom_text_args = list(size = 10), 
                   theme_config = list(text = element_text(size = 8), axis.text.x = element_text(angle = 90)))
```

We see that there are clusters of high correlation between the variables so let's add a step to remove them and see how it affects performance.

```{r recurs-feat-corr, warning = FALSE, results='hide', message=FALSE, cache=TRUE}
rfe_sizes <- seq(2, 50, 2)

rfe_rec_corr <- rfe_rec %>%
  step_corr(all_numeric_predictors(), threshold = 0.5)

# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# 
# snow::clusterEvalQ(my_cluster, library(recipes))
# snow::clusterExport(my_cluster, "int_formula")
# 
# system.time({
#  set.seed(8584)
#  rfe_acc_corr <- rfe(rfe_rec_corr, data = rfe_train, method = "glm", family = "binomial", metric = "Accuracy", sizes = rfe_sizes,
#                      rfeControl = rfe_ctrl)
# })
# 
# save(rfe_acc_corr, file = "Recursive feature elimination with interactions removed correlated.RData")
# 
# stopCluster(my_cluster)
# unregister()

load("Recursive feature elimination with interactions removed correlated.RData")
```

```{r recurs-feat-corr2, warning = FALSE, message=FALSE, fig.cap="Estimated performance with recursive feature elimination based on interactions identified through penalized regression. Correlated variables have been removed."}
rfe_acc_corr$results %>%
  ggplot(data = ., aes(x = Variables, y = Accuracy)) +
  geom_point() +
  geom_line()
```

The results from RFE process with correlations removed also indicate that the entire set of variables is best to use and provides similar accuracy.

```{r recurs-feat-corr3, warning = FALSE, message=FALSE, fig.cap="Estimated performance with recursive feature elimination based on interactions identified from penalized regression. Correlated variables have been removed."}
rfe_acc_corr_res <- rfe_acc_corr$resample %>%
  rename(Variables, Iter = Variables) %>%
  group_by(Iter) %>%
  summarise(Accuracy = sum(Accuracy) / length(unique(rfe_acc_corr$resample$Resample))) %>%
  ungroup() %>%
  mutate(Resample = "Averaged") %>%
  bind_rows(rfe_acc_corr$resample %>% rename(Variables, Iter = Variables), .) %>%
  mutate(Resample = str_split_i(Resample, "\\.", 1),
         colour_grp = if_else(Resample == "Averaged", "yes", "no"))

rfe_acc_corr_avg <- rfe_acc_corr_res %>% filter(Resample == "Averaged") %>% select(Iter, Accuracy)
  
ggplot(rfe_acc_corr_res, aes(x = Iter, y = Accuracy, colour = colour_grp)) +
  geom_point() +
  facet_wrap(~Resample, scales = "free") +
  theme(legend.position = "none")
```

We can repeat the process above with interactions we identified from our tree model to see if they are able to improve performance.

```{r rfe-tree-int, warning = FALSE, results='hide', message=FALSE, cache=TRUE}
int_formula <- tree_int_vars %>%
  select(ForFormula, RevFormula) %>%
  unlist() %>%
  unname() %>%
  str_flatten(., collapse = "+") %>%
  str_c("~", .) %>%
  as.formula(.)

rfe_rec_tree <- recipe(x = rfe_train, vars = rfe_vars$Variables, roles = rfe_vars$Roles) %>%
  step_normalize(Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, CabinNumber, LastNameAsNumber, PassengerGroup) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(int_formula) %>%
  step_zv(all_predictors()) %>%
  step_corr(all_numeric_predictors(), threshold = 0.5)

# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# 
# snow::clusterEvalQ(my_cluster, library(recipes))
# snow::clusterExport(my_cluster, "int_formula")
# 
# system.time({
#   set.seed(8584)
#   rfe_acc_corr_tree <- rfe(rfe_rec_tree, data = rfe_train, method = "glm", family = "binomial", metric = "Accuracy", 
#                            sizes = rfe_sizes, rfeControl = rfe_ctrl)
# })
# 
# save(rfe_acc_corr_tree, file = "RFE with tree interactions corr.RData")
# 
# stopCluster(my_cluster)
# unregister()

load("RFE with tree interactions corr.RData")
```

```{r rfe-tree-int2, warning = FALSE, message=FALSE, fig.cap="Estimated performance with recursive feature elimination based on interactions identified from a tree model. Correlated variables have been removed."}
rfe_acc_corr_tree_res <- rfe_acc_corr_tree$resample %>%
  rename(Variables, Iter = Variables) %>%
  group_by(Iter) %>%
  summarise(Accuracy = sum(Accuracy) / length(unique(rfe_acc_corr_tree$resample$Resample))) %>%
  ungroup() %>%
  mutate(Resample = "Averaged") %>%
  bind_rows(rfe_acc_corr_tree$resample %>% rename(Variables, Iter = Variables), .) %>%
  mutate(Resample = str_split_i(Resample, "\\.", 1),
         colour_grp = if_else(Resample == "Averaged", "yes", "no"))

rfe_acc_corr_tree_avg <- rfe_acc_corr_tree_res %>% filter(Resample == "Averaged") %>% select(Iter, Accuracy)
  
ggplot(rfe_acc_corr_tree_res, aes(x = Iter, y = Accuracy, colour = colour_grp)) +
  geom_point() +
  facet_wrap(~Resample, scales = "free") +
  theme(legend.position = "none")
```

The accuracy estimates are very similar from both sets of interactions. To reduce computation time, we'll use the tree model based interactions for our other methods.

## Simulated annealing
Kuhn and Johnson describe the process of simulated annealing in detail in their book but the main purpose is to iterate over random subsets of predictor variables and calculate performance until an optimal set is found. The process uses randomness to avoid to converge on a subset of variables that has high performance in a particular resample (local optima).

We'll use our tree-based interactions together with the original variables and run them through this process.

```{r simulated-annealing, warning = FALSE, results='hide', message=FALSE, cache=TRUE}
sa_df <- train6 %>%
  select(PassengerId, CryoSleep, HomePlanet, Destination, VIP, Deck, Side, Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck,
         CabinNumber, LastNameAsNumber, PassengerGroup, Transported)

set.seed(8584)
sa_split <- initial_split(sa_df, prop = 0.8)
sa_train <- training(sa_split)

sa_vars <- data.frame(Variables = names(sa_df)) %>%
  mutate(Roles = case_when(Variables == "PassengerId" ~ "id",
                           Variables == "Transported" ~ "outcome",
                           .default = "predictor"))

int_formula <- tree_int_vars %>%
  select(ForFormula, RevFormula) %>%
  unlist() %>%
  unname() %>%
  str_flatten(., collapse = "+") %>%
  str_c("~", .) %>%
  as.formula(.)

sa_rec <- recipe(x = sa_train, vars = sa_vars$Variables, roles = sa_vars$Roles) %>%
  step_normalize(Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck, CabinNumber, LastNameAsNumber, PassengerGroup) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(int_formula) %>%
  step_zv(all_predictors()) %>%
  step_corr(all_numeric_predictors(), threshold = 0.5)

sa_funcs <- caretSA
sa_funcs$fitness_extern <- many_stats
sa_funcs$initial <- function(vars, prob = 0.50, ...) {
  sort(sample.int(vars, size = floor(vars * prob) + 1))
}

# Inner control
sa_ctrl_inner <- trainControl(method = "boot", p = 0.90, number = 1, summaryFunction = many_stats, classProbs = TRUE, 
                        allowParallel = FALSE)

# Outer control for SA
sa_ctrl_outer <- safsControl(method = "cv", metric = c(internal = "Accuracy", external = "Accuracy"), 
                       maximize = c(internal = TRUE, external = TRUE), functions = sa_funcs, improve = 20, returnResamp = "all",
                       verbose = FALSE, allowParallel = TRUE)

# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# clusterExport(cl = my_cluster, "int_formula")
# 
# system.time({
#   set.seed(8485)
#   sim_anneal_50_pct <- safs(sa_rec, data = sa_train, iters = 50, safsControl = sa_ctrl_outer, method = "glm",
#                             trControl = sa_ctrl_inner, metric = "Accuracy")
# })
# 
# save(sim_anneal_50_pct, file = "Simulated annealing with 50% inital.RData")
# 
# stopCluster(my_cluster)
# unregister()

load("Simulated annealing with 50% inital.RData")
```

```{r simulated-annealing-internal, warning = FALSE, results='hide', message=FALSE, fig.cap="Estimated performance of internal resamples from simulated annealing."}
sim_ann_50_int <- sim_anneal_50_pct$internal
sim_ann_50_int2 <- sim_ann_50_int %>%
  group_by(Iter) %>%
  summarise(Accuracy = sum(Accuracy) / length(unique(sim_ann_50_int$Resample))) %>%
  ungroup() %>%
  mutate(Resample = "Averaged") %>%
  bind_rows(sim_ann_50_int, .) %>%
  mutate(colour_grp = if_else(Resample == "Averaged", "yes", "no"))

sim_ann_50_int_avg <- sim_ann_50_int2 %>% filter(Resample == "Averaged") %>% select(Iter, Accuracy)
  
ggplot(sim_ann_50_int2, aes(x = Iter, y = Accuracy, colour = colour_grp)) +
  geom_point() +
  facet_wrap(~Resample) +
  theme(legend.position = "none")
```

The results from the internal simulated annealing process suggest that the process works, based on how the average accuracy increases with iterations, but that a plateau is reached perhaps around 0.73 accuracy. This is lower than the accuracy from RFE but remember that we used a `glm` model for this process instead of random forest.

```{r simulated-annealing-external, warning = FALSE, message=FALSE, fig.cap="Estimated performance of external resamples from simulated annealing."}
sim_ann_50_ext <- sim_anneal_50_pct$external
sim_ann_50_ext_avg <- sim_ann_50_ext %>%
  group_by(Iter) %>%
  summarise(Accuracy = sum(Accuracy) / length(unique(sim_ann_50_int$Resample))) %>%
  ungroup() %>%
  mutate(Resample = "Averaged") %>%
  bind_rows(sim_ann_50_int, .) %>%
  mutate(colour_grp = if_else(Resample == "Averaged", "yes", "no")) %>%
  filter(Resample == "Averaged") %>% 
  select(Iter, Accuracy)

ext_int_corr <- round(cor(sim_ann_50_int_avg$Accuracy, sim_ann_50_ext_avg$Accuracy), 2)

ggplot(mapping = aes(x = Iter, y = Accuracy)) +
  geom_point(data = sim_ann_50_int_avg, aes(colour = "Internal")) +
  geom_point(data = sim_ann_50_ext_avg, aes(colour = "External")) +
  geom_label(data = sim_ann_50_ext_avg, x = 5, y = 0.73, label = str_c("Corr: ", ext_int_corr)) +
  labs(colour = "Estimate") +
  scale_colour_manual(values = c("Internal" = "red", "External" = "green"))
```

The correlation between the average internal and external resample is very high which suggests that the internal resampling folds didn't overfit.

```{r simulated-annealing-best, warning = FALSE, message=FALSE, fig.cap="Estimated performance of best resample from simulated annealing."}
sim_ann_50_final <- sim_anneal_50_pct$sa
sim_ann_50_final2 <- data.frame(Iter = sim_ann_50_final[["internal"]]$Iter, Accuracy = sim_ann_50_final[["internal"]]$Accuracy,
                                Subset_Size = unlist(lapply(sim_ann_50_final[["subsets"]], length))) %>%
  pivot_longer(-Iter)

ggplot(sim_ann_50_final2, aes(x = Iter, y = value)) +
  geom_point() +
  facet_wrap(~name, nrow = 2, ncol = 1, scales = "free_y")
```

The results from the external simulated annealing process seems to indicate that there is very little improvement in accuracy between a subset of one (!) variable and a subset of 50, although there is some improvement towards the latter. The best subset is 41 variables with an accuracy of 0.7714 but a very close second is one with 24 variables with an accuracy of 0.7709. The latter one is similar to the results we saw from the RFE process.

## Genetic algorithm
The final process for feature selection that we'll use is a genetic algorithm. It groups subsets of variables into groups whose performance is evaluated and a selection of them are selected for 'mating' to create the next generation. The process also involves steps to avoid local optima. 

Again, I will use the same set of variables (with tree based interactions) to evaluate the features.

```{r genetic-algorithm, warning = FALSE, results='hide', message=FALSE, cache=TRUE}
ga_df <- train6 %>%
  select(PassengerId, CryoSleep, HomePlanet, Destination, VIP, Deck, Side, Age, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck,
         CabinNumber, LastNameAsNumber, PassengerGroup, Transported)

set.seed(8584)
ga_split <- initial_split(ga_df, prop = 0.8)
ga_train <- training(ga_split)

ga_vars <- data.frame(Variables = names(ga_df)) %>%
  mutate(Roles = case_when(Variables == "PassengerId" ~ "id",
                           Variables == "Transported" ~ "outcome",
                           .default = "predictor"))

int_formula <- tree_int_vars %>%
  select(ForFormula, RevFormula) %>%
  unlist() %>%
  unname() %>%
  str_flatten(., collapse = "+") %>%
  str_c("~", .) %>%
  as.formula(.)

ga_rec <- recipe(x = ga_train, vars = ga_vars$Variables, roles = ga_vars$Roles) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(int_formula) %>%
  step_zv(all_predictors()) %>%
  step_corr(all_numeric_predictors(), threshold = 0.5)

ga_funcs <- caretGA
ga_funcs$fitness_extern <- many_stats
# ga_funcs$initial <- function(vars, popSize, ...)  {
#   x <- matrix(NA, nrow = popSize, ncol = vars)
#   probs <- seq(0.1, 0.90, length = popSize)
#   for (i in 1:popSize) {
#     x[i, ] <-
#       sample(0:1, replace = TRUE, size = vars, prob = c(probs[i], 1 - probs[i]))
#   }
#   var_count <- apply(x, 1, sum)
#   if (any(var_count == 0)) {
#     for (i in which(var_count == 0)) {
#       p <- sample(1:length(vars), size = 2)
#       x[i, p] <- 1
#     }
#   }
#   return(x)
# }

# Inner control
ga_ctrl_inner <- trainControl(method = "boot", p = 0.90, number = 1, summaryFunction = many_stats, classProbs = TRUE,
                        allowParallel = FALSE)

# Outer control for SA
ga_ctrl_outer <- gafsControl(method = "cv", metric = c(internal = "Accuracy", external = "Accuracy"),
                       maximize = c(internal = TRUE, external = TRUE), functions = ga_funcs, returnResamp = "all",
                       verbose = FALSE, allowParallel = TRUE)

# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# clusterExport(cl = my_cluster, "int_formula")
# 
# system.time({
#   set.seed(8584)
#   ga_acc <- gafs(ga_rec, data = ga_train, iters = 50, gafsControl = ga_ctrl_outer, method = "glm", 
#                  trControl = ga_ctrl_inner, metric = "Accuracy")
# })
# 
# save(ga_acc, file = "Genetic algorithm feature selection.RData")
# 
# stopCluster(my_cluster)
# unregister()

load("Genetic algorithm feature selection.RData")
```

```{r genetic-internal, warning = FALSE, message=FALSE, fig.cap="Estimated performance of internal resamples from the genetic algorithm."}
ga_internal <- ga_acc$internal
ga_internal2 <- ga_internal %>%
  group_by(Iter) %>%
  summarise(Accuracy = sum(Accuracy) / length(unique(ga_internal$Resample))) %>%
  ungroup() %>%
  mutate(Resample = "Averaged") %>%
  bind_rows(ga_internal, .) %>%
  mutate(colour_grp = if_else(Resample == "Averaged", "yes", "no"))

ga_internal_avg <- ga_internal2 %>% filter(Resample == "Averaged") %>% select(Iter, Accuracy)
  
ggplot(ga_internal2, aes(x = Iter, y = Accuracy, colour = colour_grp)) +
  geom_point() +
  facet_wrap(~Resample) +
  theme(legend.position = "none")
```

The inner resamples of the genetic algorithm suggest a plateau after around twenty iterations.

```{r genetic-external, warning = FALSE, message=FALSE, fig.cap="Estimated performance of external resamples from the genetic algorithm."}
ga_external <- ga_acc$external
ga_external2 <- ga_external %>%
  group_by(Iter) %>%
  summarise(Accuracy = sum(Accuracy) / length(unique(ga_external$Resample))) %>%
  ungroup() %>%
  mutate(Resample = "Averaged") %>%
  bind_rows(ga_external, .) %>%
  mutate(colour_grp = if_else(Resample == "Averaged", "yes", "no"))

ga_external_avg <- ga_external2 %>% filter(Resample == "Averaged") %>% select(Iter, Accuracy)

ga_ext_int_corr <- round(cor(ga_internal_avg$Accuracy, ga_external_avg$Accuracy), 2)

ggplot(mapping = aes(x = Iter, y = Accuracy)) +
  geom_point(data = ga_internal_avg, aes(colour = "Internal")) +
  geom_point(data = ga_external_avg, aes(colour = "External")) +
  geom_label(data = ga_external_avg, x = 5, y = 0.78, label = str_c("Corr: ", ga_ext_int_corr)) +
  labs(colour = "Estimate") +
  scale_colour_manual(values = c("Internal" = "red", "External" = "green"))
```

For the genetic algorithm, we see that the internal folds got better results which could indicate that the selections of subsets of variables (the populations) overfit to the data while the external validations (number of generations) found a more general population mix that hopefully better describes the unseen data.

## Check performance with random subsets
To get a sense of how well these processes have selected subsets of variables, we can run an iteration that takes random subsets and evaluates performance. This will tell us if the processes are better than random chance. In this case, we don't use interactions.
```{r rf-random, warning = FALSE, results='hide', message=FALSE, cache=TRUE}
my_subset_size <- length(sim_anneal_50_pct$optVariables)
sa_bake <- sa_rec %>% 
  prep() %>% 
  bake(new_data = NULL) %>%
  select(-PassengerId)
full_vars <- sa_bake %>% names(.)
map_seq <- 1:(length(full_vars)/2) # Number of iterations half of total number of variables to ensure sufficient combinations

rand_subset <- map(map_seq, .f = \(x) sample(full_vars, my_subset_size))
rand_data <- map(rand_subset, .f = \(x) sa_bake %>% dplyr::select(Transported, x))
rand_rec <- map(rand_data, .f = \(x) recipe(Transported ~ ., data = x))

subset_ctrl <- trainControl(method = "cv", classProbs = TRUE, summaryFunction = many_stats)

# my_cluster <- snow::makeCluster(detectCores() - 1, type = 'SOCK')
# registerDoSNOW(my_cluster)
# 
# system.time({
#   subset_model <- map2(.x = rand_rec, .y = rand_data,
#                        .f = \(rec, df) train(rec, data = df, method = "glm", trControl = subset_ctrl, metric = "Accuracy"))
# })
# 
# subset_perf <- map(subset_model, .f = \(m) getTrainPerf(m))
# 
# save(subset_perf, file = "Random subsets performance.RData")
# 
# stopCluster(my_cluster)
# unregister()

load("Random subsets performance.RData")
```

```{r feature-selection-comparison, fig.cap="Comparison of different feature selection results against a random subset result."}
rf_random_avg <- map_dbl(subset_perf, .f = \(x) x$TrainAccuracy) %>%
  enframe()

ggplot() +
  geom_point(data = rfe_acc_corr_avg, aes(x = Iter, y = Accuracy, colour = "RFE Pen")) +
  geom_point(data = rfe_acc_corr_tree_avg, aes(x = Iter, y = Accuracy, colour = "RFE Tree")) +
  geom_point(data = sim_ann_50_ext_avg, aes(x = Iter, y = Accuracy, colour = "SA")) +
  geom_point(data = ga_external_avg, aes(x = Iter, y = Accuracy, colour = "GA")) +
  geom_point(data = rf_random_avg, aes(x = name, y = value, colour = "Random")) +
  scale_colour_manual(values = c("RFE Pen" = "green", "RFE Tree" = "blue", "SA" = "orange", "GA" = "red", "Random" = "darkgrey")) +
  labs(x = "Iterations", y = "Accuracy", colour = "Method")
```

We can see that both the recursive feature elimination process and the genetic algorithm outperforms the random variable subset but that the simulated annealing process doesn't. A test of significance shows us that the differences are relevant.

```{r rf-random-comparison-sign, fig.cap="Tests if significance between a random sample selection and different feature selection methods. Values below the red line (p.value < 0.05) indicate that there is a difference in performance."}
p_wilcoc_rfe_pen <- wilcox.test(sort(rfe_acc_corr_avg$Accuracy, decreasing = TRUE)[1:length(rf_random_avg$value)], 
            sort(rf_random_avg$value, decreasing = TRUE), paired = TRUE, alternative = "greater")$p.value
p_ttest_rfe_pen <- t.test(sort(rfe_acc_corr_avg$Accuracy, decreasing = TRUE)[1:length(rf_random_avg$value)], 
            sort(rf_random_avg$value, decreasing = TRUE), paired = TRUE, alternative = "greater")$p.value

p_wilcoc_rfe_tree <- wilcox.test(sort(rfe_acc_corr_tree_avg$Accuracy, decreasing = TRUE)[1:length(rf_random_avg$value)], 
            sort(rf_random_avg$value, decreasing = TRUE), paired = TRUE, alternative = "greater")$p.value
p_ttest_rfe_tree <- t.test(sort(rfe_acc_corr_tree_avg$Accuracy, decreasing = TRUE)[1:length(rf_random_avg$value)], 
            sort(rf_random_avg$value, decreasing = TRUE), paired = TRUE, alternative = "greater")$p.value

p_wilcoc_sa <- wilcox.test(sort(sim_ann_50_ext_avg$Accuracy, decreasing = TRUE)[1:length(rf_random_avg$value)], 
            sort(rf_random_avg$value, decreasing = TRUE), paired = TRUE, alternative = "greater")$p.value
p_ttest_sa <- t.test(sort(sim_ann_50_ext_avg$Accuracy, decreasing = TRUE)[1:length(rf_random_avg$value)], 
            sort(rf_random_avg$value, decreasing = TRUE), paired = TRUE, alternative = "greater")$p.value

p_wilcoc_ga <- wilcox.test(sort(ga_external_avg$Accuracy, decreasing = TRUE)[1:length(rf_random_avg$value)], 
            sort(rf_random_avg$value, decreasing = TRUE), paired = TRUE, alternative = "greater")$p.value
p_ttest_ga <- t.test(sort(ga_external_avg$Accuracy, decreasing = TRUE)[1:length(rf_random_avg$value)],
            sort(rf_random_avg$value, decreasing = TRUE),  paired = TRUE, alternative = "greater")$p.value

p_tests <- data.frame(test = rep(c("Wilcoxon paired", "t.test paired"), 4),
                      colour_grp = rep(c("RFE Pen", "RFE Tree", "SA", "GA"), 2),
                      value = c(p_wilcoc_rfe_pen, p_wilcoc_rfe_tree, p_wilcoc_sa, p_wilcoc_ga, 
                                p_ttest_rfe_pen, p_ttest_rfe_tree, p_ttest_sa, p_ttest_ga))

ggplot(p_tests, aes(x = test, y = value, colour = colour_grp)) +
  geom_point(size = 5) + 
  geom_hline(yintercept = 0.05, linetype = "dashed", colour = "red", show.legend = FALSE) +
  scale_colour_manual(values = c("RFE Pen" = "green", "RFE Tree" = "blue", "SA" = "orange", "GA" = "red", "size" = "none")) +
  labs(x = "Tests of difference", y = "P value", colour = "Method")
```

The Wilcoxon test checks whether the median of the difference in accuracy between a processed sample and a random sample are significant and it doesn't require the distribution of the differences between the paired samples to be normal. The t.test compares the means and does require the sample differences to be relatively normal. The null hypothesis in both cases is that the accuracies from the processed samples are smaller or equal to accuracies from random samples (and so low p-values mean that the processed samples have significantly greater accuracies that random samples). We see that all of the methods have significant improvement over the random sample and from Figure \@ref(fig:feature-selection-comparison), we can conclude that the RFE process that used the variables identified from the penalized model produces the best results.

Lets save this set of best features so that we can use it for our modelling stage.
```{r final-features}
best_features <- rfe_acc_corr$optVariables

best_interactions <- data.frame(features = best_features) %>%
  filter(str_detect(features, "_x_")) %>%
  mutate(V1 = str_split_i(features, "_x_", 1),
         V2 = str_split_i(features, "_x_", 2),
         ForFormula = str_c(V1, ":", V2),
         RevFormula = str_c(V2, ":", V1),
         PivotCol = "Pivot") %>%
  select(PivotCol, ForFormula, RevFormula) %>%
  pivot_longer(!PivotCol) %>%
  select(value) %>%
  distinct() %>%
  deframe() %>%
  str_flatten(., collapse = "+") %>%
  str_c("~", .) %>%
  as.formula(.)

best_vars <- data.frame(features = best_features) %>%
  filter(str_detect(features, "_x_", negate = TRUE)) %>%
  mutate(features = str_split_i(features, "_", 1)) %>%
  deframe()

save(best_interactions, file = "Best interactions.RData")
save(best_vars, file = "Best variables.RData")
```


```{r remove-06, include=FALSE}
rm(rfe_split, rfe_train, rfe_funcs, rfe_vars, int_formula, rfe_rec, rfe_bake, rfe_sizes, rfe_ctrl, rfe_rec_corr, rfe_bake_corr,
   rfe_split_all, rfe_train_all, rfe_rec_all, rfe_bake_all, rfe_sizes_all, rfe_vars_best_weird, int_form_weird,
   rfe_rec_all_weird, rf_mod, rfe_all_weird_wf, sa_split, sa_train, my_vars, sa_rec, sa_bake, sa_funcs, sa_grid, sa_ctrl_inner,
   sa_ctrl_outer, my_frq, pen_int_best, rfe_acc, rfe_acc_all, rfe_acc_all_weird, rfe_acc_corr, sim_ann_50_ext, sim_ann_50_ext2,
   sim_ann_50_int, sim_ann_50_int2, rand_subset, rand_data, rand_rec, subset_model)
```
